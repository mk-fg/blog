2010/1/Wheee-Ive-got-a-blog--=|||=-2010-01-30T21:06:51-=|||=-Wheee, I've got a blog ;)-=|||=--=|||=-There are times when even almighty google can't give a clear solution to some simple-enough problem, and it seem to be happening more frequently so far, so I thought I better write it all down somewhere, so here goes...<br /><br />The idea formed quite a while ago, but I've always either dismissed it as useless or was too lazy to implement it.<br /><br />Not that it's any difficult to start a blog these days, but hosting it on some major software platform like blogspot doesn't seem right to me, since I got too used to be able to access the code and tweak anything I don't like (yes, open-source has got me), and that should be pretty much impossible there.<br /><br />Other extreme is writing my own platform from scratch.<br />Not a bad thought altogether, but too much of a wasted effort, especially since I don't really like web development, web design and associated voodoo rituals. <br />Besides, it'd be too buggy anyway ;)<br /><br />So, I thought to get my hands on some simple and working out-of-the-box blog engine and fit it to my purposes as needed.<br />Since don't like php, 95% of such engines were out of question.<br />Surprisingly few platforms are written on py or lisp, and I wasn't fond of the idea of weaving separate cgi/fcgi module into my site.<br />Although it wasn't much of a problem with twisted, since control over request handling there is absolute and expressed in simple py code, I've stumbled upon my long-neglected google-apps account and a <a title="bloog project homepage" href="http://bloog.billkatz.com/">bloog</a> project.<br />Having played with gapps about two years ago, I really liked the idea: you get all the flexibility you want without having to care about things like db and some buggy api for it in the app, authorization and bot-protection, content serving mechanism, availability, even page generation, since google has django for it. In a nutshell I got a very simple representation layer between gdb and django, easy to bend in any way I want. As a bonus, bloog is not just simple and stupid tool, but quite nice and uniform restful api with YUI-based client.<br />Two evenings of customization and I'm pretty happy with the result and completely familiar with the inner workings of the thing. Thanks to Bill Katz for sharing it.<br /><br />All in all, it's an interesting experience. Blogosphere seem to have evolved into some kind of sophisticated ecosystem, with it's own platforms, resources, syndication rules, etc. While I'm pretty sure I won't blend in, at least I can study it a bit.<br /><br />So ends the first entry. Quite more of it than I've expected, actually.<br />More to come? I wonder.-=|||=-There are times when even almighty google can't give a clear solution to some simple-enough problem, and it seem to be happening more frequently so far, so I thought I better write it all down somewhere, so here goes...<br /><br />The idea formed quite a while ago, but I've always either dismissed it as useless or was too lazy to implement it.<br /><br />Not that it's any difficult to start a blog these days, but hosting it on some major software platform like blogspot doesn't seem right to me, since I got too used to be able to access the code and tweak anything I don't like (yes, open-source has got me), and that should be pretty much impossible there.<br /><br />Other extreme is writing my own platform from scratch.<br />Not a bad thought altogether, but too much of a wasted effort, especially since I don't really like web development, web design and associated voodoo rituals. <br />Besides, it'd be too buggy anyway ;)<br /><br />So, I thought to get my hands on some simple and working out-of-the-box blog engine and fit it to my purposes as needed.<br />Since don't like php, 95% of such engines were out of question.<br />Surprisingly few platforms are written on py or lisp, and I wasn't fond of the idea of weaving separate cgi/fcgi module into my site.<br />Although it wasn't much of a problem with twisted, since control over request handling there is absolute and expressed in simple py code, I've stumbled upon my long-neglected google-apps account and a <a title="bloog project homepage" href="http://bloog.billkatz.com/">bloog</a> project.<br />Having played with gapps about two years ago, I really liked the idea: you get all the flexibility you want without having to care about things like db and some buggy api for it in the app, authorization and bot-protection, content serving mechanism, availability, even page generation, since google has django for it. In a nutshell I got a very simple representation layer between gdb and django, easy to bend in any way I want. As a bonus, bloog is not just simple and stupid tool, but quite nice and uniform restful api with YUI-based client.<br />Two evenings of customization and I'm pretty happy with the result and completely familiar with the inner workings of the thing. Thanks to Bill Katz for sharing it.<br /><br />All in all, it's an interesting experience. Blogosphere seem to have evolved into some kind of sophisticated ecosystem, with it's own platforms, resources, syndication rules, etc. While I'm pretty sure I won't blend in, at least I can study it a bit.<br /><br />So ends the first entry. Quite more of it than I've expected, actually.<br />More to come? I wonder.-=||||||||||=-2010/2/POSIX-capabilities-for-python-=|||=-2010-02-01T06:19:21-=|||=-POSIX capabilities for python-=|||=-[u'Python', u'Unix']-=|||=-<p>I bet everyone who did any sysadmin tasks for linux/*bsd/whatever, stumbled upon the need to elevate privileges for some binary or script.</p><p>And most of the time if there's any need for privileges at all, it's for the ones that only root has: changing uid/gid on files, full backup, moving stuff owned by root/other-uids, signaling daemons, network tasks, etc.</p><p>Most of these tasks require only a fragment of root's power, so capabilities(7) is a nice way to get what you need without compromising anything. Great feat of caps is that they aren't inherited on exec, which seem to beat most of vulnerabilities for scripts, which don't usually suffer from C-like code shortcomings, provided the interpreter itself is up-to-date.</p><p>However, I've found that support for capabilities in linux (gentoo in my case, but that seem to hold true for other distros) is quite lacking. While they've been around for quite a while, even simpliest ping util still has suid bit instead of single cap_net_*, daemons get root just to bind a socket on a privileged port and service scripts just to send signal some pid.</p><p>For my purposes, I needed to backup FS with rsync, synchronize data between laptops and control autofs/mounts, all that from py scripts, and using full root for any of these tasks isn't necessary at all.</p><p>First problem is to give limited capabilities to a script.</p><p>One way to get them is to get everything from sudo or suid bit (aka get root), then drop everything that isn't needed, which is certainly better than having root all the time, but still excessive, since I don't need full and inheritable root at any point.</p><p>Another way is to inherit caps from cap-enabled binary. Just like suid, but you don't need to get all of them, they won't have to be inheritable and it doesn't have to be root-or-nothing. This approach looks a way nicer than the first one, so I decided to stick with it.</p><p>For py script, it means that the interpreter has to inherit some caps from     something else, since it wouldn't be wise to give caps to all py scripts indiscriminatively. "some_caps=i" (according to libcap text representation format, see cap_to_text(3)) or even "all=i" are certainly better.</p><p>To get caps from nothing, a simple C wrapper would suffice, but I'm a bit too lazy to write one for every script I run so I wrote one that gets all the caps and drops them to the subset that script file's inherited set. More on this (a bit unrelated) subject <a title="PPy project" href="http://fraggod.net/prj/ppy/">here</a>.</p><p>That leads to the point there py code starts with some permitted, but not immediately effective, set of capabilities.</p><p>Tinkering with caps in C is possible via <a title="libcap" href="http://www.friedhoff.org/posixfilecaps.html">libcap</a> and <a title="libcap-ng" href="http://people.redhat.com/sgrubb/libcap-ng/">libcap-ng</a>, and the only module for py seem to be cap-ng bindings. And they do suck.</p><p>Not only it's a direct C calls translation, but the interface is sorely lacking as well. Say, you need something extremely simple: to remove cap from some set, to activate permitted caps as effective or copy them to inherited set... well, no way to do that, what a tool. Funny thing, libcap can't do that in any obvious way either!</p><p>So here goes my solution - dumped whole cap-manipulation interface of both libs apart from dump-restore from/to string functions, wrote simple <a title="C module for caps storage" href="http://fraggod.net/svc/git/fgc/tree/strcaps.c">py-C interface</a> to it and wrapped them in python OO interface - <a title="py caps interface" href="http://fraggod.net/svc/git/fgc/tree/fgc/caps.py">Caps class</a>.</p><p>And the resulting high-level py code to make permitted caps effective goes like this:</p><pre name="code" class="python">Caps.from_process().activate().apply()</pre><p>To make permitted caps inheritable:</p><pre name="code" class="python">caps.propagnate().apply()</pre><p>And the rest of the ops is just like this:</p><pre name="code" class="python">caps['inheritable'].add('cap_dac_read_search')
caps.apply()
</pre><p>Well, friendly enough for me, and less than hundred lines of py code (which does all the work apart from load-save) for that.</p><p>While the code is part of a larger toolkit (<a title="fgc module" href="http://fraggod.net/svc/git/fgc/">fgc</a>), it doesn't depend on any other part of it - just C module and py wrapper.</p><p>Of course, I was wondering why no-one actually wrote something like this before, but looks like not many people actually use caps at all, even though it's worth it, supported by the fact that while I've managed to find the <a title="cap_dac_read_search bypass bug" href="http://bugzilla.kernel.org/show_bug.cgi?id=14913">bug in .32 and .33-rc* kernel</a>, preventing prehaps one of the most useful caps (cap_dac_read_search) from working ;(</p><p>Well, whatever.</p><p>Guess I'll write more about practical side and my application of this stuff next time.</p>-=|||=-<p>I bet everyone who did any sysadmin tasks for linux/*bsd/whatever, stumbled upon the need to elevate privileges for some binary or script.</p><p>And most of the time if there's any need for privileges at all, it's for the ones that only root has: changing uid/gid on files, full backup, moving stuff owned by root/other-uids, signaling daemons, network tasks, etc.</p><p>Most of these tasks require only a fragment of root's power, so capabilities(7) is a nice way to get what you need without compromising anything. Great feat of caps is that they aren't inherited on exec, which seem to beat most of vulnerabilities for scripts, which don't usually suffer from C-like code shortcomings, provided the interpreter itself is up-to-date.</p><p>However, I've found that support for capabilities in linux (gentoo in my case, but that seem to hold true for other distros) is quite lacking. While they've been around for quite a while, even simpliest ping util still has suid bit instead of single cap_net_*, daemons get root just to bind a socket on a privileged port and service scripts just to send signal some pid.</p><p>For my purposes, I needed to backup FS with rsync, synchronize data between laptops and control autofs/mounts, all that from py scripts, and using full root for any of these tasks isn't necessary at all.</p><p>First problem is to give limited capabilities to a script.</p><p>One way to get them is to get everything from sudo or suid bit (aka get root), then drop everything that isn't needed, which is certainly better than having root all the time, but still excessive, since I don't need full and inheritable root at any point.</p><p>Another way is to inherit caps from cap-enabled binary. Just like suid, but you don't need to get all of them, they won't have to be inheritable and it doesn't have to be root-or-nothing. This approach looks a way nicer than the first one, so I decided to stick with it.</p><p>For py script, it means that the interpreter has to inherit some caps from     something else, since it wouldn't be wise to give caps to all py scripts indiscriminatively. "some_caps=i" (according to libcap text representation format, see cap_to_text(3)) or even "all=i" are certainly better.</p><p>To get caps from nothing, a simple C wrapper would suffice, but I'm a bit too lazy to write one for every script I run so I wrote one that gets all the caps and drops them to the subset that script file's inherited set. More on this (a bit unrelated) subject <a title="PPy project" href="http://fraggod.net/prj/ppy/">here</a>.</p><p>That leads to the point there py code starts with some permitted, but not immediately effective, set of capabilities.</p><p>Tinkering with caps in C is possible via <a title="libcap" href="http://www.friedhoff.org/posixfilecaps.html">libcap</a> and <a title="libcap-ng" href="http://people.redhat.com/sgrubb/libcap-ng/">libcap-ng</a>, and the only module for py seem to be cap-ng bindings. And they do suck.</p><p>Not only it's a direct C calls translation, but the interface is sorely lacking as well. Say, you need something extremely simple: to remove cap from some set, to activate permitted caps as effective or copy them to inherited set... well, no way to do that, what a tool. Funny thing, libcap can't do that in any obvious way either!</p><p>So here goes my solution - dumped whole cap-manipulation interface of both libs apart from dump-restore from/to string functions, wrote simple <a title="C module for caps storage" href="http://fraggod.net/svc/git/fgc/tree/strcaps.c">py-C interface</a> to it and wrapped them in python OO interface - <a title="py caps interface" href="http://fraggod.net/svc/git/fgc/tree/fgc/caps.py">Caps class</a>.</p><p>And the resulting high-level py code to make permitted caps effective goes like this:</p><pre name="code" class="python">Caps.from_process().activate().apply()</pre><p>To make permitted caps inheritable:</p><pre name="code" class="python">caps.propagnate().apply()</pre><p>And the rest of the ops is just like this:</p><pre name="code" class="python">caps['inheritable'].add('cap_dac_read_search')<br />caps.apply()<br /></pre><p>Well, friendly enough for me, and less than hundred lines of py code (which does all the work apart from load-save) for that.</p><p>While the code is part of a larger toolkit (<a title="fgc module" href="http://fraggod.net/svc/git/fgc/">fgc</a>), it doesn't depend on any other part of it - just C module and py wrapper.</p><p>Of course, I was wondering why no-one actually wrote something like this before, but looks like not many people actually use caps at all, even though it's worth it, supported by the fact that while I've managed to find the <a title="cap_dac_read_search bypass bug" href="http://bugzilla.kernel.org/show_bug.cgi?id=14913">bug in .32 and .33-rc* kernel</a>, preventing prehaps one of the most useful caps (cap_dac_read_search) from working ;(</p><p>Well, whatever.</p><p>Guess I'll write more about practical side and my application of this stuff next time.</p>-=||||||||||=-2010/2/My-simple-ok-not-quite-backup-system-=|||=-2010-02-11T22:58:33-=|||=-My "simple" (ok, not quite) backup system-=|||=-[u'Unix', u'SysAdmin']-=|||=-<p>There's saying: "there are two kinds of sysadmins - the ones that aren't making backups yet, and the ones that already do". I'm not sure if the essence of the phrase wasn't lost in translation (ru->eng), but the point is that it's just a matter of time, 'till you start backing-up your data.</p><p>Luckily for me, I've got it quite fast, and consider making backups
on a daily basis is a must-have practice for any developer/playground
machine or under-development server. It saved me on a countless
occasions, and there were quite a few times when I just needed to check
if everything in my system is still in place and were there before.</p><p>Here I'll try to describe my sample backup system operation and the reasons for building it like that.</p><p>Ok, what do I need from the backup ecosystem?</p><ul>
<li>Obviously, it'd be a bother to backup each machine manually every day, so there's a cron.</li>
<li>Backing up to the same machine obviously isn't a good idea, so the backup has to be transferred to remote system, preferrably several ones, in different environments.</li>
<li>Another thing to consider is the size of such backups and efficient method of storage, transfer and access to them.</li>
<li>Then there's a security issue - full fs read capabilities are required to create the backup, and that can be easily abused.</li>
</ul>
<p>First two points suggest that you either need privileged remote access to the machine (like root ssh, which is a security issue) or make backups (local fs replicas) locally then transfer them to remote with unprivileged access (just to these backups).</p><p>Local backups make third point (space efficiency) more difficult, since you either have to make full backups locally (and transferring them, at the very least, is not-so-efficient at all) or keep some metadata about the state of all the files (like "md5deep -r /", but with file metadata checksums as well), so you can efficiently generate increments.</p><p>Traditional hacky way to avoid checksumming is to look at inode mtimes only, but that is unreliable, especially so, since I like to use stuff like "cp -a" and "rsync -a" (synchronises timestamps) on a daily basis and play with timestamps any way I like to.</p><p>Space efficiency usually achieved via incremental archives. Not really my thing, since they have terrible accessibility - tar (and any other streaming formats like cpio) especially, <a title="Disk ARchive" href="http://dar.linux.free.fr/">dar</a> less so, since it has random access and file subset merge features, but still bad at keeping increments (reference archive have to be preserved, for one thing) and is not readily-browseable - you have to unpack it to some tmp path before doing anything useful with files. There's also <a title="SquashFS" href="http://squashfs.sourceforge.net/">SquashFS</a>, which is sorta "browsable archive", but it has not increment-tracking features at all ;(</p><p>Another way to preserve space is to forget about these archive formats and just use filesystem to store backed-up tree. Compression is also an option here with ZFS or Btrfs or some FUSE layer like <a title="fusecompress filesystem layer" href="http://www.miio.net/fusecompress/">fusecompress</a>, keeping increments is also simple with either hardlinks or snapshots. Obviously, accessibility (and simplicity, btw) here is next to nothing, and you can use diff, rsync and rest of the usual tools to do anything you want with it, which I see as a great feat. And should you need to transfer it in a container - just tar it right to the medium in question. Of course, I liked this way a lot more than the archives, and decided to stick with it.</p><p>So, at this point the task was refined to just rsync from backed-up machine to backup storage.<br />Since I have two laptops which mightn't always be accessible to backup host and should be able to initiate backup when I need to without much effort, it's best if the backups are initiated from backed-up machine.</p><p>That said...</p><ul>
<li>I don't want to have any kind of access to backup storage from this machine or know anything about backup storage layout, so direct rsync to storage is out of question.</li>
<li>At the same time, I don't need any-time root - or any other kind of - access to local machine form backup host, I only need it when I do request a backup locally (or local cron does it for me).</li>
<li>In fact, even then, I don't need backup host to have anything but read-only access to local filesystem. This effectively obsoletes the idea of unprivileged access just-to-local-backups, since they are the same read-only (...replicas of...) local filesystem, so there's just no need to make them.</li>
</ul>
<p>Obvious tool for the task is rsync-pull, initiated from backup host (and triggered by backed-up host), with some sort of one-time pass, given by the backed-up machine.</p><p>And local rsync should be limited to read-only access, so it can't be used by backup-host imposter to zero or corrupt local rootfs. Ok, that's quite a paranoid scenario, especially if you can identify backup host by something like ssh key fingerprint, but it's still a good policy.</p><p>Ways to limit local rsync to read-only, but otherwise unrestricted, access I've considered were:</p><ul>
<li>Locally-initiated rsync with parameters, passed from backup host, like "rsync -a / host:/path/to/storage". Not a good option, since that requres parameter checking and that's proven to be error-prone soul-sucking task (just look at the sudo or suid-perl), plus it'd need some one-time and restricted access mechanism on backup host.</li>
<li>Local rsyncd with one-time credentials. Not a bad way. Simple, for one thing, but the link between the hosts can be insecure (wireless) and rsync protocol does not provide any encryption for the passed data - and that's the whole filesystem piped through. Also, there's no obvious way to make sure it'd process only one connection (from backup host, just to read fs once) - credentials can be sniffed and used again.</li>
<li>Same as before, but via locally-initiated reverse-ssh tunnel to rsyncd.</li>
<li>One-shot local sshd with rsync-only command restriction, one-time generated keypair and remote ip restriction.</li>
</ul>
<p>Last two options seem to be the best, being pretty much the same thing, with the last one more robust and secure, since there's no need to tamper with rsyncd and it's really one-shot.</p><p>Caveat however, is how to give rsync process read-only access. Luckily, there's dac_read_search posix capability, which allows just that - all that's needed is to make it inheritable-effective for rsync binary in question, which can be separate statically-linked one, just for these backup purposes.<br />Separate one-shot sshd also friendly to nice/ionice setting and traffic shaping (since it's listening on separate port), which is quite crucial for wireless upload bandwidth since it has a major impact on interactive connections - output pfifo gets swarmed by ssh-data packets and every other connection actions (say, ssh session keypress) lag until it's packets wait in this line... but that's a bit unrelated note (see <a title="Linux Advanced Routing and Traffic Control HOWTO" href="http://lartc.org/howto/lartc.qdisc.html">LARTC</a> if you don't know what it's all about, mandatory).</p><span>And that actually concludes the overall plan, which comes to these steps:</span><ul style="font-family: inherit;">
<li style="font-family: inherit;"><span><span>Backed-up host:</span></span></li>
</ul>
<ul>
</ul>
<ol>
<li>Generates ssh keypair (ssh-keygen).</li>
<li>Starts one-shot sshd ("-d" option) with authorization only for generated public key, command ("ForceCommand" option), remote ip ("from=" option) and other (no tunneling, key-only auth, etc) restrictions.</li>
<li>Connects (ssh, naturally) to backup host's unprivileged user or restricted shell and sends it's generated (private) key for sshd auth, waits.</li>
</ol>
<li><ul>
</ul>
<ul>
<li>Backup host:</li>
</ul>
<ul>
</ul></li>
<li><ol>
<li>Receives private ssh key from backed-up host.</li>
<li>rsync backed-up-host:/ /path/to/local/storage</li>
</ol>
</li>
<li><ul>
</ul>
<br /><p>Minor details:</p><ul>
<li>ssh pubkey authentication is used to open secure channel to a backup host, precluding any mitm attacks, non-interactive cron-friendly.</li>
<li>sshd has lowered nice/ionice and bandwidth priority, so it won't interfere with host operation in any way.</li>
<li>Backup host receives link destination for rsync along with the private key, so it won't have to guess who requested the backup and which port it should use.</li>
<li>ForceCommand can actually point to the same "backup initiator" script, which will act as a shell with full rsync command in SSH_ORIGINAL_COMMAND env var, so additional checks or privilege manipulations can be performed immediately before sync.</li>
<li>Minimal set of tools used: openssh, rsync and two (fairly simple) scripts on both ends.</li>
</ul>
<p>Phew... and I've started writing this just as an example usage of posix capabilities for <a title="POSIX capabilities for python" href="http://blog.fraggod.net/2010/2/POSIX-capabilities-for-python">previous entry</a>. Guess I'll leave implementation details for the next one.</p></li>-=|||=-<p>There's saying: "there are two kinds of sysadmins - the ones that aren't making backups yet, and the ones that already do". I'm not sure if the essence of the phrase wasn't lost in translation (ru->eng), but the point is that it's just a matter of time, 'till you start backing-up your data.</p><p>Luckily for me, I've got it quite fast, and consider making backups
on a daily basis is a must-have practice for any developer/playground
machine or under-development server. It saved me on a countless
occasions, and there were quite a few times when I just needed to check
if everything in my system is still in place and were there before.</p><p>Here I'll try to describe my sample backup system operation and the reasons for building it like that.</p><p>Ok, what do I need from the backup ecosystem?</p><ul>
<li>Obviously, it'd be a bother to backup each machine manually every day, so there's a cron.</li>
<li>Backing up to the same machine obviously isn't a good idea, so the backup has to be transferred to remote system, preferrably several ones, in different environments.</li>
<li>Another thing to consider is the size of such backups and efficient method of storage, transfer and access to them.</li>
<li>Then there's a security issue - full fs read capabilities are required to create the backup, and that can be easily abused.</li>
</ul>
<p>First two points suggest that you either need privileged remote access to the machine (like root ssh, which is a security issue) or make backups (local fs replicas) locally then transfer them to remote with unprivileged access (just to these backups).</p><p>Local backups make third point (space efficiency) more difficult, since you either have to make full backups locally (and transferring them, at the very least, is not-so-efficient at all) or keep some metadata about the state of all the files (like "md5deep -r /", but with file metadata checksums as well), so you can efficiently generate increments.</p><p>Traditional hacky way to avoid checksumming is to look at inode mtimes only, but that is unreliable, especially so, since I like to use stuff like "cp -a" and "rsync -a" (synchronises timestamps) on a daily basis and play with timestamps any way I like to.</p><p>Space efficiency usually achieved via incremental archives. Not really my thing, since they have terrible accessibility - tar (and any other streaming formats like cpio) especially, <a title="Disk ARchive" href="http://dar.linux.free.fr/">dar</a> less so, since it has random access and file subset merge features, but still bad at keeping increments (reference archive have to be preserved, for one thing) and is not readily-browseable - you have to unpack it to some tmp path before doing anything useful with files. There's also <a title="SquashFS" href="http://squashfs.sourceforge.net/">SquashFS</a>, which is sorta "browsable archive", but it has not increment-tracking features at all ;(</p><p>Another way to preserve space is to forget about these archive formats and just use filesystem to store backed-up tree. Compression is also an option here with ZFS or Btrfs or some FUSE layer like <a title="fusecompress filesystem layer" href="http://www.miio.net/fusecompress/">fusecompress</a>, keeping increments is also simple with either hardlinks or snapshots. Obviously, accessibility (and simplicity, btw) here is next to nothing, and you can use diff, rsync and rest of the usual tools to do anything you want with it, which I see as a great feat. And should you need to transfer it in a container - just tar it right to the medium in question. Of course, I liked this way a lot more than the archives, and decided to stick with it.</p><p>So, at this point the task was refined to just rsync from backed-up machine to backup storage.<br />Since I have two laptops which mightn't always be accessible to backup host and should be able to initiate backup when I need to without much effort, it's best if the backups are initiated from backed-up machine.</p><p>That said...</p><ul>
<li>I don't want to have any kind of access to backup storage from this machine or know anything about backup storage layout, so direct rsync to storage is out of question.</li>
<li>At the same time, I don't need any-time root - or any other kind of - access to local machine form backup host, I only need it when I do request a backup locally (or local cron does it for me).</li>
<li>In fact, even then, I don't need backup host to have anything but read-only access to local filesystem. This effectively obsoletes the idea of unprivileged access just-to-local-backups, since they are the same read-only (...replicas of...) local filesystem, so there's just no need to make them.</li>
</ul>
<p>Obvious tool for the task is rsync-pull, initiated from backup host (and triggered by backed-up host), with some sort of one-time pass, given by the backed-up machine.</p><p>And local rsync should be limited to read-only access, so it can't be used by backup-host imposter to zero or corrupt local rootfs. Ok, that's quite a paranoid scenario, especially if you can identify backup host by something like ssh key fingerprint, but it's still a good policy.</p><p>Ways to limit local rsync to read-only, but otherwise unrestricted, access I've considered were:</p><ul>
<li>Locally-initiated rsync with parameters, passed from backup host, like "rsync -a / host:/path/to/storage". Not a good option, since that requres parameter checking and that's proven to be error-prone soul-sucking task (just look at the sudo or suid-perl), plus it'd need some one-time and restricted access mechanism on backup host.</li>
<li>Local rsyncd with one-time credentials. Not a bad way. Simple, for one thing, but the link between the hosts can be insecure (wireless) and rsync protocol does not provide any encryption for the passed data - and that's the whole filesystem piped through. Also, there's no obvious way to make sure it'd process only one connection (from backup host, just to read fs once) - credentials can be sniffed and used again.</li>
<li>Same as before, but via locally-initiated reverse-ssh tunnel to rsyncd.</li>
<li>One-shot local sshd with rsync-only command restriction, one-time generated keypair and remote ip restriction.</li>
</ul>
<p>Last two options seem to be the best, being pretty much the same thing, with the last one more robust and secure, since there's no need to tamper with rsyncd and it's really one-shot.</p><p>Caveat however, is how to give rsync process read-only access. Luckily, there's dac_read_search posix capability, which allows just that - all that's needed is to make it inheritable-effective for rsync binary in question, which can be separate statically-linked one, just for these backup purposes.<br />Separate one-shot sshd also friendly to nice/ionice setting and traffic shaping (since it's listening on separate port), which is quite crucial for wireless upload bandwidth since it has a major impact on interactive connections - output pfifo gets swarmed by ssh-data packets and every other connection actions (say, ssh session keypress) lag until it's packets wait in this line... but that's a bit unrelated note (see <a title="Linux Advanced Routing and Traffic Control HOWTO" href="http://lartc.org/howto/lartc.qdisc.html">LARTC</a> if you don't know what it's all about, mandatory).</p><span>And that actually concludes the overall plan, which comes to these steps:</span><ul style="font-family: inherit;">
<li style="font-family: inherit;"><span><span>Backed-up host:</span></span></li>
</ul>
<ul>
</ul>
<ol>
<li>Generates ssh keypair (ssh-keygen).</li>
<li>Starts one-shot sshd ("-d" option) with authorization only for generated public key, command ("ForceCommand" option), remote ip ("from=" option) and other (no tunneling, key-only auth, etc) restrictions.</li>
<li>Connects (ssh, naturally) to backup host's unprivileged user or restricted shell and sends it's generated (private) key for sshd auth, waits.</li>
</ol>
<li><ul>
</ul>
<ul>
<li>Backup host:</li>
</ul>
<ul>
</ul></li>

<li><ol>
<li>Receives private ssh key from backed-up host.</li>
<li>rsync backed-up-host:/ /path/to/local/storage</li>
</ol>
</li>
<li><ul>
</ul>
<br /><p>Minor details:</p><ul>
<li>ssh pubkey authentication is used to open secure channel to a backup host, precluding any mitm attacks, non-interactive cron-friendly.</li>
<li>sshd has lowered nice/ionice and bandwidth priority, so it won't interfere with host operation in any way.</li>
<li>Backup host receives link destination for rsync along with the private key, so it won't have to guess who requested the backup and which port it should use.</li>
<li>ForceCommand can actually point to the same "backup initiator" script, which will act as a shell with full rsync command in SSH_ORIGINAL_COMMAND env var, so additional checks or privilege manipulations can be performed immediately before sync.</li>
<li>Minimal set of tools used: openssh, rsync and two (fairly simple) scripts on both ends.</li>
</ul>
<p>Phew... and I've started writing this just as an example usage of posix capabilities for <a title="POSIX capabilities for python" href="http://blog.fraggod.net/2010/2/POSIX-capabilities-for-python">previous entry</a>. Guess I'll leave implementation details for the next one.</p></li>-=||||||||||=-2010/2/My-simple-ok-not-quite-backup-system-implementation-backup-host-=|||=-2010-02-14T09:51:50-=|||=-My "simple" (ok, not quite) backup system - implementation (backup host)-=|||=-[u'Python', u'Unix', u'SysAdmin', u'SSH', u'Rsync']-=|||=-<p>According to the <a title="Overview of the whole system" href="http://blog.fraggod.net/2010/2/My-simple-ok-not-quite-backup-system">general plan</a>, with <a title="Backed-up side configuration" href="http://blog.fraggod.net/2010/2/My-simple-ok-not-quite-backup-system-implementation-backed-up-side">backed-up side scripts in place</a>, some backup-grab mechanism is needed on the backup host.</p><p>So far, sshd provides secure channel and authentication, launching control script as a shell, <span>backed-up side script</span> provides hostname:port for one-shot ssh link on the commandline, with private key to this link and backup-exclusion paths list piped in.</p><p>All that's left to do on this side is to read the data from a pipe and start rsync over this link, with a few preceding checks, like a free space check, so backup process won't be strangled by its abscence and as many as possible backups will be preserved for as long as possible, removing them right before receiving new ones.</p><p>Historically, this script also works with any specified host,
interactively logging into it as root for rsync operation, so there's
bit of interactive voodoo involved, which isn't relevant for the
remotely-initiated backup case.<br />
</p><p>Ssh parameters for rsync transport are passed to rsync itself, since
it starts ssh process, via "--rsh" option. Inside the script,these are accumulated in bak_src_ext variable 
</p><p>Note that in case then this script is started as a shell, user is not a root, yet it needs to store filesystem metadata like uids, gids, acls, etc.<br />To that end, rsync can employ user_xattr's, although it looks extremely unportable and inproper to me, since nothing but rsync will translate them back to original metadata, so rsync need to be able to change fs metadata directly, and to that end there's posix capabilities.</p><p>I use <a title="POSIX caps for python implementation details" href="http://blog.fraggod.net/2010/2/POSIX-capabilities-for-python">my module</a> for capability manipulation, as well as <a title="fgc toolkit" href="http://fraggod.net/svc/git/fgc/">other convenience modules</a> here and there, their purpose is quite obvious and replacing these with stdlib functions should be pretty straightforward, if necessary.</p><p>Activating the inherited capabilities:</p><pre name="code" class="python">bak_user = os.getuid()
if bak_user:
	from fgc.caps import Caps
	import pwd
	os.putenv('HOME', pwd.getpwuid(bak_user).pw_dir)
	Caps.from_process().activate().apply()
</pre><p>But first things first - there's data waiting on commandline and stdin. Getting the hostname and port...</p><pre name="code" class="python">bak_src = argz[0]
try: bak_src, bak_src_ext = bak_src.split(':')
except: bak_src_ext = tuple()
else: bak_src_ext = '-p', bak_src_ext
</pre><p>...and the key / exclusions:</p><pre name="code" class="python">bak_key = bak_sub('.key_{0}'.format(bak_host))
password, reply = it.imap(
	op.methodcaller('strip', spaces), sys.stdin.read().split('\n\n\n', 1) )
open(bak_key, 'w').write(password)
sh.chmod(bak_key, 0400)
bak_src_ext += '-i', os.path.realpath(bak_key)
</pre><p>Then, basic rsync invocation options can be constructed:</p><pre name="code" class="python">sync_optz = [ '-HaAXz',
	('--skip-compress='
		r'gz/bz2/t\[gb\]z/tbz2/lzma/7z/zip/rar'
		r'/rpm/deb/iso'
		r'/jpg/gif/png/mov/avi/ogg/mp\[34g\]/flv/pdf'),
	'--super',
	'--exclude-from={0}'.format(bak_exclude_server),
	'--rsync-path=ionice -c3 rsync',
	'--rsh=ssh {0}'.format(' '.join(bak_src_ext)) ]
</pre><p>Excluded paths list here is written to a local file, to keep track which paths were excluded in each backup.<br />"--super" option is actually necessary if local user is not root, rsync drops all the metadata otherwise.<br />"HaAX" is like "preserve all" flags - Hardlinks, ownership/modes/times ("a" flag), Acl's, eXtended attrs.<br />"--rsh" here is the ssh command, with parameters, determined above.</p><p>Aside from that, there's also need to specify hardlink destination path, which should be a previous backup, and that traditionnaly is the domain of ugly perlisms - regexps.</p><pre name="code" class="python">bakz_re = re.compile(r'^([^.].*)\.\d+-\d+-\d+.\d+$') # host.YYYY-mm-dd.unix_time
bakz = list( bak for bak in os.listdir(bak_root)
 if bakz_re.match(bak) ) # all backups
bakz_host = sorted(it.ifilter(op.methodcaller(
	'startswith', bak_host ), bakz), reverse=True)
</pre><p>So, the final sync options come to these:</p><pre name="code" class="python">src = '{0}:/'.format(src)
sync_optz = list(dta.chain( sync_optz, '--link-dest={0}'\
		.format(os.path.realpath(bakz_host[0])), src, bak_path ))\
	if bakz_host else list(dta.chain(sync_optz, src, bak_path))
</pre><p>The only interlude is to cleanup backup partition if it gets too crowded:</p><pre name="code" class="python">## Free disk space check / cleanup
ds, df = sh.df(bak_root)
min_free = ( max(min_free_avg( (ds-df) / len(bakz)), min_free_abs*G)
	if min_free_avg and bakz else min_free_abs*G )

def bakz_rmq():
	'''Iterator that returns bakz in order of removal'''
	bakz_queue = list( list(bakz) for host,bakz in it.groupby(sorted(bakz),
		key=lambda bak: bakz_re.match(bak).group(1)) )
	while bakz_queue:
		bakz_queue.sort(key=len)
		bakz_queue[-1].sort(reverse=True)
		if len(bakz_queue[-1]) &lt;= min_keep: break
		yield bakz_queue[-1].pop()

if df &lt; min_free:
	for bak in bakz_rmq():
		log.info('Removing backup: {0}'.format(bak))
		sh.rr(bak, onerror=False)
		ds, df = sh.df(bak_root)
		if df &gt;= min_free: break
	else:
		log.fatal( 'Not enough space on media:'
				' {0:.1f}G, need {1:.1f}G, {2} backups min)'\
			.format( op.truediv(df, G),
				op.truediv(min_free, G), min_keep ), crash=2 )
</pre><p>And from here it's just to start rsync and wait 'till the job's done.</p><p>This thing works for months now, and saved my day on many occasions, but the most important thing here I think is the knowledge that the backup is there should you need one, so you never have to worry about breaking your system or losing anything important there, whatever you do.</p><p><span><a title="Full script code" href="http://fraggod.net/oss/bin_scrz/fs_backup_grab.py">Here</a>'s the full script</span>.</p><p>Actually, there's more to the story, since just keeping backups on single local harddisk (raid1 of two disks, actually) isn't enough for me. Call this paranoia, but setting up system from scratch and restoring all the data I have is a horrible nightmare, and there are possibility of fire, robbery, lighting, voltage surge or some other disaster that can easily take this disk(s) out of the picture, and few gigabytes of space in the web come almost for free these days - there are p2p storages like wuala, dropbox, google apps/mail with their unlimited quotas...</p><p>So, why not upload all this stuff there and be absolutely sure it'd never go down, whatever happens? Sure thing.<br />Guess I'll write a note on the topic as much to document it for myself as for the case someone might find it useful as well, plus the ability to link it instead of explaining ;)</p>-=|||=-<p>According to the <a title="Overview of the whole system" href="http://blog.fraggod.net/2010/2/My-simple-ok-not-quite-backup-system">general plan</a>, with <a title="Backed-up side configuration" href="http://blog.fraggod.net/2010/2/My-simple-ok-not-quite-backup-system-implementation-backed-up-side">backed-up side scripts in place</a>, some backup-grab mechanism is needed on the backup host.</p><p>So far, sshd provides secure channel and authentication, launching control script as a shell, <span>backed-up side script</span> provides hostname:port for one-shot ssh link on the commandline, with private key to this link and backup-exclusion paths list piped in.</p><p>All that's left to do on this side is to read the data from a pipe and start rsync over this link, with a few preceding checks, like a free space check, so backup process won't be strangled by its abscence and as many as possible backups will be preserved for as long as possible, removing them right before receiving new ones.</p><p>Historically, this script also works with any specified host,
interactively logging into it as root for rsync operation, so there's
bit of interactive voodoo involved, which isn't relevant for the
remotely-initiated backup case.<br />
</p><p>Ssh parameters for rsync transport are passed to rsync itself, since
it starts ssh process, via "--rsh" option. Inside the script,these are accumulated in bak_src_ext variable 
</p><p>Note that in case then this script is started as a shell, user is not a root, yet it needs to store filesystem metadata like uids, gids, acls, etc.<br />To that end, rsync can employ user_xattr's, although it looks extremely unportable and inproper to me, since nothing but rsync will translate them back to original metadata, so rsync need to be able to change fs metadata directly, and to that end there's posix capabilities.</p><p>I use <a title="POSIX caps for python implementation details" href="http://blog.fraggod.net/2010/2/POSIX-capabilities-for-python">my module</a> for capability manipulation, as well as <a title="fgc toolkit" href="http://fraggod.net/svc/git/fgc/">other convenience modules</a> here and there, their purpose is quite obvious and replacing these with stdlib functions should be pretty straightforward, if necessary.</p><p>Activating the inherited capabilities:</p><pre name="code" class="python">bak_user = os.getuid()<br />if bak_user:<br />	from fgc.caps import Caps<br />	import pwd<br />	os.putenv('HOME', pwd.getpwuid(bak_user).pw_dir)<br />	Caps.from_process().activate().apply()<br /></pre><p>But first things first - there's data waiting on commandline and stdin. Getting the hostname and port...</p><pre name="code" class="python">bak_src = argz[0]<br />try: bak_src, bak_src_ext = bak_src.split(':')<br />except: bak_src_ext = tuple()<br />else: bak_src_ext = '-p', bak_src_ext<br /></pre><p>...and the key / exclusions:</p><pre name="code" class="python">bak_key = bak_sub('.key_{0}'.format(bak_host))<br />password, reply = it.imap(<br />	op.methodcaller('strip', spaces), sys.stdin.read().split('\n\n\n', 1) )<br />open(bak_key, 'w').write(password)<br />sh.chmod(bak_key, 0400)<br />bak_src_ext += '-i', os.path.realpath(bak_key)<br /></pre><p>Then, basic rsync invocation options can be constructed:</p><pre name="code" class="python">sync_optz = [ '-HaAXz',<br />	('--skip-compress='<br />		r'gz/bz2/t\[gb\]z/tbz2/lzma/7z/zip/rar'<br />		r'/rpm/deb/iso'<br />		r'/jpg/gif/png/mov/avi/ogg/mp\[34g\]/flv/pdf'),<br />	'--super',<br />	'--exclude-from={0}'.format(bak_exclude_server),<br />	'--rsync-path=ionice -c3 rsync',<br />	'--rsh=ssh {0}'.format(' '.join(bak_src_ext)) ]<br /></pre><p>Excluded paths list here is written to a local file, to keep track which paths were excluded in each backup.<br />"--super" option is actually necessary if local user is not root, rsync drops all the metadata otherwise.<br />"HaAX" is like "preserve all" flags - Hardlinks, ownership/modes/times ("a" flag), Acl's, eXtended attrs.<br />"--rsh" here is the ssh command, with parameters, determined above.</p><p>Aside from that, there's also need to specify hardlink destination path, which should be a previous backup, and that traditionnaly is the domain of ugly perlisms - regexps.</p><pre name="code" class="python">bakz_re = re.compile(r'^([^.].*)\.\d+-\d+-\d+.\d+$') # host.YYYY-mm-dd.unix_time<br />bakz = list( bak for bak in os.listdir(bak_root)<br /> if bakz_re.match(bak) ) # all backups<br />bakz_host = sorted(it.ifilter(op.methodcaller(<br />	'startswith', bak_host ), bakz), reverse=True)<br /></pre><p>So, the final sync options come to these:</p><pre name="code" class="python">src = '{0}:/'.format(src)<br />sync_optz = list(dta.chain( sync_optz, '--link-dest={0}'\<br />		.format(os.path.realpath(bakz_host[0])), src, bak_path ))\<br />	if bakz_host else list(dta.chain(sync_optz, src, bak_path))<br /></pre><p>The only interlude is to cleanup backup partition if it gets too crowded:</p><pre name="code" class="python">## Free disk space check / cleanup<br />ds, df = sh.df(bak_root)<br />min_free = ( max(min_free_avg( (ds-df) / len(bakz)), min_free_abs*G)<br />	if min_free_avg and bakz else min_free_abs*G )<br /><br />def bakz_rmq():<br />	'''Iterator that returns bakz in order of removal'''<br />	bakz_queue = list( list(bakz) for host,bakz in it.groupby(sorted(bakz),<br />		key=lambda bak: bakz_re.match(bak).group(1)) )<br />	while bakz_queue:<br />		bakz_queue.sort(key=len)<br />		bakz_queue[-1].sort(reverse=True)<br />		if len(bakz_queue[-1]) &lt;= min_keep: break<br />		yield bakz_queue[-1].pop()<br /><br />if df &lt; min_free:<br />	for bak in bakz_rmq():<br />		log.info('Removing backup: {0}'.format(bak))<br />		sh.rr(bak, onerror=False)<br />		ds, df = sh.df(bak_root)<br />		if df &gt;= min_free: break<br />	else:<br />		log.fatal( 'Not enough space on media:'<br />				' {0:.1f}G, need {1:.1f}G, {2} backups min)'\<br />			.format( op.truediv(df, G),<br />				op.truediv(min_free, G), min_keep ), crash=2 )<br /></pre><p>And from here it's just to start rsync and wait 'till the job's done.</p><p>This thing works for months now, and saved my day on many occasions, but the most important thing here I think is the knowledge that the backup is there should you need one, so you never have to worry about breaking your system or losing anything important there, whatever you do.</p><p><span><a title="Full script code" href="http://fraggod.net/oss/bin_scrz/fs_backup_grab.py">Here</a>'s the full script</span>.</p><p>Actually, there's more to the story, since just keeping backups on single local harddisk (raid1 of two disks, actually) isn't enough for me. Call this paranoia, but setting up system from scratch and restoring all the data I have is a horrible nightmare, and there are possibility of fire, robbery, lighting, voltage surge or some other disaster that can easily take this disk(s) out of the picture, and few gigabytes of space in the web come almost for free these days - there are p2p storages like wuala, dropbox, google apps/mail with their unlimited quotas...</p><p>So, why not upload all this stuff there and be absolutely sure it'd never go down, whatever happens? Sure thing.<br />Guess I'll write a note on the topic as much to document it for myself as for the case someone might find it useful as well, plus the ability to link it instead of explaining ;)</p>-=||||||||||=-2010/2/My-simple-ok-not-quite-backup-system-implementation-backed-up-side-=|||=-2010-02-13T14:29:46-=|||=-My "simple" (ok, not quite) backup system - implementation (backed-up side)-=|||=-[u'Python', u'Unix', u'SysAdmin', u'SSH']-=|||=-<p>As I've already outlined <a title="Previous note on the subject" href="http://blog.fraggod.net/2010/2/My-simple-ok-not-quite-backup-system">before</a>, my idea of backups comes down to these points:</p><ul>
<li>No direct access to backup storage from backed-up machine, no knowledge about backup storage layout there.</li>
<li>No any-time access from backup machine to backed-up one. Access should be granted on the basis of request from backed-up host, for one connection only.</li>
<li>Read-only access to filesystem only, no shell or network access.</li>
<li>Secure transfer channel.</li>
<li>Incremental, yet independent backups, retaining all fs metadata.</li>
<li>No extra strain on network (wireless) or local disk space.</li>
<li>Non-interactive usage (from cron).</li>
<li>No root involved on any side at any time.</li>
</ul>
<p>And the idea is to implement these with openssh, rsync and a pair of scripts.</p><p>Ok, the process is initiated by backed-up host, which will spawn sshd for single secure backup channel, so first thing to do is to invoke of ssh-keygen and get the pair of one-time keys from it.</p><p>As an extra precaution, there's no need to write private key to local filesystem, as it's only needed by ssh-client on a remote (backup) host. Funny thing is that ssh-keygen doesn't actually allow that, although it's possible to make it use fifo socket instead of file.<br />FIFO socket implies blocking I/O however, so one more precaution should be taken for script not to hang indefinitely.</p><p>A few convenience functions here and there are imported from <a title="fgc toolkit" href="http://fraggod.net/svc/git/fgc/">fgc</a> module, but can be replaced by standard counterparts (POpen, unlink, etc) without problem - no magic there.</p><p>Here we go:</p><pre name="code" class="python">def unjam(sig, frm):
	raise RuntimeError, 'no data from ssh-keygen'
signal.signal(signal.SIGALRM, unjam)

os.mkfifo(key)
keygen = exe.proc( 'ssh-keygen', '-q',
	'-t', 'rsa', '-b', '2048', '-N', '', '-f', key )

signal.alarm(5)
key_sub = open(key).read()
sh.rm(key, onerror=False)
if keygen.wait(): raise RuntimeError, 'ssh-keygen has failed'
signal.alarm(0)
</pre><p>Public key can then be used to generate one-time ACL file, aka "authorized_hosts" file:</p><pre name="code" class="python">keygen = open(key_pub, 'r').read().strip(spaces)
open(key_pub, 'w').write(
	'from="{0}" {1}\n'.format(remote_ip, keygen) )
</pre><p>So, we have an ACL file and matching private key. It's time to start sshd:</p><pre name="code" class="python">sshd = exe.proc( '/usr/sbin/sshd', '-6', '-de', '-p{0}'.format(port),
	'-oChallengeResponseAuthentication=no', # no password prompt
	'-oAllowAgentForwarding=no', # no need for this
	'-oAllowTcpForwarding=no', # no port-forwarding
	'-oPermitTunnel=no', # no tunneling
	'-oCompression=no', # minor speedup, since it's handled by rsync
	'-oForceCommand=/usr/bin/ppy {0} -c'\
		.format(os.path.realpath(__file__)), # enforce this script checks
	'-oAuthorizedKeysFile={0}'\
		.format(os.path.realpath(key_pub)), silent=True )
</pre><p>A bit of an explaination here.</p><p>"silent" keyword here just eats verbose stdout/stderr, since it's not needed for these purposes.</p><p>According to original plan, I use "ForceCommand" to start the same initiator-script (but with "-c" parameter), so it will invoke rsync (and rsync only) with some optional checks and scheduling priority enforcements.</p><p>Plus, since initial script and sshd are started by ordinary user, we'd need to get dac_read_search capability for rsync to be able to read (and only read) every single file on local filesystem. That's where <a title="Privileged python wrapper" href="http://fraggod.net/prj/ppy/">ppy binary</a> comes in, launching this script with additional capabilities, defined for the script file. Script itself doesn't need to make the caps effective - just pass as inherited further to rsync binary, and that's where it, and I mean cap_dac_read_search, should be activated and used.</p><p>To that end, system should have aforementioned wrapper (<a title="Privileged python wrapper" href="http://fraggod.net/prj/ppy/">ppy</a>) with permitted-effective caps, to provide them in the first place, python binary with "cap_dac_read_search=i" and rsync with "cap_dac_read_search=ei" (since it doesn't have option to activate caps from it's code). This may look like an awful lot of privileged bits, but it's absolutely not!<br />Inheritable caps is just that - inheritable, they won't get set by this bit by itself. In fact, one can think of whole fs as suid-inheritable, and here inheritance only works for a small fragment of root's power and that only for three files, w/o capability to propagnate anywhere else, if there'd be some exec in a bogus commandline.</p><p>Anyway, everything's set and ready for backup host to go ahead and grab local fs.</p><p>Note that backup of every file isn't really necessary, since sometimes most heavy ones are just caches, games or media content, readily available for downloading from the net, so I just glance at my fs with <a>xdiskusage</a> tool (which is awesome, btw, even for remote servers' df monitoring: "ssh remote du -k / | xdiskusage") to see if it's in need of cleanup and to add largest paths to backup-exclude list.</p><p>Actually, I thought of dynamically excluding pretty much everything that can be easily rebuilt by package manager ([http://www.gentoo.org/proj/en/portage/index.xml]portage in my case), but decided that I have space for these, and backing it all up makes "rm -f", updates or compiler errors (since I'm going to try <a title="Intel C Compiler" href="http://www.intel.com/software/products/compilers/clin/">icc</a>) much less scary anyway.</p><p>Ok, here goes the backup request:</p><pre name="code" class="python">ssh = exe.proc( 'ssh', remote,
	'{0}:{1}'.format(os.uname()[1], port), stdin=exe.PIPE )
ssh.stdin.write(key_sub)
ssh.stdin.write('\n\n\n')
ssh.stdin.write(open('/etc/bak_exclude').read())
ssh.stdin.close()

if ssh.wait(): raise RuntimeError, 'Remote call failed'
</pre><p>"remote" here is some unprivileged user on a backup host with backup-grab script set as a shell. Pubkey auth is used, so no interaction is required.</p><p>And that actually concludes locally-initiated operations - it's just wait to confirm that the task's completed.<br />Now backup host have the request, to-be-backed-up hostname and port on the commandline, with private key and paths-to-exclude list piped through.</p><p>One more thing done locally though is the invocation of this script when backup host will try to grab fs, but it's simple and straightforward as well:</p><pre name="code" class="python">cmd = os.getenv('SSH_ORIGINAL_COMMAND')
if not cmd: parser.error('No SSH_ORIGINAL_COMMAND in ENV')
if not re.match(
		r'^(ionice -c\d( -n\d)? )?rsync --server', cmd ):
	parser.error('Disallowed command: {0}'.format(cmd))
try: cmd, argz = cmd.split(' ', 1)
except ValueError: argz = ''
os.execlp(cmd, os.path.basename(cmd), *argz.split())
</pre><p>Rsync takes control from here and reads fs tree, checking files and their attributes against previous backups with it's handy rolling-checksums, creating hardlinks on match and transferring only mismatching pieces, if any, but more on that later, in the next post about implementation of the other side of this operation.</p><p>Full version of this script can be found <a title="Full version of the script" href="http://fraggod.net/oss/bin_scrz/fs_backup.py">here</a>.</p>-=|||=-<p>As I've already outlined <a title="Previous note on the subject" href="http://blog.fraggod.net/2010/2/My-simple-ok-not-quite-backup-system">before</a>, my idea of backups comes down to these points:</p><ul>
<li>No direct access to backup storage from backed-up machine, no knowledge about backup storage layout there.</li>
<li>No any-time access from backup machine to backed-up one. Access should be granted on the basis of request from backed-up host, for one connection only.</li>
<li>Read-only access to filesystem only, no shell or network access.</li>
<li>Secure transfer channel.</li>
<li>Incremental, yet independent backups, retaining all fs metadata.</li>
<li>No extra strain on network (wireless) or local disk space.</li>
<li>Non-interactive usage (from cron).</li>
<li>No root involved on any side at any time.</li>
</ul>
<p>And the idea is to implement these with openssh, rsync and a pair of scripts.</p><p>Ok, the process is initiated by backed-up host, which will spawn sshd for single secure backup channel, so first thing to do is to invoke of ssh-keygen and get the pair of one-time keys from it.</p><p>As an extra precaution, there's no need to write private key to local filesystem, as it's only needed by ssh-client on a remote (backup) host. Funny thing is that ssh-keygen doesn't actually allow that, although it's possible to make it use fifo socket instead of file.<br />FIFO socket implies blocking I/O however, so one more precaution should be taken for script not to hang indefinitely.</p><p>A few convenience functions here and there are imported from <a title="fgc toolkit" href="http://fraggod.net/svc/git/fgc/">fgc</a> module, but can be replaced by standard counterparts (POpen, unlink, etc) without problem - no magic there.</p><p>Here we go:</p><pre name="code" class="python">def unjam(sig, frm):<br />	raise RuntimeError, 'no data from ssh-keygen'<br />signal.signal(signal.SIGALRM, unjam)<br /><br />os.mkfifo(key)<br />keygen = exe.proc( 'ssh-keygen', '-q',<br />	'-t', 'rsa', '-b', '2048', '-N', '', '-f', key )<br /><br />signal.alarm(5)<br />key_sub = open(key).read()<br />sh.rm(key, onerror=False)<br />if keygen.wait(): raise RuntimeError, 'ssh-keygen has failed'<br />signal.alarm(0)<br /></pre><p>Public key can then be used to generate one-time ACL file, aka "authorized_hosts" file:</p><pre name="code" class="python">keygen = open(key_pub, 'r').read().strip(spaces)<br />open(key_pub, 'w').write(<br />	'from="{0}" {1}\n'.format(remote_ip, keygen) )<br /></pre><p>So, we have an ACL file and matching private key. It's time to start sshd:</p><pre name="code" class="python">sshd = exe.proc( '/usr/sbin/sshd', '-6', '-de', '-p{0}'.format(port),<br />	'-oChallengeResponseAuthentication=no', # no password prompt<br />	'-oAllowAgentForwarding=no', # no need for this<br />	'-oAllowTcpForwarding=no', # no port-forwarding<br />	'-oPermitTunnel=no', # no tunneling<br />	'-oCompression=no', # minor speedup, since it's handled by rsync<br />	'-oForceCommand=/usr/bin/ppy {0} -c'\<br />		.format(os.path.realpath(__file__)), # enforce this script checks<br />	'-oAuthorizedKeysFile={0}'\<br />		.format(os.path.realpath(key_pub)), silent=True )<br /></pre><p>A bit of an explaination here.</p><p>"silent" keyword here just eats verbose stdout/stderr, since it's not needed for these purposes.</p><p>According to original plan, I use "ForceCommand" to start the same initiator-script (but with "-c" parameter), so it will invoke rsync (and rsync only) with some optional checks and scheduling priority enforcements.</p><p>Plus, since initial script and sshd are started by ordinary user, we'd need to get dac_read_search capability for rsync to be able to read (and only read) every single file on local filesystem. That's where <a title="Privileged python wrapper" href="http://fraggod.net/prj/ppy/">ppy binary</a> comes in, launching this script with additional capabilities, defined for the script file. Script itself doesn't need to make the caps effective - just pass as inherited further to rsync binary, and that's where it, and I mean cap_dac_read_search, should be activated and used.</p><p>To that end, system should have aforementioned wrapper (<a title="Privileged python wrapper" href="http://fraggod.net/prj/ppy/">ppy</a>) with permitted-effective caps, to provide them in the first place, python binary with "cap_dac_read_search=i" and rsync with "cap_dac_read_search=ei" (since it doesn't have option to activate caps from it's code). This may look like an awful lot of privileged bits, but it's absolutely not!<br />Inheritable caps is just that - inheritable, they won't get set by this bit by itself. In fact, one can think of whole fs as suid-inheritable, and here inheritance only works for a small fragment of root's power and that only for three files, w/o capability to propagnate anywhere else, if there'd be some exec in a bogus commandline.</p><p>Anyway, everything's set and ready for backup host to go ahead and grab local fs.</p><p>Note that backup of every file isn't really necessary, since sometimes most heavy ones are just caches, games or media content, readily available for downloading from the net, so I just glance at my fs with <a>xdiskusage</a> tool (which is awesome, btw, even for remote servers' df monitoring: "ssh remote du -k / | xdiskusage") to see if it's in need of cleanup and to add largest paths to backup-exclude list.</p><p>Actually, I thought of dynamically excluding pretty much everything that can be easily rebuilt by package manager ([http://www.gentoo.org/proj/en/portage/index.xml]portage in my case), but decided that I have space for these, and backing it all up makes "rm -f", updates or compiler errors (since I'm going to try <a title="Intel C Compiler" href="http://www.intel.com/software/products/compilers/clin/">icc</a>) much less scary anyway.</p><p>Ok, here goes the backup request:</p><pre name="code" class="python">ssh = exe.proc( 'ssh', remote,<br />	'{0}:{1}'.format(os.uname()[1], port), stdin=exe.PIPE )<br />ssh.stdin.write(key_sub)<br />ssh.stdin.write('\n\n\n')<br />ssh.stdin.write(open('/etc/bak_exclude').read())<br />ssh.stdin.close()<br /><br />if ssh.wait(): raise RuntimeError, 'Remote call failed'<br /></pre><p>"remote" here is some unprivileged user on a backup host with backup-grab script set as a shell. Pubkey auth is used, so no interaction is required.</p><p>And that actually concludes locally-initiated operations - it's just wait to confirm that the task's completed.<br />Now backup host have the request, to-be-backed-up hostname and port on the commandline, with private key and paths-to-exclude list piped through.</p><p>One more thing done locally though is the invocation of this script when backup host will try to grab fs, but it's simple and straightforward as well:</p><pre name="code" class="python">cmd = os.getenv('SSH_ORIGINAL_COMMAND')<br />if not cmd: parser.error('No SSH_ORIGINAL_COMMAND in ENV')<br />if not re.match(<br />		r'^(ionice -c\d( -n\d)? )?rsync --server', cmd ):<br />	parser.error('Disallowed command: {0}'.format(cmd))<br />try: cmd, argz = cmd.split(' ', 1)<br />except ValueError: argz = ''<br />os.execlp(cmd, os.path.basename(cmd), *argz.split())<br /></pre><p>Rsync takes control from here and reads fs tree, checking files and their attributes against previous backups with it's handy rolling-checksums, creating hardlinks on match and transferring only mismatching pieces, if any, but more on that later, in the next post about implementation of the other side of this operation.</p><p>Full version of this script can be found <a title="Full version of the script" href="http://fraggod.net/oss/bin_scrz/fs_backup.py">here</a>.</p>-=||||||||||=-2010/2/Listening-to-music-over-the-net-with-authentication-and-cache-=|||=-2010-02-17T18:09:52-=|||=-Listening to music over the 'net with authentication and cache-=|||=-[u'Unix', u'NFS', u'Caching', u'SSH']-=|||=-<p>Having seen people really obsessed with the music, I don't consider myself to be much into it, yet I've managed to accumulate more than 70G of it, and counting. That's probably because I don't like to listen to something on a loop over and over, so, naturally, it's quite a bother to either keep the collection on every machine I use or copy the parts of it just to listen and replace.</p><p>Ideal solution for me is to mount whole hoard right from home server, and mounting it over the internet means that I need some kind of authentication.<br />Since I also use it at work, encryption is also nice, so I can always pass this bandwith as something work-friendly and really necessary, should it arouse any questions.<br />And while bandwith at work is pretty much unlimited, it's still controlled, so I wouldn't like to overuse it too much, and listening to oggs, mp3 and flacs for the whole work-day can generate traffic of 500-800 megs, and that's quite excessive to that end, in my own estimation.</p><p>The easiest solution for me was trusty sshfs - it's got the best authentication, nice performance and encryption off-the-shelf with just one simple command. Problem here is the last aforementioned point - sshfs would generate as much bandwith as possible, caching content only temporarily in volatile RAM.</p><p>Persistent caching seem to be quite easy to implement in userspace with either fuse layer over network filesystem or something even simplier (and more hacky), like aufs and inotify, catching IN_OPEN events and pulling files in question to intermediate layer of fs-union.</p><p>Another thing I've considered was fs-cache in-kernel mechanism, which appeared in the main tree since around 2.6.30, but the bad thing about was that while being quite efficient, it only worked for NFS or AFS. Second was clearly excessive for my purposes, and the first one I've come to hate for being extremely ureliable and limiting. In fact, NFS never gave me anything but trouble in the past, yet since I haven't found any decent implementations of the above ideas, I'd decided to give it (plus fs-cache) a try.</p><p>Setting up nfs server is no harder than sharing dir on windows - just write a line to /etc/exports and fire up nfs initscript. Since nfs4 seems superior than nfs in every way, I've used that version.<br />Trickier part is authentication. With nfs' "rape-me" auth model and kerberos being out of question, it has to be implemented on some transport layer in the middle.<br />Luckily, ssh is always there to provide a secure authenticated channel and nfs actually supports tcp these days. So the idea is to start nfs on localhost on server and use ssh tunnel to connecto to it from the client.</p><p>Setting up tunnel was quite straightforward, although I've put together a simple script to avoid re-typing the whole thing and to make sure there aren't any dead ssh processes laying around.</p><p style="font-family: yui-tmp;">#!/bin/sh<br /><br />PID="/tmp/.$(basename $0).$(echo "$1.$2" | md5sum | cut -b-5)"<br />touch "$PID"<br /><br />flock -n 3 3&lt;"$PID" || exit 0<br />exec 3&gt;"$PID"<br /><br />( flock -n 3 || exit 0<br /> exec ssh\<br />  -oControlPath=none\<br />  -oControlMaster=no\<br />  -oServerAliveInterval=3\<br />  -oServerAliveCountMax=5\<br />  -oConnectTimeout=5\<br />  -qyTnN $3 -L "$1" "$2" ) &amp;<br /><br />echo $! &gt;&amp;3<br />exit 0</p><p>That way, ssh process is daemonized right away. Simple locking is also implemented, based on tunnel and ssh destination, so it might be put as a cronjob (just like "ssh_tunnel 2049:127.0.0.1:2049 user@remote") to keep the link alive.</p><p>Then I've put a line like this to /etc/exports:</p><p style="font-family: yui-tmp;">/var/export/leech 127.0.0.1/32(ro,async,no_subtree_check,insecure)</p><p>...and tried to "mount -t nfs4 localhost:/ /mnt/music" on the remote. Guess what? "No such file or dir" error ;(</p><p>Ok, nfs3-way to "mount -t nfs4 localhost:/var/export/leech /mnt/music" doesn't work as well. No indication of why it is whatsoever.</p><p>Then it gets even better - "mount -t nfs localhost:/var/export/leech /mnt/music" actually works (locally, since nfs3 defaults to udp). Completely useless errors and nothing on the issue in manpages was quite weird, but prehaps I haven't looked at it well enough.</p><p>Gotcha was in the fact that it wasn't allowed to mount nfs4 root, so tweaking exports file like this...</p><p style="font-family: yui-tmp;">/var/export/leech 127.0.0.1/32(ro,async,no_subtree_check,insecure,fsid=0)<br />/var/export/leech/music 127.0.0.1/32(ro,async,no_subtree_check,insecure,fsid=1)</p><p>...and "mount -t nfs4 localhost:/music /mnt/music" actually solved the issue.</p><p>Why can't I use one-line exports and why the fuck it's not on the first (or any!) line of manpage escapes me completely, but at least it works now even from remote. Hallelujah.</p><p>Next thing is the cache layer. Luckily, it doesn't look as crappy as nfs and tying them together can be done with a single mount parameter. One extra thing needed, aside from the kernel part, here, is cachefilesd. Strange thing it's not in gentoo portage yet (since it's kinda necessary for kernel mechanism and quite aged already), but there's an <a title="cachefilesd ebuild submission" href="http://bugs.gentoo.org/show_bug.cgi?id=275014">ebuild in b.g.o</a> (now mirrored to <a>my overlay</a>, as well).</p><p>Setting it up is even simplier. Config is well-documented and consists of five lines only, the only relevant of which is the path to fs-backend, oh, and the last one seem to need user_xattr support enabled.</p><p>fstab lines for me were these:</p><p style="font-family: yui-tmp;">/dev/dump/nfscache /var/fscache ext4 user_xattr<br />localhost:/music /mnt/music nfs4 ro,nodev,noexec,intr,noauto,user,fsc</p><p>First two days got me 800+ megs in cache and from there it was even better bandwidth-wise, so, all-in-all, this nfs circus was worth it.</p><p>Another upside of nfs was that I could easily share it with workmates just by binding ssh tunnel endpoint to a non-local interface - all that's needed from them is to issue the mount command, although I didn't came to like to implementation any more than I did before.<br />Wonder if it's just me, but, whatever...</p>-=|||=-<p>Having seen people really obsessed with the music, I don't consider myself to be much into it, yet I've managed to accumulate more than 70G of it, and counting. That's probably because I don't like to listen to something on a loop over and over, so, naturally, it's quite a bother to either keep the collection on every machine I use or copy the parts of it just to listen and replace.</p><p>Ideal solution for me is to mount whole hoard right from home server, and mounting it over the internet means that I need some kind of authentication.<br />Since I also use it at work, encryption is also nice, so I can always pass this bandwith as something work-friendly and really necessary, should it arouse any questions.<br />And while bandwith at work is pretty much unlimited, it's still controlled, so I wouldn't like to overuse it too much, and listening to oggs, mp3 and flacs for the whole work-day can generate traffic of 500-800 megs, and that's quite excessive to that end, in my own estimation.</p><p>The easiest solution for me was trusty sshfs - it's got the best authentication, nice performance and encryption off-the-shelf with just one simple command. Problem here is the last aforementioned point - sshfs would generate as much bandwith as possible, caching content only temporarily in volatile RAM.</p><p>Persistent caching seem to be quite easy to implement in userspace with either fuse layer over network filesystem or something even simplier (and more hacky), like aufs and inotify, catching IN_OPEN events and pulling files in question to intermediate layer of fs-union.</p><p>Another thing I've considered was fs-cache in-kernel mechanism, which appeared in the main tree since around 2.6.30, but the bad thing about was that while being quite efficient, it only worked for NFS or AFS. Second was clearly excessive for my purposes, and the first one I've come to hate for being extremely ureliable and limiting. In fact, NFS never gave me anything but trouble in the past, yet since I haven't found any decent implementations of the above ideas, I'd decided to give it (plus fs-cache) a try.</p><p>Setting up nfs server is no harder than sharing dir on windows - just write a line to /etc/exports and fire up nfs initscript. Since nfs4 seems superior than nfs in every way, I've used that version.<br />Trickier part is authentication. With nfs' "rape-me" auth model and kerberos being out of question, it has to be implemented on some transport layer in the middle.<br />Luckily, ssh is always there to provide a secure authenticated channel and nfs actually supports tcp these days. So the idea is to start nfs on localhost on server and use ssh tunnel to connecto to it from the client.</p><p>Setting up tunnel was quite straightforward, although I've put together a simple script to avoid re-typing the whole thing and to make sure there aren't any dead ssh processes laying around.</p><p style="font-family: yui-tmp;">#!/bin/sh<br /><br />PID="/tmp/.$(basename $0).$(echo "$1.$2" | md5sum | cut -b-5)"<br />touch "$PID"<br /><br />flock -n 3 3&lt;"$PID" || exit 0<br />exec 3&gt;"$PID"<br /><br />( flock -n 3 || exit 0<br /> exec ssh\<br />  -oControlPath=none\<br />  -oControlMaster=no\<br />  -oServerAliveInterval=3\<br />  -oServerAliveCountMax=5\<br />  -oConnectTimeout=5\<br />  -qyTnN $3 -L "$1" "$2" ) &amp;<br /><br />echo $! &gt;&amp;3<br />exit 0</p><p>That way, ssh process is daemonized right away. Simple locking is also implemented, based on tunnel and ssh destination, so it might be put as a cronjob (just like "ssh_tunnel 2049:127.0.0.1:2049 user@remote") to keep the link alive.</p><p>Then I've put a line like this to /etc/exports:</p><p style="font-family: yui-tmp;">/var/export/leech 127.0.0.1/32(ro,async,no_subtree_check,insecure)</p><p>...and tried to "mount -t nfs4 localhost:/ /mnt/music" on the remote. Guess what? "No such file or dir" error ;(</p><p>Ok, nfs3-way to "mount -t nfs4 localhost:/var/export/leech /mnt/music" doesn't work as well. No indication of why it is whatsoever.</p><p>Then it gets even better - "mount -t nfs localhost:/var/export/leech /mnt/music" actually works (locally, since nfs3 defaults to udp). Completely useless errors and nothing on the issue in manpages was quite weird, but prehaps I haven't looked at it well enough.</p><p>Gotcha was in the fact that it wasn't allowed to mount nfs4 root, so tweaking exports file like this...</p><p style="font-family: yui-tmp;">/var/export/leech 127.0.0.1/32(ro,async,no_subtree_check,insecure,fsid=0)<br />/var/export/leech/music 127.0.0.1/32(ro,async,no_subtree_check,insecure,fsid=1)</p><p>...and "mount -t nfs4 localhost:/music /mnt/music" actually solved the issue.</p><p>Why can't I use one-line exports and why the fuck it's not on the first (or any!) line of manpage escapes me completely, but at least it works now even from remote. Hallelujah.</p><p>Next thing is the cache layer. Luckily, it doesn't look as crappy as nfs and tying them together can be done with a single mount parameter. One extra thing needed, aside from the kernel part, here, is cachefilesd. Strange thing it's not in gentoo portage yet (since it's kinda necessary for kernel mechanism and quite aged already), but there's an <a title="cachefilesd ebuild submission" href="http://bugs.gentoo.org/show_bug.cgi?id=275014">ebuild in b.g.o</a> (now mirrored to <a>my overlay</a>, as well).</p><p>Setting it up is even simplier. Config is well-documented and consists of five lines only, the only relevant of which is the path to fs-backend, oh, and the last one seem to need user_xattr support enabled.</p><p>fstab lines for me were these:</p><p style="font-family: yui-tmp;">/dev/dump/nfscache /var/fscache ext4 user_xattr<br />localhost:/music /mnt/music nfs4 ro,nodev,noexec,intr,noauto,user,fsc</p><p>First two days got me 800+ megs in cache and from there it was even better bandwidth-wise, so, all-in-all, this nfs circus was worth it.</p><p>Another upside of nfs was that I could easily share it with workmates just by binding ssh tunnel endpoint to a non-local interface - all that's needed from them is to issue the mount command, although I didn't came to like to implementation any more than I did before.<br />Wonder if it's just me, but, whatever...</p>-=||||||||||=-2010/2/libnotify-notification-daemon-shortcomings-and-my-solution-=|||=-2010-02-26T16:44:54-=|||=-libnotify, notification-daemon shortcomings and my solution-=|||=-[u'Python', u'Desktop', u'Unix', u'Notification', u'Rate-limiting']-=|||=-<p>Everyone who uses OSS desktop these days probably seen <a title="libnotify home" href="http://www.galago-project.org/">libnotify</a> magic in action - small popup windows that appear at some corner of the screen, announcing events from other apps.</p><p>libnotify itself, however, is just a convenience lib for dispatching these notifications over dbus, so the latter can pass it app listening on this interface or even start it beforehand.<br />Standard app for rendering such messages is notification-daemon, which is developed alongside with libnotify, but there are drop-in replacements like <a title="xfce4-notifyd home" href="http://spuriousinterrupt.org/projects/xfce4-notifyd">xfce4-notifyd</a> or <a title="e17 home" href="http://www.enlightenment.org/">e17 notification module</a>.<br />In dbus rpc mechanism call signatures are clearly defined and visible, so it's pretty easy to implement replacement for aforementioned daemons, plus vanilla notification-daemon has introspection calls and dbus itself can be easily monitored (via dbus-monitor utility) which make it's implementation even more transparent.</p><p>Now, polling every window for updates manually is quite inefficient - new mail, xmpp messages, IRC chat lines, system events etc sometimes arrive every few seconds, and going over all the windows (and by that I mean workspaces where they're stacked) just to check them is a huge waste of time, especially when some (or even most, in case of IRC) of these are not really important.<br />Either response time or focus (and, in extreme case, sanity) has to be sacrificed in such approach. Luckily, there's another way to monitor this stuff - small pop-up notifications allow to see what's happening right away, w/o much attention-switching or work required from an end-user.</p><p>But that's the theory.<br />In practice, I've found that enabling notifications in IRC or jabber is pretty much pointless, since you'll be swarmed by these as soon as any real activity starts there. And w/o them it's a stupid wasteful poll practice, mentioned above.<br />Notification-daemon has no tricks to remedy the situation, but since the whole thing is so abstract and transparent I've had no problem making my own fix.</p><p><img alt="Notification digest" title="Notification digest" style="width: 300px; height: 476px;" src="http://blog.fraggod.net/static/embed/notification_proxy_digest.jpg" align="right" />Solution I've came up with is to batch the notification messages into a digests as soon as there are too many of them, displaying such digest pop-ups with some time interval, so I can keep a grip on what's going on just by glancing at these as they arrive, switching my activities if something there is important enough.</p><p>Having played with schedulers and network shaping/policing before, not much imagination was required to devise a way to control the message flow rate.<br />I chose <a title="token bucket algorithm (wiki)" href="http://en.wikipedia.org/wiki/Token_bucket">token-bucket algorithm</a> at first, but since prolonged flood of I-don't-care-about activity have gradually decreasing value, I didn't want to receive digests of it every N seconds, so I batched it with a gradual digest interval increase and <a title="leaky bucket algorithm (wiki)" href="http://en.wikipedia.org/wiki/Leaky_bucket">leaky-bucket</a> mechanism, so digests won't get too fat over these intervals.<br />Well, the result exceeded my expectations, and now I can use libnotify freely even to indicate that some rsync just finished in a terminal on another workspace. Wonder why such stuff isn't built into existing notification daemons...</p><p>Then, there was another, even more annoying issue: notifications during fullscreen apps! WTF!?<br />Wonder if everyone got used to this ugly flickering in fullscreen mplayer, huge lags in GL games like SpringRTS or I'm just re-inventing the wheel here, since it's done in gnome or kde (knotify, huh?), but since I'm not gonna use either one I just added <a title="check code" href="http://fraggod.net/svc/git/fgc/tree/fgc/wm.py#n99">fullscreen-app check</a> before notification output, queueing them to digest if that is the case.</p><p>Ok, a few words about implementation.<br />Token bucket itself is based on <a title="simple token bucket implementation" href="http://code.activestate.com/recipes/511490/">activestate recipe</a> with some heavy improvements to adjust flow on constant under/over-flow, plus with a bit more pythonic style and features, take a look <a title="token bucket revamp" href="http://fraggod.net/svc/git/fgc/tree/fgc/fc.py#n64">here</a>. Leaky bucket implemented by <a title="leaky bucket implementation" href="http://fraggod.net/svc/git/fgc/tree/fgc/fc.py#n34">this class</a>.<br />Aside from that it's just dbus magic and a <a title="lots of cli options ;)" href="http://fraggod.net/svc/git/fg_overlay/tree/x11-misc/notification-daemon/files/notification-proxy#n20">quite extensive CLI interface</a> to control the filters.</p><p>Main dbus magic, however, lies outside the script, since dbus calls cannot be intercepted and the scheduler can't get'em with notification-daemon already listening on this interface.<br />Solution is easy, of course - scheduler can <a title="dbus service description file" href="http://fraggod.net/svc/git/fg_overlay/tree/x11-misc/notification-daemon/files/org.freedesktop.Notifications.service">replace the real daemon</a> and proxy mangled calls to it as necessary. It takes <a title="patching sources: the sed way" href="http://fraggod.net/svc/git/fg_overlay/tree/x11-misc/notification-daemon/notification-daemon-0.4.0-r3.ebuild#n41">this sed line</a> for notification-daemon as well, since interface is hard-coded there.</p><p>Grab the script <a title="notification proxy script" href="http://fraggod.net/oss/projects/notification-proxy.py">here</a>. Needs <a title="fgc toolkit" href="http://fraggod.net/svc/git/fgc/">fgc module</a>, but it's just a hundred lines on meaningful code, ffs.</p><p>One more step to making linux desktop more comfortable. Oh, joy ;)</p>-=|||=-<p>Everyone who uses OSS desktop these days probably seen <a title="libnotify home" href="http://www.galago-project.org/">libnotify</a> magic in action - small popup windows that appear at some corner of the screen, announcing events from other apps.</p><p>libnotify itself, however, is just a convenience lib for dispatching these notifications over dbus, so the latter can pass it app listening on this interface or even start it beforehand.<br />Standard app for rendering such messages is notification-daemon, which is developed alongside with libnotify, but there are drop-in replacements like <a title="xfce4-notifyd home" href="http://spuriousinterrupt.org/projects/xfce4-notifyd">xfce4-notifyd</a> or <a title="e17 home" href="http://www.enlightenment.org/">e17 notification module</a>.<br />In dbus rpc mechanism call signatures are clearly defined and visible, so it's pretty easy to implement replacement for aforementioned daemons, plus vanilla notification-daemon has introspection calls and dbus itself can be easily monitored (via dbus-monitor utility) which make it's implementation even more transparent.</p><p>Now, polling every window for updates manually is quite inefficient - new mail, xmpp messages, IRC chat lines, system events etc sometimes arrive every few seconds, and going over all the windows (and by that I mean workspaces where they're stacked) just to check them is a huge waste of time, especially when some (or even most, in case of IRC) of these are not really important.<br />Either response time or focus (and, in extreme case, sanity) has to be sacrificed in such approach. Luckily, there's another way to monitor this stuff - small pop-up notifications allow to see what's happening right away, w/o much attention-switching or work required from an end-user.</p><p>But that's the theory.<br />In practice, I've found that enabling notifications in IRC or jabber is pretty much pointless, since you'll be swarmed by these as soon as any real activity starts there. And w/o them it's a stupid wasteful poll practice, mentioned above.<br />Notification-daemon has no tricks to remedy the situation, but since the whole thing is so abstract and transparent I've had no problem making my own fix.</p><p><img alt="Notification digest" title="Notification digest" style="width: 300px; height: 476px;" src="http://blog.fraggod.net/static/embed/notification_proxy_digest.jpg" align="right" />Solution I've came up with is to batch the notification messages into a digests as soon as there are too many of them, displaying such digest pop-ups with some time interval, so I can keep a grip on what's going on just by glancing at these as they arrive, switching my activities if something there is important enough.</p><p>Having played with schedulers and network shaping/policing before, not much imagination was required to devise a way to control the message flow rate.<br />I chose <a title="token bucket algorithm (wiki)" href="http://en.wikipedia.org/wiki/Token_bucket">token-bucket algorithm</a> at first, but since prolonged flood of I-don't-care-about activity have gradually decreasing value, I didn't want to receive digests of it every N seconds, so I batched it with a gradual digest interval increase and <a title="leaky bucket algorithm (wiki)" href="http://en.wikipedia.org/wiki/Leaky_bucket">leaky-bucket</a> mechanism, so digests won't get too fat over these intervals.<br />Well, the result exceeded my expectations, and now I can use libnotify freely even to indicate that some rsync just finished in a terminal on another workspace. Wonder why such stuff isn't built into existing notification daemons...</p><p>Then, there was another, even more annoying issue: notifications during fullscreen apps! WTF!?<br />Wonder if everyone got used to this ugly flickering in fullscreen mplayer, huge lags in GL games like SpringRTS or I'm just re-inventing the wheel here, since it's done in gnome or kde (knotify, huh?), but since I'm not gonna use either one I just added <a title="check code" href="http://fraggod.net/svc/git/fgc/tree/fgc/wm.py#n99">fullscreen-app check</a> before notification output, queueing them to digest if that is the case.</p><p>Ok, a few words about implementation.<br />Token bucket itself is based on <a title="simple token bucket implementation" href="http://code.activestate.com/recipes/511490/">activestate recipe</a> with some heavy improvements to adjust flow on constant under/over-flow, plus with a bit more pythonic style and features, take a look <a title="token bucket revamp" href="http://fraggod.net/svc/git/fgc/tree/fgc/fc.py#n64">here</a>. Leaky bucket implemented by <a title="leaky bucket implementation" href="http://fraggod.net/svc/git/fgc/tree/fgc/fc.py#n34">this class</a>.<br />Aside from that it's just dbus magic and a <a title="lots of cli options ;)" href="http://fraggod.net/svc/git/fg_overlay/tree/x11-misc/notification-daemon/files/notification-proxy#n20">quite extensive CLI interface</a> to control the filters.</p><p>Main dbus magic, however, lies outside the script, since dbus calls cannot be intercepted and the scheduler can't get'em with notification-daemon already listening on this interface.<br />Solution is easy, of course - scheduler can <a title="dbus service description file" href="http://fraggod.net/svc/git/fg_overlay/tree/x11-misc/notification-daemon/files/org.freedesktop.Notifications.service">replace the real daemon</a> and proxy mangled calls to it as necessary. It takes <a title="patching sources: the sed way" href="http://fraggod.net/svc/git/fg_overlay/tree/x11-misc/notification-daemon/notification-daemon-0.4.0-r3.ebuild#n41">this sed line</a> for notification-daemon as well, since interface is hard-coded there.</p><p>Grab the script <a title="notification proxy script" href="http://fraggod.net/oss/projects/notification-proxy.py">here</a>. Needs <a title="fgc toolkit" href="http://fraggod.net/svc/git/fgc/">fgc module</a>, but it's just a hundred lines on meaningful code, ffs.</p><p>One more step to making linux desktop more comfortable. Oh, joy ;)</p>-=||||||||||=-2010/2/snmpd-pyagentx-or-re-discovery-of-sfnet-=|||=-2010-02-28T14:49:02-=|||=-snmpd-pyagentx or re-discovery of sf.net-=|||=-[u'Python', u'SysAdmin', u'Web']-=|||=-<p>Since I've put some two-day effort into creation of <a title="Net-SNMP project" href="http://net-snmp.sourceforge.net/">net-snmp</a> snmpd extension and had some free time to report bug in <a title="python-agentx project" href="http://sourceforge.net/projects/python-agentx/">source of this inspiration</a>, thought I might as well save someone trouble of re-inventing the wheel and publish it somewhere, since snmpd extension definitely looks like a black area from python perspective.</p><p>I've sampled <a title="SourceForge.net" href="http://sf.net/">sf.net</a> as a project admin before, publishing some crappy php code for <a title="hyscar project" href="http://sourceforge.net/projects/hyscar/">hyscar project</a> with pretty much the same reasons in mind, and I didn't like the experience much - cvs for code storage and weird interface are among the reasons I can remember, but I'll gladly take all this criticism back - current interface has by far exceed all my expectations (well, prehaps they were too low in the first place?).<br />Putting up a full-fledged project page took me (a complete n00b at that) about half an hour, everything being simple and obvious as it is, native-to-me <a title="git vcs home" href="http://www.git-scm.com/">git vcs</a>, and even <a title="Trac project" href="http://trac.edgewall.com/">trac</a> among the (numerous) features. Damn pleasant xp, making you wanna upload something else just for the sake of it ;)</p><p>Oh, and the project is <a title="snmpd-pyagentx project itself" href="http://sourceforge.net/projects/snmpd-pyagentx/">snmpd-pyagentx</a>, <a title="Freshmeat page" href="http://freshmeat.net/projects/snmpd-pyagentx">freshmeat page</a> included.<br />Just an alpha right now, but I'll polish and deploy it in production in a day or two, so no worries.</p>-=|||=-<p>Since I've put some two-day effort into creation of <a title="Net-SNMP project" href="http://net-snmp.sourceforge.net/">net-snmp</a> snmpd extension and had some free time to report bug in <a title="python-agentx project" href="http://sourceforge.net/projects/python-agentx/">source of this inspiration</a>, thought I might as well save someone trouble of re-inventing the wheel and publish it somewhere, since snmpd extension definitely looks like a black area from python perspective.</p><p>I've sampled <a title="SourceForge.net" href="http://sf.net/">sf.net</a> as a project admin before, publishing some crappy php code for <a title="hyscar project" href="http://sourceforge.net/projects/hyscar/">hyscar project</a> with pretty much the same reasons in mind, and I didn't like the experience much - cvs for code storage and weird interface are among the reasons I can remember, but I'll gladly take all this criticism back - current interface has by far exceed all my expectations (well, prehaps they were too low in the first place?).<br />Putting up a full-fledged project page took me (a complete n00b at that) about half an hour, everything being simple and obvious as it is, native-to-me <a title="git vcs home" href="http://www.git-scm.com/">git vcs</a>, and even <a title="Trac project" href="http://trac.edgewall.com/">trac</a> among the (numerous) features. Damn pleasant xp, making you wanna upload something else just for the sake of it ;)</p><p>Oh, and the project is <a title="snmpd-pyagentx project itself" href="http://sourceforge.net/projects/snmpd-pyagentx/">snmpd-pyagentx</a>, <a title="Freshmeat page" href="http://freshmeat.net/projects/snmpd-pyagentx">freshmeat page</a> included.<br />Just an alpha right now, but I'll polish and deploy it in production in a day or two, so no worries.</p>-=||||||||||=-2010/3/Single-instance-daemon-or-invisible-dock-=|||=-2010-03-10T02:41:43-=|||=-Single-instance daemon or "invisible dock"-=|||=-[u'Python', u'Desktop']-=|||=-<p>Docks.<br />You always have the touch-sensitive, solid, reliable dock right under your hands - the keyboard, so what's the point of docks?</p><ul>
<li>Mouse-user-friendly</li>
<li>Look cool (cairo-dock, kiba-dock, macosx)</li>
<li>Provide control over the launched instances of each app</li>
</ul>
<p>Two first points I don't care much about, but the last one sounds really nice - instead of switching to app workspace, you can just push the same hotkey and it'll even be raised for you in case WS is messed up with stacked windows.<br />Kinda excessive to install a full-fledged dock for just that, besides it'd eat screen space and resources for no good reason, so I made my own "dock".</p><p>But it's not really a "dock", since it's actually invisible and basically is just a wrapper for launched commands to check if last process spawned by identical command exists and just bring it to foreground in this case.</p><p>For reliable monitoring of spawned processes there has to be a daemon and wrappers should relay either command (and env) or spawned process info to it, which inplies some sort of IPC.<br />Choosing dbus as that IPC handles the details like watcher-daemon starting and serialization of data and makes the wrapper itself quite liteweight:</p><pre name="code" class="python">#!/usr/bin/env python
# -*- coding: utf-8 -*-

dbus_iface = 'net.fraggod.SID'
dbus_path = '/net/fraggod/SID'

import os, sys, dbus
sid = dbus.SessionBus().get_object(dbus_iface, dbus_path)

if sys.argv[1][0] != '/':
	for path in os.getenv('PATH').split(os.pathsep):
		path = os.path.join(path, sys.argv[1])
		if os.path.exists(path):
			sys.argv[1] = path
			break

sid.instance_request(sys.argv[1:], dict(os.environ))
</pre><p>And that's it, most of these just resolves binary location via PATH so it can be used as unique-index in daemon process right off the pipe.</p><p>Daemonized part of the scheme just takes the instance off it's stack, fires up a new one or returs back some error message:</p><pre name="code" class="python">@dbus.service.method( dbus_iface,
	in_signature='asa{ss}', out_signature='s' )
def instance_request(self, argz, env):
	try:
		data = self.pop_instance(argz, env)
		return data if data else ''
	except Exception, err: return 'ERROR: {0}'.format(err)

def pop_instance(self, argz, env):
	ps = argz[0]
	log.info('InstanceRequest: {0}'.format(argz))
	if ps[0] != '/': raise TypeError, 'App path must be absolute'
	ps = os.path.realpath(ps)
	log.debug('Pulling out "{0}"'.format(ps))
	try:
		app = self.apps[ps]
		log.debug('App "{0}" exists, pulling to fg'.format(ps))
		app.show()
	except KeyError:
		log.debug('No app "{0}", starting'.format(ps))
		self.apps[ps] = AppInstance(argz, env, self.log)
		return 'Started'
</pre><p>Dead apps are collected on SIGCHLD and some extra precautions should be taken for the case when the signal arrives during the collector code execution, like when several apps die simultaneously:</p><pre name="code" class="python">def reap_apps(self, sig, frm):
	log.debug('Got app exit signal')
	try:
		locked = self.lock.acquire(False)
		self.lock_req = True # indicates that apps have to be re-checked
		if not locked:
			log.debug('Reap is in progress, re-check scheduled')
			return

		while self.lock_req:
			self.lock_req = False
			log.debug('Reaping dead apps')
			for k,app in self.apps.iteritems():
				if app.dead:
					del self.apps[k]
					log.debug('App "{0}" was released'.format(k))

	finally:
		if locked: self.lock.release()
		global loop_interrupt
		loop_interrupt = True
		log.debug('Reaper done')
</pre><p>That way, collector should run until signals stop arriving and shouldn't miss any app under any circumstances.</p><p>AppInstance objects incapsulate all operations concerning each app from starting it to focus and waitpid:</p><pre name="code" class="python">class AppInstance(object):
	_id = None # for debugging purposes only
	_ps = _win = None

	def __init__(self, argz, env, logfile=False):
		log.debug('Creating instance with argz: {0}'.format(argz))
		self._id = argz[0]
		self._ps = exe.proc( *argz,
			preexec_fn=os.setsid, env=env,
			stdout=logfile, stderr=exe.STDOUT, stdin=False )

	def show(self):
		if self.windows:
			for win in self.windows: win.focus()
		else: log.debug('No window for app "{0}"'.format(self._id))

	@property
	def windows(self):
		if self._win is None:
			self._win = wm.Window.by_pid(self._ps.pid)
			if self._win: self._win = list(self._win) # all windows for pid
			else: self._win = False
		return self._win

	@property
	def dead(self):
		return self._ps.wait(0) is not None
</pre><p>WM ops here are from <a title="fgc toolkit" href="http://fraggod.net/svc/git/fgc/">fgc package</a>.</p><p>From here all that's left to code is to create dbus-handler instance and get the loop running.<br />I called the daemon itself as "sid" and the wrapper as "sic".</p><p>To make dbus aware of the service, short note should be put to <em>/usr/share/dbus-1/services/net.fraggod.SID.service</em> with path to daemon binary:</p>[D-BUS Service]<br />Name=net.fraggod.SID<br />Exec=/usr/libexec/sid<br /><p>...plus the hotkeys rebound from "myapp" to just "sic myapp" and the key-dock is ready.</p><p>Works especially well with WMs that can keep app windows' props between launches, so just pressing the relevant keys should launch every app where it belongs with correct window parameters and you won't have to do any WM-related work at all.</p><p>Code: <a title="client part" href="http://fraggod.net/oss/projects/sic.py">sic.py</a> <a title="daemon part" href="http://fraggod.net/oss/projects/sid.py">sid.py</a></p><p>What can be more user-friendly than that? Gotta think about it...</p>-=|||=-<p>Docks.<br />You always have the touch-sensitive, solid, reliable dock right under your hands - the keyboard, so what's the point of docks?</p><ul>
<li>Mouse-user-friendly</li>
<li>Look cool (cairo-dock, kiba-dock, macosx)</li>
<li>Provide control over the launched instances of each app</li>
</ul>
<p>Two first points I don't care much about, but the last one sounds really nice - instead of switching to app workspace, you can just push the same hotkey and it'll even be raised for you in case WS is messed up with stacked windows.<br />Kinda excessive to install a full-fledged dock for just that, besides it'd eat screen space and resources for no good reason, so I made my own "dock".</p><p>But it's not really a "dock", since it's actually invisible and basically is just a wrapper for launched commands to check if last process spawned by identical command exists and just bring it to foreground in this case.</p><p>For reliable monitoring of spawned processes there has to be a daemon and wrappers should relay either command (and env) or spawned process info to it, which inplies some sort of IPC.<br />Choosing dbus as that IPC handles the details like watcher-daemon starting and serialization of data and makes the wrapper itself quite liteweight:</p><pre name="code" class="python">#!/usr/bin/env python<br /># -*- coding: utf-8 -*-<br /><br />dbus_iface = 'net.fraggod.SID'<br />dbus_path = '/net/fraggod/SID'<br /><br />import os, sys, dbus<br />sid = dbus.SessionBus().get_object(dbus_iface, dbus_path)<br /><br />if sys.argv[1][0] != '/':<br />	for path in os.getenv('PATH').split(os.pathsep):<br />		path = os.path.join(path, sys.argv[1])<br />		if os.path.exists(path):<br />			sys.argv[1] = path<br />			break<br /><br />sid.instance_request(sys.argv[1:], dict(os.environ))<br /></pre><p>And that's it, most of these just resolves binary location via PATH so it can be used as unique-index in daemon process right off the pipe.</p><p>Daemonized part of the scheme just takes the instance off it's stack, fires up a new one or returs back some error message:</p><pre name="code" class="python">@dbus.service.method( dbus_iface,<br />	in_signature='asa{ss}', out_signature='s' )<br />def instance_request(self, argz, env):<br />	try:<br />		data = self.pop_instance(argz, env)<br />		return data if data else ''<br />	except Exception, err: return 'ERROR: {0}'.format(err)<br /><br />def pop_instance(self, argz, env):<br />	ps = argz[0]<br />	log.info('InstanceRequest: {0}'.format(argz))<br />	if ps[0] != '/': raise TypeError, 'App path must be absolute'<br />	ps = os.path.realpath(ps)<br />	log.debug('Pulling out "{0}"'.format(ps))<br />	try:<br />		app = self.apps[ps]<br />		log.debug('App "{0}" exists, pulling to fg'.format(ps))<br />		app.show()<br />	except KeyError:<br />		log.debug('No app "{0}", starting'.format(ps))<br />		self.apps[ps] = AppInstance(argz, env, self.log)<br />		return 'Started'<br /></pre><p>Dead apps are collected on SIGCHLD and some extra precautions should be taken for the case when the signal arrives during the collector code execution, like when several apps die simultaneously:</p><pre name="code" class="python">def reap_apps(self, sig, frm):<br />	log.debug('Got app exit signal')<br />	try:<br />		locked = self.lock.acquire(False)<br />		self.lock_req = True # indicates that apps have to be re-checked<br />		if not locked:<br />			log.debug('Reap is in progress, re-check scheduled')<br />			return<br /><br />		while self.lock_req:<br />			self.lock_req = False<br />			log.debug('Reaping dead apps')<br />			for k,app in self.apps.iteritems():<br />				if app.dead:<br />					del self.apps[k]<br />					log.debug('App "{0}" was released'.format(k))<br /><br />	finally:<br />		if locked: self.lock.release()<br />		global loop_interrupt<br />		loop_interrupt = True<br />		log.debug('Reaper done')<br /></pre><p>That way, collector should run until signals stop arriving and shouldn't miss any app under any circumstances.</p><p>AppInstance objects incapsulate all operations concerning each app from starting it to focus and waitpid:</p><pre name="code" class="python">class AppInstance(object):<br />	_id = None # for debugging purposes only<br />	_ps = _win = None<br /><br />	def __init__(self, argz, env, logfile=False):<br />		log.debug('Creating instance with argz: {0}'.format(argz))<br />		self._id = argz[0]<br />		self._ps = exe.proc( *argz,<br />			preexec_fn=os.setsid, env=env,<br />			stdout=logfile, stderr=exe.STDOUT, stdin=False )<br /><br />	def show(self):<br />		if self.windows:<br />			for win in self.windows: win.focus()<br />		else: log.debug('No window for app "{0}"'.format(self._id))<br /><br />	@property<br />	def windows(self):<br />		if self._win is None:<br />			self._win = wm.Window.by_pid(self._ps.pid)<br />			if self._win: self._win = list(self._win) # all windows for pid<br />			else: self._win = False<br />		return self._win<br /><br />	@property<br />	def dead(self):<br />		return self._ps.wait(0) is not None<br /></pre><p>WM ops here are from <a title="fgc toolkit" href="http://fraggod.net/svc/git/fgc/">fgc package</a>.</p><p>From here all that's left to code is to create dbus-handler instance and get the loop running.<br />I called the daemon itself as "sid" and the wrapper as "sic".</p><p>To make dbus aware of the service, short note should be put to <em>/usr/share/dbus-1/services/net.fraggod.SID.service</em> with path to daemon binary:</p>[D-BUS Service]<br />Name=net.fraggod.SID<br />Exec=/usr/libexec/sid<br /><p>...plus the hotkeys rebound from "myapp" to just "sic myapp" and the key-dock is ready.</p><p>Works especially well with WMs that can keep app windows' props between launches, so just pressing the relevant keys should launch every app where it belongs with correct window parameters and you won't have to do any WM-related work at all.</p><p>Code: <a title="client part" href="http://fraggod.net/oss/projects/sic.py">sic.py</a> <a title="daemon part" href="http://fraggod.net/oss/projects/sid.py">sid.py</a></p><p>What can be more user-friendly than that? Gotta think about it...</p>-=||||||||||=-2010/4/Auto-away-for-pidgin-=|||=-2010-04-10T06:02:06-=|||=-Auto-away for pidgin-=|||=-[u'Python', u'Desktop', u'IM']-=|||=-<p>Lately I've migrated back to <a title="pidgin project site" href="http://pidgin.im/">pidgin</a> from <a title="gajim project site" href="http://www.gajim.org/">gajim</a> through <a title="emacs-jabber project site" href="http://emacs-jabber.sourceforge.net/">jabber.el</a>. The thing which made it necessary was XFire support (via <a title="gfire project site" href="http://gfireproject.org/">gfire plugin</a>), which I've needed to communicate w/ my <a title="Spring RTS site" href="http://springrts.com/">spring</a> clanmates.<br />I'd have preferred jabber-xfire transport instead, but most projects there look abandoned and I don't really need extensive jabber-features support, so pidgin is fine with me.</p><p>The only thing that's not working there is auto-away support, so it can change my status due to inactivity.<br />Actually it changes the status to "away" but for no reason at all, regardless of idle time, and refuses to set it back to active even when I click it's window and options.</p><p>Well, good thing is that pidgin's mature enough to have dbus interface, so as the most problems in life, this one can be solved with python ;)</p><p>First thing to check is <a title="pidgin-dbus reference" href="http://developer.pidgin.im/wiki/DbusHowto">pidgin dbus interface</a> and figure out how the states work there: you have to create a "state" with the appropriate message or find it among stored ones then set it as active, storing id of the previous one.</p><p>Next thing is to determine a way to get idle time.<br />Luckily, X keeps track of activity and I've already used <a title="xprintidle utility" href="http://www.dtek.chalmers.se/%7Ehenoch/text/xprintidle.html">xprintidle</a> with jabber.el, so it's not a problem.<br />Not quite a native py solution, but it has workaround for <a title="Xorg idle time bug" href="https://bugs.freedesktop.org/buglist.cgi?quicksearch=6439">one bug</a> and is much more liteweight than code using py-xlib.</p><p>From there it's just an endless sleep/check loop with occasional dbus calls.<br />One gotcha there is that pidgin can die or be closed, so the loop has to deal with this as well.</p><p>All there is...</p><p>Get idle time:</p><pre name="code" class="python">def get_idle():
 proc = Popen(['xprintidle'], stdout=PIPE)
 idle = int(proc.stdout.read().strip()) // 1000
 proc.wait()
 return idle
</pre><p>Simple dbus client code:</p><pre name="code" class="python">pidgin = dbus.SessionBus().get_object(
 'im.pidgin.purple.PurpleService', '/im/pidgin/purple/PurpleObject' )
iface = dbus.Interface(pidgin, 'im.pidgin.purple.PurpleInterface')
</pre><p>Get initial (available) status:</p><pre name="code" class="python">st = iface.PurpleSavedstatusGetCurrent()
st_type = iface.PurpleSavedstatusGetType(st)
st_msg = iface.PurpleSavedstatusGetMessage(st)
</pre><p>Create away/na statuses:</p><pre name="code" class="python">st_away = iface.PurpleSavedstatusNew('', status.away)
iface.PurpleSavedstatusSetMessage(
 st_away, 'AFK (>{0}m)'.format(optz.away // 60) )
st_na = iface.PurpleSavedstatusNew('', status.xaway)
iface.PurpleSavedstatusSetMessage(
 st_na, 'AFK for quite a while (>{0}m)'.format(optz.na // 60) )
</pre><p>And the main loop:</p><pre name="code" class="python">while True:
 idle = get_idle()
 if idle > optz.away:
  if idle > optz.na:
   iface.PurpleSavedstatusActivate(st_na)
   log.debug('Switched status to N/A (idle: {0})'.format(idle//60))
  else:
   iface.PurpleSavedstatusActivate(st_away)
   log.debug('Switched status to away (idle: {0})'.format(idle//60))
  sleep(optz.poll)
 else:
  if iface.PurpleSavedstatusGetType(
    iface.PurpleSavedstatusGetCurrent() ) in (status.away, status.xaway):
   iface.PurpleSavedstatusActivate(st)
   log.debug('Restored original status (idle: {0})'.format(idle//60))
  sleep(optz.away)
</pre><p>Bonus of such approach is that any other checks can be easily added - fullscreen-video-status, for example, or emacs-dont-disturb status. I bet there are other plugins for this purposes, but I'd prefer few lines of clean py to some buggy .so anytime ;)</p><p><a title="Code" href="http://fraggod.net/oss/projects/status_watcher.py">Here's the full code</a>.</p>-=|||=-<p>Lately I've migrated back to <a title="pidgin project site" href="http://pidgin.im/">pidgin</a> from <a title="gajim project site" href="http://www.gajim.org/">gajim</a> through <a title="emacs-jabber project site" href="http://emacs-jabber.sourceforge.net/">jabber.el</a>. The thing which made it necessary was XFire support (via <a title="gfire project site" href="http://gfireproject.org/">gfire plugin</a>), which I've needed to communicate w/ my <a title="Spring RTS site" href="http://springrts.com/">spring</a> clanmates.<br />I'd have preferred jabber-xfire transport instead, but most projects there look abandoned and I don't really need extensive jabber-features support, so pidgin is fine with me.</p><p>The only thing that's not working there is auto-away support, so it can change my status due to inactivity.<br />Actually it changes the status to "away" but for no reason at all, regardless of idle time, and refuses to set it back to active even when I click it's window and options.</p><p>Well, good thing is that pidgin's mature enough to have dbus interface, so as the most problems in life, this one can be solved with python ;)</p><p>First thing to check is <a title="pidgin-dbus reference" href="http://developer.pidgin.im/wiki/DbusHowto">pidgin dbus interface</a> and figure out how the states work there: you have to create a "state" with the appropriate message or find it among stored ones then set it as active, storing id of the previous one.</p><p>Next thing is to determine a way to get idle time.<br />Luckily, X keeps track of activity and I've already used <a title="xprintidle utility" href="http://www.dtek.chalmers.se/%7Ehenoch/text/xprintidle.html">xprintidle</a> with jabber.el, so it's not a problem.<br />Not quite a native py solution, but it has workaround for <a title="Xorg idle time bug" href="https://bugs.freedesktop.org/buglist.cgi?quicksearch=6439">one bug</a> and is much more liteweight than code using py-xlib.</p><p>From there it's just an endless sleep/check loop with occasional dbus calls.<br />One gotcha there is that pidgin can die or be closed, so the loop has to deal with this as well.</p><p>All there is...</p><p>Get idle time:</p><pre name="code" class="python">def get_idle():<br /> proc = Popen(['xprintidle'], stdout=PIPE)<br /> idle = int(proc.stdout.read().strip()) // 1000<br /> proc.wait()<br /> return idle<br /></pre><p>Simple dbus client code:</p><pre name="code" class="python">pidgin = dbus.SessionBus().get_object(<br /> 'im.pidgin.purple.PurpleService', '/im/pidgin/purple/PurpleObject' )<br />iface = dbus.Interface(pidgin, 'im.pidgin.purple.PurpleInterface')<br /></pre><p>Get initial (available) status:</p><pre name="code" class="python">st = iface.PurpleSavedstatusGetCurrent()<br />st_type = iface.PurpleSavedstatusGetType(st)<br />st_msg = iface.PurpleSavedstatusGetMessage(st)<br /></pre><p>Create away/na statuses:</p><pre name="code" class="python">st_away = iface.PurpleSavedstatusNew('', status.away)<br />iface.PurpleSavedstatusSetMessage(<br /> st_away, 'AFK (>{0}m)'.format(optz.away // 60) )<br />st_na = iface.PurpleSavedstatusNew('', status.xaway)<br />iface.PurpleSavedstatusSetMessage(<br /> st_na, 'AFK for quite a while (>{0}m)'.format(optz.na // 60) )<br /></pre><p>And the main loop:</p><pre name="code" class="python">while True:<br /> idle = get_idle()<br /> if idle > optz.away:<br />  if idle > optz.na:<br />   iface.PurpleSavedstatusActivate(st_na)<br />   log.debug('Switched status to N/A (idle: {0})'.format(idle//60))<br />  else:<br />   iface.PurpleSavedstatusActivate(st_away)<br />   log.debug('Switched status to away (idle: {0})'.format(idle//60))<br />  sleep(optz.poll)<br /> else:<br />  if iface.PurpleSavedstatusGetType(<br />    iface.PurpleSavedstatusGetCurrent() ) in (status.away, status.xaway):<br />   iface.PurpleSavedstatusActivate(st)<br />   log.debug('Restored original status (idle: {0})'.format(idle//60))<br />  sleep(optz.away)<br /></pre><p>Bonus of such approach is that any other checks can be easily added - fullscreen-video-status, for example, or emacs-dont-disturb status. I bet there are other plugins for this purposes, but I'd prefer few lines of clean py to some buggy .so anytime ;)</p><p><a title="Code" href="http://fraggod.net/oss/projects/status_watcher.py">Here's the full code</a>.</p>-=||||||||||=-2010/4/Availability-stats-and-history-log-with-relational-database-postgresql-=|||=-2010-04-10T14:29:20-=|||=-Availability stats (and history log) with relational database (postgresql)-=|||=-[u'SQL', u'SysAdmin', u'Web']-=|||=-<p>Last month I've been busy setting up a monitoring system at work.<br />Mostly it's the graphs with dynamic data like cpu/mem/io/net loads and application-specific stats (which I'll probably get around to describing sometime later), for which there is a nice RRD solutions (I've used <a title="cacti home" href="http://www.cacti.net/">cacti</a> + <a title="net-snmp project" href="http://www.net-snmp.org/">snmpd</a> + <a title="snmpd-pyagentx project" href="http://sourceforge.net/projects/snmpd-pyagentx/">my python extensions</a> + <a title="pyrrd project" href="http://code.google.com/p/pyrrd/">pyrrd</a> + <a title="rrdtool site" href="http://oss.oetiker.ch/rrdtool/">rrdtool</a> directly), but there was also one specific task of setting up websites' http-availability monitoring, spread on several shared-hosting servers.</p><p>There's about 4k of such websites and the data needed is close to boolean - whether site returns http code below 500 or it's considered "down", but it'd have been nice to know the code it returns.<br />Plus, of course, this responses have to be logged, so availability for any specific period can be calculated (in my case just as 1 - time_down / time_total). And these shouldn't include random stuff like 503 "downtime" because the poller got a bad luck on one poll or 500 because apache got a HUP while processing a request (in theory, these shouldn't happen of course, but...).<br />And on top of that, response delay have to be measured as well. And that is data which should be averaged and selected on some non-trivial basis.<br />Sites' list changes on a daily basis, polled data should be closed to real-time, so it's 5-10 minutes poll interval at most.</p><p>Clearly, it's time-series data yet rrd is unsuitable for the task - neither it's well-suited for complex data analysis, nor it can handle dynamic datasets. Creating a hundred rrds and maintaining the code for their management on fs looks like a world of pain.<br />Plain-log approach looks highly unoptimal, plus it's a lot of processing and logfile-management code.<br />Both approaches also needed some sort of (although trivial) network interface to data as well.</p><p>SQL-based DB engines handle storage and some-criteria-based selection and have an efficient network interface outta the box, so it wasn't much of a hard choice. And the only decent DBs I know out there are PostgreSQL and Oracle, sqlite or MySQL are rather limited solutions and I've never used interbase/firebird.<br />4k*5min is a lot of values though, tens-hundreds of millions of them actually, and RDBMS become quite sluggish on such amounts of data, so some aggregation or processing was in order and that's what this entry's about.</p><p>First, I've needed to keep one list of domains to check.<br />These came from the individual hosts where they were, well, hosted, so poller can periodically get this list and check all the domains there.</p><pre name="code" class="sql">CREATE TABLE state_hoard.list_http_availability (
 id serial NOT NULL,
 target character varying(128) NOT NULL,
 "domain" character varying(128) NOT NULL,
 check_type state_hoard.check_type NOT NULL,
 "timestamp" numeric,
 source character varying(16),
 CONSTRAINT state_ha__id PRIMARY KEY (id),
 CONSTRAINT state_ha__domain_ip_check_type
 UNIQUE (target, domain, check_type) );
</pre><p>It should probably be extended with other checks later on so there's check_type field with enum like this:</p><pre name="code" class="sql">CREATE TYPE state_hoard.check_type AS ENUM ('http', 'https');</pre><p>Target (IP) and domain (hostname) are separate fields here, since dns data is not to be trusted but the http request should have host-field to be processed correctly.</p><p>Resulting table:<img alt="list_http_availability table data" title="list_http_availability table data" style="display: block;" src="http://blog.fraggod.net/static/embed/http_availability_list.jpg" align="center" /></p><p>List is updated via third-party scripts which shouldn't care for internal db structure even a little bit, so they only need to do insert/delete ops when the list changes, so the db can take care of the rest, thanks to triggers.<br />Replace via delete/insert approach isn't an option here, since other tables are linked vs this one, so update is the way.</p><pre name="code" class="sql">CREATE OR REPLACE FUNCTION state_hoard.list_ha_replace()
 RETURNS trigger AS
$BODY$
DECLARE
 updated integer;

BEGIN

-- Implicit timestamping
NEW.timestamp := COALESCE( NEW.timestamp,
 EXTRACT('epoch' FROM CURRENT_TIMESTAMP) );

UPDATE state_hoard.list_http_availability
 SET timestamp = NEW.timestamp, source = NEW.source
 WHERE domain = NEW.domain
 AND target = NEW.target
 AND check_type = NEW.check_type;

-- Check if the row still need to be inserted
GET DIAGNOSTICS updated = ROW_COUNT;
IF updated = 0
THEN RETURN NEW;
ELSE RETURN NULL;
END IF;

END;
$BODY$
 LANGUAGE 'plpgsql' VOLATILE
 COST 100;


CREATE TRIGGER list_ha__replace
 BEFORE INSERT
 ON state_hoard.list_http_availability
 FOR EACH ROW
 EXECUTE PROCEDURE state_hoard.list_ha_replace();
</pre><p>From there I had two ideas on how to use this data and store immediate results, from the poller perspective:</p><ul>
<li>To replicate the whole table into some sort of "check-list", filling 
fields there as the data arrives.</li>
<li>To create persistent linked tables with polled data, which just 
replaced (on unique-domain basis) with each new poll.<br />
</li>
</ul>
<p>While former looks appealing since it allows to keep state in DB, not the poller, latter provides persistent availability/delay tables and that's one of the things I need.</p><pre name="code" class="sql">CREATE TABLE state_hoard.state_http_availability (
 check_id integer NOT NULL,
 host character varying(32) NOT NULL,
 code integer,
 "timestamp" numeric,
 CONSTRAINT state_ha__check_host PRIMARY KEY (check_id, host),
 CONSTRAINT state_http_availability_check_id_fkey FOREIGN KEY (check_id)
 REFERENCES state_hoard.list_http_availability (id) MATCH SIMPLE
 ON UPDATE RESTRICT ON DELETE CASCADE );

CREATE TABLE state_hoard.state_http_delay (
 check_id integer NOT NULL,
 host character varying(32) NOT NULL,
 delay numeric,
 "timestamp" numeric,
 CONSTRAINT state_http_delay_check_id_fkey FOREIGN KEY (check_id)
 REFERENCES state_hoard.list_http_availability (id) MATCH SIMPLE
 ON UPDATE NO ACTION ON DELETE CASCADE );
</pre><p>These can be thought of as an extensions of the main (list_http_availability) table, containing "current state" columns for each polled domain, and when domain is no longer polled, it gets dropped from these tables as well.<br />Poller just gets the list and inserts the values into these, w/o even having permissions to alter the list itself.</p><p>Since these tables are for latest data, duplicate inserts should be handled and timestamps can be generated implicitly.<br />For current-state table it's just a replace on each insert. PostgreSQL doesn't have convenient "replace" statement like MySQL but the triggers can easily compensate for that:</p><pre name="code" class="sql">CREATE OR REPLACE FUNCTION state_hoard.state_ha_replace()
 RETURNS trigger AS
$BODY$
BEGIN

-- Drop old record, if any
DELETE FROM state_hoard.state_http_availability WHERE check_id = NEW.check_id AND host = NEW.host;

-- Implicit timestamp setting, if it's omitted
NEW.timestamp := COALESCE(NEW.timestamp, EXTRACT('epoch' FROM CURRENT_TIMESTAMP));

RETURN NEW;

END;
$BODY$
 LANGUAGE 'plpgsql' VOLATILE
 COST 100;


CREATE TRIGGER state_ha__replace
 BEFORE INSERT
 ON state_hoard.state_http_availability
 FOR EACH ROW
 EXECUTE PROCEDURE state_hoard.state_ha_replace();
</pre><p>Individual http delays can have quite high entropy, since the http-response processing in poller can't be truly asynchronous with such a number of hosts and in fact it's a single-thread eventloop (twisted) anyway, so values here are kept for some time, so they can be averaged later with a simple group-by.<br />Timestamp-based cleanup is built into the poller itself, so the trigger here only fills implicit timestamps.</p><pre name="code" class="sql">CREATE OR REPLACE FUNCTION state_hoard.state_hd_insert()
 RETURNS trigger AS
$BODY$
BEGIN

-- Implicit timestamp setting, if it's omitted
NEW.timestamp := COALESCE( NEW.timestamp,
 EXTRACT('epoch' FROM CURRENT_TIMESTAMP) );

RETURN NEW;

END;
$BODY$
 LANGUAGE 'plpgsql' VOLATILE
 COST 100;


CREATE TRIGGER state_hd__insert
 BEFORE INSERT
 ON state_hoard.state_http_delay
 FOR EACH ROW
 EXECUTE PROCEDURE state_hoard.state_hd_insert();
</pre><p>After that comes the logging part, and the logged part is http response codes.</p><p>These shouldn't change frequently, so it's only logical to write changes-only log.<br />To grind out random errors I write a longer-than-poll-time (10 minutes, actually) averages to the intermediate table, while keeping track of such errors anyway, but in separate log table.</p><pre name="code" class="sql">CREATE TABLE state_hoard.log_http_availability (
 "domain" character varying(128) NOT NULL,
 code integer,
 "timestamp" numeric NOT NULL,
 CONSTRAINT log_ha__domain_timestamp PRIMARY KEY (domain, "timestamp") );
</pre><p>Interval for these averages can be acquired via simple rounding, and it's convenient to have single function for that, plus the step in retriveable form. "Immutable" type here means that the results will be cached for each set of parameters.</p><pre name="code" class="sql">CREATE OR REPLACE FUNCTION state_hoard.log_ha_step()
 RETURNS integer AS
'SELECT 600;'
 LANGUAGE 'sql' IMMUTABLE
 COST 100;

CREATE OR REPLACE FUNCTION state_hoard.log_ha_discrete_time(numeric)
 RETURNS numeric AS
'SELECT (div($1, state_hoard.log_ha_step()::numeric) + 1) * state_hoard.log_ha_step();'
 LANGUAGE 'sql' IMMUTABLE
 COST 100;
</pre><p>"Averaging" for the logs is actually just dropping errors if there's at least one success in the interval.<br />It's only logical to do this right on insert into the log-table:</p><pre name="code" class="sql">CREATE OR REPLACE FUNCTION state_hoard.log_ha_coerce()
 RETURNS trigger AS
$BODY$
DECLARE
 updated integer;

BEGIN

-- Implicit timestamp setting, if it's omitted
NEW.timestamp := state_hoard.log_ha_discrete_time(
 COALESCE( NEW.timestamp,
 EXTRACT('epoch' FROM CURRENT_TIMESTAMP) )::numeric );

IF NEW.code = 200
THEN
 -- Successful probe overrides (probably random) errors
 UPDATE state_hoard.log_http_availability
 SET code = NEW.code
 WHERE domain = NEW.domain AND timestamp = NEW.timestamp;
 GET DIAGNOSTICS updated = ROW_COUNT;

ELSE
 -- Errors don't override anything
 SELECT COUNT(*)
 FROM state_hoard.log_http_availability
 WHERE domain = NEW.domain AND timestamp = NEW.timestamp
 INTO updated;

END IF;

-- True for first value in a new interval
IF updated = 0
THEN RETURN NEW;
ELSE RETURN NULL;
END IF;

END;
$BODY$
 LANGUAGE 'plpgsql' VOLATILE
 COST 100;


CREATE TRIGGER log_ha__coerce
 BEFORE INSERT
 ON state_hoard.log_http_availability
 FOR EACH ROW
     EXECUTE PROCEDURE state_hoard.log_ha_coerce();
</pre><p>The only thing left at this point is to actually tie this intermediate log-table with the state-table, and after-insert/update hooks are good place for that.</p><pre name="code" class="sql">CREATE OR REPLACE FUNCTION state_hoard.state_ha_log()
 RETURNS trigger AS
$BODY$

DECLARE
 domain_var character varying (128);
 code_var integer;

 -- Timestamp of the log entry, explicit to get the older one, checking for random errors
 ts numeric := state_hoard.log_ha_discrete_time(EXTRACT('epoch' FROM CURRENT_TIMESTAMP));

BEGIN

SELECT domain FROM state_hoard.list_http_availability
 WHERE id = NEW.check_id INTO domain_var;

SELECT code FROM state_hoard.log_http_availability
 WHERE domain = domain_var AND timestamp = ts
 INTO code_var;

-- This actually replaces older entry, see log_ha_coerce hook
INSERT INTO state_hoard.log_http_availability (domain, code, timestamp)
 VALUES (domain_var, NEW.code, ts);

-- Random errors' trapping
IF code_var != NEW.code AND (NEW.code > 400 OR code_var > 400) THEN
 code_var = CASE WHEN NEW.code > 400 THEN NEW.code ELSE code_var END;
 INSERT INTO state_hoard.log_http_random_errors (domain, code)
 VALUES (domain_var, code_var);
END IF;

RETURN NULL;

END;
$BODY$
 LANGUAGE 'plpgsql' VOLATILE
 COST 100;


CREATE TRIGGER state_ha__log_insert
 AFTER INSERT
 ON state_hoard.state_http_availability
 FOR EACH ROW
 EXECUTE PROCEDURE state_hoard.state_ha_log();

CREATE TRIGGER state_ha__log_update
 AFTER UPDATE
 ON state_hoard.state_http_availability
 FOR EACH ROW
 EXECUTE PROCEDURE state_hoard.state_ha_log();
</pre><p>From here, the log will get populated already, but in a few days it will get millions of entries and counting, so it have to be aggregated and the most efficient method for this sort of data seem to be in keeping just change-points for return codes since they're quite rare.</p><p>"Random errors" are trapped here as well and stored to the separate table. They aren't frequent, so no other action is taken there.</p><p>The log-diff table is just that - code changes.<br />"code_prev" field is here for convenience, since I needed to get if there were any changes for a given period, so the rows there would give complete picture.</p><pre name="code" class="sql">CREATE TABLE state_hoard.log_http_availability_diff (
 "domain" character varying(128) NOT NULL,
 code integer,
 code_prev integer,
 "timestamp" numeric NOT NULL,
 CONSTRAINT log_had__domain_timestamp PRIMARY KEY (domain, "timestamp") );
</pre><p>Updates to this table happen on cron-basis and generated right inside the db, thanks to plpgsql for that.</p><pre name="code" class="sql">LOCK TABLE log_http_availability_diff IN EXCLUSIVE MODE;
LOCK TABLE log_http_availability IN EXCLUSIVE MODE;

INSERT INTO log_http_availability_diff
 SELECT * FROM log_ha_diff_for_period(NULL, NULL)
 AS data(domain character varying, code int, code_prev int, timestamp numeric);

TRUNCATE TABLE log_http_availability;
</pre><p>And the diff-generation code:</p><pre name="code" class="sql">CREATE OR REPLACE FUNCTION state_hoard.log_ha_diff_for_period(ts_min numeric, ts_max numeric)
 RETURNS SETOF record AS
$BODY$

DECLARE
 rec state_hoard.log_http_availability%rowtype;
 rec_next state_hoard.log_http_availability%rowtype;
 rec_diff state_hoard.log_http_availability_diff%rowtype;

BEGIN

FOR rec_next IN
 EXECUTE 'SELECT domain, code, timestamp
 FROM state_hoard.log_http_availability'
 || CASE WHEN NOT (ts_min IS NULL AND ts_max IS NULL) THEN
 ' WHERE timestamp BETWEEN '||ts_min||' AND '||ts_max ELSE '' END ||
 ' ORDER BY domain, timestamp'
LOOP

 IF NOT rec_diff.domain IS NULL AND rec_diff.domain != rec_next.domain THEN
 -- Last record for this domain - skip unknown vals and code change check
 rec_diff.domain = NULL;
 END IF;

 IF NOT rec_diff.domain IS NULL

 THEN
 -- Time-skip (unknown values) addition
 rec_diff.timestamp = state_hoard.log_ha_discrete_time(rec.timestamp + 1);
 IF rec_diff.timestamp &lt; rec_next.timestamp THEN
 -- Map unknown interval
 rec_diff.code = NULL;
 rec_diff.code_prev = rec.code;
 RETURN NEXT rec_diff;
 END IF;

 -- rec.code here should be affected by unknown-vals as well
 IF rec_diff.code != rec_next.code THEN
 rec_diff.code_prev = rec_diff.code;
 rec_diff.code = rec_next.code;
 rec_diff.timestamp = rec_next.timestamp;
 RETURN NEXT rec_diff;
 END IF;

 ELSE
 -- First record for new domain or whole loop (not returned)
 -- RETURN NEXT rec_next;
 rec_diff.domain = rec_next.domain;

 END IF;

 rec.code = rec_next.code;
 rec.timestamp = rec_next.timestamp;

END LOOP;

END;

$BODY$
 LANGUAGE 'plpgsql' VOLATILE
 COST 100
 ROWS 1000;
</pre><p>So that's the logging into the database.<br />Not as nice and simple as rrd but much more flexible in the end.</p><p>And since PostgreSQL already <a title="PL/Python docs" href="http://www.postgresql.org/docs/8.4/interactive/plpython.html">allows to hook up PL/Python</a>, there's no problem adding a few triggers to the log-diff table to send out notifications in case there's a problem.<br />Whether it's wise to put all the logic into the database like that is a good question though, I'd probably opt for some sort of interface on the database -> outside-world path, so db queries won't have full-fledged scripting language at their disposal and db event handlers would be stored on the file system, where they belong, w/o tying db to the host that way.</p>-=|||=-<p>Last month I've been busy setting up a monitoring system at work.<br />Mostly it's the graphs with dynamic data like cpu/mem/io/net loads and application-specific stats (which I'll probably get around to describing sometime later), for which there is a nice RRD solutions (I've used <a title="cacti home" href="http://www.cacti.net/">cacti</a> + <a title="net-snmp project" href="http://www.net-snmp.org/">snmpd</a> + <a title="snmpd-pyagentx project" href="http://sourceforge.net/projects/snmpd-pyagentx/">my python extensions</a> + <a title="pyrrd project" href="http://code.google.com/p/pyrrd/">pyrrd</a> + <a title="rrdtool site" href="http://oss.oetiker.ch/rrdtool/">rrdtool</a> directly), but there was also one specific task of setting up websites' http-availability monitoring, spread on several shared-hosting servers.</p><p>There's about 4k of such websites and the data needed is close to boolean - whether site returns http code below 500 or it's considered "down", but it'd have been nice to know the code it returns.<br />Plus, of course, this responses have to be logged, so availability for any specific period can be calculated (in my case just as 1 - time_down / time_total). And these shouldn't include random stuff like 503 "downtime" because the poller got a bad luck on one poll or 500 because apache got a HUP while processing a request (in theory, these shouldn't happen of course, but...).<br />And on top of that, response delay have to be measured as well. And that is data which should be averaged and selected on some non-trivial basis.<br />Sites' list changes on a daily basis, polled data should be closed to real-time, so it's 5-10 minutes poll interval at most.</p><p>Clearly, it's time-series data yet rrd is unsuitable for the task - neither it's well-suited for complex data analysis, nor it can handle dynamic datasets. Creating a hundred rrds and maintaining the code for their management on fs looks like a world of pain.<br />Plain-log approach looks highly unoptimal, plus it's a lot of processing and logfile-management code.<br />Both approaches also needed some sort of (although trivial) network interface to data as well.</p><p>SQL-based DB engines handle storage and some-criteria-based selection and have an efficient network interface outta the box, so it wasn't much of a hard choice. And the only decent DBs I know out there are PostgreSQL and Oracle, sqlite or MySQL are rather limited solutions and I've never used interbase/firebird.<br />4k*5min is a lot of values though, tens-hundreds of millions of them actually, and RDBMS become quite sluggish on such amounts of data, so some aggregation or processing was in order and that's what this entry's about.</p><p>First, I've needed to keep one list of domains to check.<br />These came from the individual hosts where they were, well, hosted, so poller can periodically get this list and check all the domains there.</p><pre name="code" class="sql">CREATE TABLE state_hoard.list_http_availability (<br /> id serial NOT NULL,<br /> target character varying(128) NOT NULL,<br /> "domain" character varying(128) NOT NULL,<br /> check_type state_hoard.check_type NOT NULL,<br /> "timestamp" numeric,<br /> source character varying(16),<br /> CONSTRAINT state_ha__id PRIMARY KEY (id),<br /> CONSTRAINT state_ha__domain_ip_check_type<br /> UNIQUE (target, domain, check_type) );<br /></pre><p>It should probably be extended with other checks later on so there's check_type field with enum like this:</p><pre name="code" class="sql">CREATE TYPE state_hoard.check_type AS ENUM ('http', 'https');</pre><p>Target (IP) and domain (hostname) are separate fields here, since dns data is not to be trusted but the http request should have host-field to be processed correctly.</p><p>Resulting table:<img alt="list_http_availability table data" title="list_http_availability table data" style="display: block;" src="http://blog.fraggod.net/static/embed/http_availability_list.jpg" align="center" /></p><p>List is updated via third-party scripts which shouldn't care for internal db structure even a little bit, so they only need to do insert/delete ops when the list changes, so the db can take care of the rest, thanks to triggers.<br />Replace via delete/insert approach isn't an option here, since other tables are linked vs this one, so update is the way.</p><pre name="code" class="sql">CREATE OR REPLACE FUNCTION state_hoard.list_ha_replace()<br /> RETURNS trigger AS<br />$BODY$<br />DECLARE<br /> updated integer;<br /><br />BEGIN<br /><br />-- Implicit timestamping<br />NEW.timestamp := COALESCE( NEW.timestamp,<br /> EXTRACT('epoch' FROM CURRENT_TIMESTAMP) );<br /><br />UPDATE state_hoard.list_http_availability<br /> SET timestamp = NEW.timestamp, source = NEW.source<br /> WHERE domain = NEW.domain<br /> AND target = NEW.target<br /> AND check_type = NEW.check_type;<br /><br />-- Check if the row still need to be inserted<br />GET DIAGNOSTICS updated = ROW_COUNT;<br />IF updated = 0<br />THEN RETURN NEW;<br />ELSE RETURN NULL;<br />END IF;<br /><br />END;<br />$BODY$<br /> LANGUAGE 'plpgsql' VOLATILE<br /> COST 100;<br /><br /><br />CREATE TRIGGER list_ha__replace<br /> BEFORE INSERT<br /> ON state_hoard.list_http_availability<br /> FOR EACH ROW<br /> EXECUTE PROCEDURE state_hoard.list_ha_replace();<br /></pre><p>From there I had two ideas on how to use this data and store immediate results, from the poller perspective:</p><ul>
<li>To replicate the whole table into some sort of "check-list", filling 
fields there as the data arrives.</li>
<li>To create persistent linked tables with polled data, which just 
replaced (on unique-domain basis) with each new poll.<br />
</li>
</ul>
<p>While former looks appealing since it allows to keep state in DB, not the poller, latter provides persistent availability/delay tables and that's one of the things I need.</p><pre name="code" class="sql">CREATE TABLE state_hoard.state_http_availability (<br /> check_id integer NOT NULL,<br /> host character varying(32) NOT NULL,<br /> code integer,<br /> "timestamp" numeric,<br /> CONSTRAINT state_ha__check_host PRIMARY KEY (check_id, host),<br /> CONSTRAINT state_http_availability_check_id_fkey FOREIGN KEY (check_id)<br /> REFERENCES state_hoard.list_http_availability (id) MATCH SIMPLE<br /> ON UPDATE RESTRICT ON DELETE CASCADE );<br /><br />CREATE TABLE state_hoard.state_http_delay (<br /> check_id integer NOT NULL,<br /> host character varying(32) NOT NULL,<br /> delay numeric,<br /> "timestamp" numeric,<br /> CONSTRAINT state_http_delay_check_id_fkey FOREIGN KEY (check_id)<br /> REFERENCES state_hoard.list_http_availability (id) MATCH SIMPLE<br /> ON UPDATE NO ACTION ON DELETE CASCADE );<br /></pre><p>These can be thought of as an extensions of the main (list_http_availability) table, containing "current state" columns for each polled domain, and when domain is no longer polled, it gets dropped from these tables as well.<br />Poller just gets the list and inserts the values into these, w/o even having permissions to alter the list itself.</p><p>Since these tables are for latest data, duplicate inserts should be handled and timestamps can be generated implicitly.<br />For current-state table it's just a replace on each insert. PostgreSQL doesn't have convenient "replace" statement like MySQL but the triggers can easily compensate for that:</p><pre name="code" class="sql">CREATE OR REPLACE FUNCTION state_hoard.state_ha_replace()<br /> RETURNS trigger AS<br />$BODY$<br />BEGIN<br /><br />-- Drop old record, if any<br />DELETE FROM state_hoard.state_http_availability WHERE check_id = NEW.check_id AND host = NEW.host;<br /><br />-- Implicit timestamp setting, if it's omitted<br />NEW.timestamp := COALESCE(NEW.timestamp, EXTRACT('epoch' FROM CURRENT_TIMESTAMP));<br /><br />RETURN NEW;<br /><br />END;<br />$BODY$<br /> LANGUAGE 'plpgsql' VOLATILE<br /> COST 100;<br /><br /><br />CREATE TRIGGER state_ha__replace<br /> BEFORE INSERT<br /> ON state_hoard.state_http_availability<br /> FOR EACH ROW<br /> EXECUTE PROCEDURE state_hoard.state_ha_replace();<br /></pre><p>Individual http delays can have quite high entropy, since the http-response processing in poller can't be truly asynchronous with such a number of hosts and in fact it's a single-thread eventloop (twisted) anyway, so values here are kept for some time, so they can be averaged later with a simple group-by.<br />Timestamp-based cleanup is built into the poller itself, so the trigger here only fills implicit timestamps.</p><pre name="code" class="sql">CREATE OR REPLACE FUNCTION state_hoard.state_hd_insert()<br /> RETURNS trigger AS<br />$BODY$<br />BEGIN<br /><br />-- Implicit timestamp setting, if it's omitted<br />NEW.timestamp := COALESCE( NEW.timestamp,<br /> EXTRACT('epoch' FROM CURRENT_TIMESTAMP) );<br /><br />RETURN NEW;<br /><br />END;<br />$BODY$<br /> LANGUAGE 'plpgsql' VOLATILE<br /> COST 100;<br /><br /><br />CREATE TRIGGER state_hd__insert<br /> BEFORE INSERT<br /> ON state_hoard.state_http_delay<br /> FOR EACH ROW<br /> EXECUTE PROCEDURE state_hoard.state_hd_insert();<br /></pre><p>After that comes the logging part, and the logged part is http response codes.</p><p>These shouldn't change frequently, so it's only logical to write changes-only log.<br />To grind out random errors I write a longer-than-poll-time (10 minutes, actually) averages to the intermediate table, while keeping track of such errors anyway, but in separate log table.</p><pre name="code" class="sql">CREATE TABLE state_hoard.log_http_availability (<br /> "domain" character varying(128) NOT NULL,<br /> code integer,<br /> "timestamp" numeric NOT NULL,<br /> CONSTRAINT log_ha__domain_timestamp PRIMARY KEY (domain, "timestamp") );<br /></pre><p>Interval for these averages can be acquired via simple rounding, and it's convenient to have single function for that, plus the step in retriveable form. "Immutable" type here means that the results will be cached for each set of parameters.</p><pre name="code" class="sql">CREATE OR REPLACE FUNCTION state_hoard.log_ha_step()<br /> RETURNS integer AS<br />'SELECT 600;'<br /> LANGUAGE 'sql' IMMUTABLE<br /> COST 100;<br /><br />CREATE OR REPLACE FUNCTION state_hoard.log_ha_discrete_time(numeric)<br /> RETURNS numeric AS<br />'SELECT (div($1, state_hoard.log_ha_step()::numeric) + 1) * state_hoard.log_ha_step();'<br /> LANGUAGE 'sql' IMMUTABLE<br /> COST 100;<br /></pre><p>"Averaging" for the logs is actually just dropping errors if there's at least one success in the interval.<br />It's only logical to do this right on insert into the log-table:</p><pre name="code" class="sql">CREATE OR REPLACE FUNCTION state_hoard.log_ha_coerce()<br /> RETURNS trigger AS<br />$BODY$<br />DECLARE<br /> updated integer;<br /><br />BEGIN<br /><br />-- Implicit timestamp setting, if it's omitted<br />NEW.timestamp := state_hoard.log_ha_discrete_time(<br /> COALESCE( NEW.timestamp,<br /> EXTRACT('epoch' FROM CURRENT_TIMESTAMP) )::numeric );<br /><br />IF NEW.code = 200<br />THEN<br /> -- Successful probe overrides (probably random) errors<br /> UPDATE state_hoard.log_http_availability<br /> SET code = NEW.code<br /> WHERE domain = NEW.domain AND timestamp = NEW.timestamp;<br /> GET DIAGNOSTICS updated = ROW_COUNT;<br /><br />ELSE<br /> -- Errors don't override anything<br /> SELECT COUNT(*)<br /> FROM state_hoard.log_http_availability<br /> WHERE domain = NEW.domain AND timestamp = NEW.timestamp<br /> INTO updated;<br /><br />END IF;<br /><br />-- True for first value in a new interval<br />IF updated = 0<br />THEN RETURN NEW;<br />ELSE RETURN NULL;<br />END IF;<br /><br />END;<br />$BODY$<br /> LANGUAGE 'plpgsql' VOLATILE<br /> COST 100;<br /><br /><br />CREATE TRIGGER log_ha__coerce<br /> BEFORE INSERT<br /> ON state_hoard.log_http_availability<br /> FOR EACH ROW<br />     EXECUTE PROCEDURE state_hoard.log_ha_coerce();<br /></pre><p>The only thing left at this point is to actually tie this intermediate log-table with the state-table, and after-insert/update hooks are good place for that.</p><pre name="code" class="sql">CREATE OR REPLACE FUNCTION state_hoard.state_ha_log()<br /> RETURNS trigger AS<br />$BODY$<br /><br />DECLARE<br /> domain_var character varying (128);<br /> code_var integer;<br /><br /> -- Timestamp of the log entry, explicit to get the older one, checking for random errors<br /> ts numeric := state_hoard.log_ha_discrete_time(EXTRACT('epoch' FROM CURRENT_TIMESTAMP));<br /><br />BEGIN<br /><br />SELECT domain FROM state_hoard.list_http_availability<br /> WHERE id = NEW.check_id INTO domain_var;<br /><br />SELECT code FROM state_hoard.log_http_availability<br /> WHERE domain = domain_var AND timestamp = ts<br /> INTO code_var;<br /><br />-- This actually replaces older entry, see log_ha_coerce hook<br />INSERT INTO state_hoard.log_http_availability (domain, code, timestamp)<br /> VALUES (domain_var, NEW.code, ts);<br /><br />-- Random errors' trapping<br />IF code_var != NEW.code AND (NEW.code > 400 OR code_var > 400) THEN<br /> code_var = CASE WHEN NEW.code > 400 THEN NEW.code ELSE code_var END;<br /> INSERT INTO state_hoard.log_http_random_errors (domain, code)<br /> VALUES (domain_var, code_var);<br />END IF;<br /><br />RETURN NULL;<br /><br />END;<br />$BODY$<br /> LANGUAGE 'plpgsql' VOLATILE<br /> COST 100;<br /><br /><br />CREATE TRIGGER state_ha__log_insert<br /> AFTER INSERT<br /> ON state_hoard.state_http_availability<br /> FOR EACH ROW<br /> EXECUTE PROCEDURE state_hoard.state_ha_log();<br /><br />CREATE TRIGGER state_ha__log_update<br /> AFTER UPDATE<br /> ON state_hoard.state_http_availability<br /> FOR EACH ROW<br /> EXECUTE PROCEDURE state_hoard.state_ha_log();<br /></pre><p>From here, the log will get populated already, but in a few days it will get millions of entries and counting, so it have to be aggregated and the most efficient method for this sort of data seem to be in keeping just change-points for return codes since they're quite rare.</p><p>"Random errors" are trapped here as well and stored to the separate table. They aren't frequent, so no other action is taken there.</p><p>The log-diff table is just that - code changes.<br />"code_prev" field is here for convenience, since I needed to get if there were any changes for a given period, so the rows there would give complete picture.</p><pre name="code" class="sql">CREATE TABLE state_hoard.log_http_availability_diff (<br /> "domain" character varying(128) NOT NULL,<br /> code integer,<br /> code_prev integer,<br /> "timestamp" numeric NOT NULL,<br /> CONSTRAINT log_had__domain_timestamp PRIMARY KEY (domain, "timestamp") );<br /></pre><p>Updates to this table happen on cron-basis and generated right inside the db, thanks to plpgsql for that.</p><pre name="code" class="sql">LOCK TABLE log_http_availability_diff IN EXCLUSIVE MODE;<br />LOCK TABLE log_http_availability IN EXCLUSIVE MODE;<br /><br />INSERT INTO log_http_availability_diff<br /> SELECT * FROM log_ha_diff_for_period(NULL, NULL)<br /> AS data(domain character varying, code int, code_prev int, timestamp numeric);<br /><br />TRUNCATE TABLE log_http_availability;<br /></pre><p>And the diff-generation code:</p><pre name="code" class="sql">CREATE OR REPLACE FUNCTION state_hoard.log_ha_diff_for_period(ts_min numeric, ts_max numeric)<br /> RETURNS SETOF record AS<br />$BODY$<br /><br />DECLARE<br /> rec state_hoard.log_http_availability%rowtype;<br /> rec_next state_hoard.log_http_availability%rowtype;<br /> rec_diff state_hoard.log_http_availability_diff%rowtype;<br /><br />BEGIN<br /><br />FOR rec_next IN<br /> EXECUTE 'SELECT domain, code, timestamp<br /> FROM state_hoard.log_http_availability'<br /> || CASE WHEN NOT (ts_min IS NULL AND ts_max IS NULL) THEN<br /> ' WHERE timestamp BETWEEN '||ts_min||' AND '||ts_max ELSE '' END ||<br /> ' ORDER BY domain, timestamp'<br />LOOP<br /><br /> IF NOT rec_diff.domain IS NULL AND rec_diff.domain != rec_next.domain THEN<br /> -- Last record for this domain - skip unknown vals and code change check<br /> rec_diff.domain = NULL;<br /> END IF;<br /><br /> IF NOT rec_diff.domain IS NULL<br /><br /> THEN<br /> -- Time-skip (unknown values) addition<br /> rec_diff.timestamp = state_hoard.log_ha_discrete_time(rec.timestamp + 1);<br /> IF rec_diff.timestamp &lt; rec_next.timestamp THEN<br /> -- Map unknown interval<br /> rec_diff.code = NULL;<br /> rec_diff.code_prev = rec.code;<br /> RETURN NEXT rec_diff;<br /> END IF;<br /><br /> -- rec.code here should be affected by unknown-vals as well<br /> IF rec_diff.code != rec_next.code THEN<br /> rec_diff.code_prev = rec_diff.code;<br /> rec_diff.code = rec_next.code;<br /> rec_diff.timestamp = rec_next.timestamp;<br /> RETURN NEXT rec_diff;<br /> END IF;<br /><br /> ELSE<br /> -- First record for new domain or whole loop (not returned)<br /> -- RETURN NEXT rec_next;<br /> rec_diff.domain = rec_next.domain;<br /><br /> END IF;<br /><br /> rec.code = rec_next.code;<br /> rec.timestamp = rec_next.timestamp;<br /><br />END LOOP;<br /><br />END;<br /><br />$BODY$<br /> LANGUAGE 'plpgsql' VOLATILE<br /> COST 100<br /> ROWS 1000;<br /></pre><p>So that's the logging into the database.<br />Not as nice and simple as rrd but much more flexible in the end.</p><p>And since PostgreSQL already <a title="PL/Python docs" href="http://www.postgresql.org/docs/8.4/interactive/plpython.html">allows to hook up PL/Python</a>, there's no problem adding a few triggers to the log-diff table to send out notifications in case there's a problem.<br />Whether it's wise to put all the logic into the database like that is a good question though, I'd probably opt for some sort of interface on the database -> outside-world path, so db queries won't have full-fledged scripting language at their disposal and db event handlers would be stored on the file system, where they belong, w/o tying db to the host that way.</p>-=||||||||||=-2010/4/Thoughts-on-VCS-supporting-documentation-and-Fossil-=|||=-2010-04-17T17:22:22-=|||=-Thoughts on VCS, supporting documentation and Fossil-=|||=-[u'SCM', u'Fossil', u'Documentation', u'Desktop', u'Web']-=|||=-<p>I'm a happy <a title="git vcs" href="http://git-scm.com/">git</a> user for several years now, and the best thing about it is that I've learned how VCS-es, and git in particular, work under the hood.<br />It expanded (and in most aspects probably formed) my view on the time-series data storage - very useful knowledge for wide range of purposes from log or configuration storage to snapshotting, backups and filesystem synchronisation. Another similar revelation in this area was probably <a title="rrdtool" href="http://oss.oetiker.ch/rrdtool/">rrdtool</a>, but still on much smaller scale.</p><p>Few years back, I've kept virtually no history of my actions, only keeping my work in <a title="CVS (R.I.P.)" href="http://en.wikipedia.org/wiki/Concurrent_Versions_System">CVS</a>/<a title="Subversion" href="http://subversion.apache.org/">SVN</a>, and even that was just for ease of collaboration.<br />Today, I can easily trace, sync and transfer virtually everything that changes and is important in my system - the code I'm working on, all the configuration files, even auto-generated ones, tasks' and thoughts' lists, state-description files like lists of installed packages (local sw state) and gentoo-portage tree (global sw state), even all the logs and binary blobs like rootfs in rsync-hardlinked backups for a few past months.</p><p>Git is a great help in these tasks, but what I feel lacking there is a first - common timeline (spanning both into the past and the future) for all these data series, and second - documentation.</p><p>Solution to the first one I've yet to find.</p><p>Second one is partially solved by commit-msgs, inline comments and even this blog for the past issues and simple todo-lists (some I keep in plaintext, some in tudu app) for the future.<br />Biggest problem I see here is the lack of consistency between all these: todo-tasks end up as dropped lines in the git-log w/o any link to the past issues or reverse link to the original idea or vision, and that's just the changes.<br />Documentation for anything more than local implementation details and it's history is virtually non-existant and most times it takes a lot of effort and time to retrace the original line of thought, reasoning and purpose behind the stuff I've done (and why I've done it like that) in the past, often with the considerable gaps and eventual re-invention of the wheels and pitfalls I've already done, due to faulty biological memory.</p><p>So, today I've decided to scour over the available project and task management software to find something that ties the vcs repositories and their logs with the future tickets and some sort of expanded notes, where needed.</p><p>Starting point was actually the <a title="trac project" href="http://trac.edgewall.org/">trac</a>, which I've used quite extensively in the past and present, and is quite fond of it's outside simplicity yet fully-featured capabilities as both wiki-engine and issue tracker. Better yet, it's py and can work with vcs.<br />The downside is that it's still a separate service and web-based one at that, meaning that it's online-only, and that the content is anchored to the server I deploy it to (not to mention underlying vcs). Hell, it's centralized and laggy, and ever since git's branching and merging ideas of decentralized work took root in my brain, I have issue with that.</p><p>It just looks like a completely wrong approach for my task, yet I thought that I can probably tolerate that if there are no better options and then I've stumbled upon <a title="Fossil's fossil ;)" href="http://www.fossil-scm.org/index.html/doc/tip/www/index.wiki">Fossil VCS</a>.</p><p>The name actually rang a bell, but from a <a title="Plan 9 from Bell Labs" href="http://plan9.bell-labs.com/plan9/index.html">9p</a> universe, where it's a name for a vcs-like filesystem which was (along with venti, built on top of it) one of two primary reasons I've even looked into <a title="Plan 9 from Bell Labs" href="http://plan9.bell-labs.com/plan9/index.html">plan9</a> (the other being its 9p/styx protocol).<br />Similary-named VCS haven't disappointed me as well, at least conceptually. The main win is in the integrated ticket system and wiki, providing just the thing I need in a distributed versioned vcs environment.</p><p>Fossil's overall <a title="Fossil principles of operation" href="http://www.fossil-scm.org/index.html/doc/tip/www/pop.wiki">design principles</a> and <a title="Fossil concepts" href="http://www.fossil-scm.org/index.html/doc/tip/www/concepts.wiki">concepts</a> (plus <a title="Moar fossil theory!" href="http://www.fossil-scm.org/index.html/doc/tip/www/theory1.wiki">this</a>) are well-documented on <a title="Fossil wiki index" href="http://www.fossil-scm.org/index.html/doc/tip/www/index.wiki">it's site</a> (which is a just a fossil repo itself), and the catch-points for me were:</p><ul>
<li>Tickets and wiki, of course. Can be edited locally, synced, distributed, have local settings and appearance, based on <a title="TH1 language" href="http://www.sqliteconcepts.org/THManual.pdf">tcl-ish domain-specific language</a>.</li>
<li>Distributed nature, yet rational position of authors on centralization and synchronization topic.</li>
<li>All-in-one-static-binary approach! Installing hundreds of git binaries to every freebsd-to-debian-based system, was a pain, plus I've ended up with 1.4-1.7 version span and some features (like "add -p") depend on a whole lot of stuff there, like perl and damn lot of it's modules. Unix-way is cool, but that's really more portable and distributed-way-friendly.</li>
<li>Repository in a single package, and not just a binary blob, but a freely-browsable <a title="SQLite project" href="http://sqlite.org/">sqlite db</a>. It certainly is a hell lot more convenient than path with over nine thousand blobs with sha1-names, even if the actual artifact-storage here is built basically the same way. And the performance should be actually better than the fs - with just index-selects BTree-based sqlite is as fast as filesystem, but keeping different indexes on fs is by sym-/hardlinking, and that's a pain that is never done right on fs.</li>
<li>As simple as possible internal blobs' format.</li>
<li>Actual symbolics and terminology. Git is a faceless tool, Fossil have some sort of a style, and that's nice ;)</li>
</ul>
<p>Yet there are some things I don't like about it:</p><ul>
<li>HTTP-only sync. In what kind of twisted world that can be better than ssh+pam or direct access? Can be fixed with a wrapper, I guess, but really, wtf...</li>
<li>SQLite container around generic artifact storage. Artifacts are pure data with a single sha1sum-key for it, and that is simple, solid and easy to work with anytime, but wrapped into sqlite db it suddenly depends on this db format, libs, command-line tool or language bindings, etc. All the other tables can be rebuilt just from these blobs, so they should be as accessible as possible, but I guess that'd violate whole single-file design concept and would require a lot of separate management code, a pity.</li>
</ul>
<p>But that's nothing more than a few hours' tour of the docs and basic hello-world tests, guess it all will look different after I'll use it for a while, which I'm intend to do right now.<br />In the worst case it's just a distributed issue tracker + wiki with cli interface and great versioning support in one-file package (including webserver) which is more than I can say about trac, anyway.</p>-=|||=-<p>I'm a happy <a title="git vcs" href="http://git-scm.com/">git</a> user for several years now, and the best thing about it is that I've learned how VCS-es, and git in particular, work under the hood.<br />It expanded (and in most aspects probably formed) my view on the time-series data storage - very useful knowledge for wide range of purposes from log or configuration storage to snapshotting, backups and filesystem synchronisation. Another similar revelation in this area was probably <a title="rrdtool" href="http://oss.oetiker.ch/rrdtool/">rrdtool</a>, but still on much smaller scale.</p><p>Few years back, I've kept virtually no history of my actions, only keeping my work in <a title="CVS (R.I.P.)" href="http://en.wikipedia.org/wiki/Concurrent_Versions_System">CVS</a>/<a title="Subversion" href="http://subversion.apache.org/">SVN</a>, and even that was just for ease of collaboration.<br />Today, I can easily trace, sync and transfer virtually everything that changes and is important in my system - the code I'm working on, all the configuration files, even auto-generated ones, tasks' and thoughts' lists, state-description files like lists of installed packages (local sw state) and gentoo-portage tree (global sw state), even all the logs and binary blobs like rootfs in rsync-hardlinked backups for a few past months.</p><p>Git is a great help in these tasks, but what I feel lacking there is a first - common timeline (spanning both into the past and the future) for all these data series, and second - documentation.</p><p>Solution to the first one I've yet to find.</p><p>Second one is partially solved by commit-msgs, inline comments and even this blog for the past issues and simple todo-lists (some I keep in plaintext, some in tudu app) for the future.<br />Biggest problem I see here is the lack of consistency between all these: todo-tasks end up as dropped lines in the git-log w/o any link to the past issues or reverse link to the original idea or vision, and that's just the changes.<br />Documentation for anything more than local implementation details and it's history is virtually non-existant and most times it takes a lot of effort and time to retrace the original line of thought, reasoning and purpose behind the stuff I've done (and why I've done it like that) in the past, often with the considerable gaps and eventual re-invention of the wheels and pitfalls I've already done, due to faulty biological memory.</p><p>So, today I've decided to scour over the available project and task management software to find something that ties the vcs repositories and their logs with the future tickets and some sort of expanded notes, where needed.</p><p>Starting point was actually the <a title="trac project" href="http://trac.edgewall.org/">trac</a>, which I've used quite extensively in the past and present, and is quite fond of it's outside simplicity yet fully-featured capabilities as both wiki-engine and issue tracker. Better yet, it's py and can work with vcs.<br />The downside is that it's still a separate service and web-based one at that, meaning that it's online-only, and that the content is anchored to the server I deploy it to (not to mention underlying vcs). Hell, it's centralized and laggy, and ever since git's branching and merging ideas of decentralized work took root in my brain, I have issue with that.</p><p>It just looks like a completely wrong approach for my task, yet I thought that I can probably tolerate that if there are no better options and then I've stumbled upon <a title="Fossil's fossil ;)" href="http://www.fossil-scm.org/index.html/doc/tip/www/index.wiki">Fossil VCS</a>.</p><p>The name actually rang a bell, but from a <a title="Plan 9 from Bell Labs" href="http://plan9.bell-labs.com/plan9/index.html">9p</a> universe, where it's a name for a vcs-like filesystem which was (along with venti, built on top of it) one of two primary reasons I've even looked into <a title="Plan 9 from Bell Labs" href="http://plan9.bell-labs.com/plan9/index.html">plan9</a> (the other being its 9p/styx protocol).<br />Similary-named VCS haven't disappointed me as well, at least conceptually. The main win is in the integrated ticket system and wiki, providing just the thing I need in a distributed versioned vcs environment.</p><p>Fossil's overall <a title="Fossil principles of operation" href="http://www.fossil-scm.org/index.html/doc/tip/www/pop.wiki">design principles</a> and <a title="Fossil concepts" href="http://www.fossil-scm.org/index.html/doc/tip/www/concepts.wiki">concepts</a> (plus <a title="Moar fossil theory!" href="http://www.fossil-scm.org/index.html/doc/tip/www/theory1.wiki">this</a>) are well-documented on <a title="Fossil wiki index" href="http://www.fossil-scm.org/index.html/doc/tip/www/index.wiki">it's site</a> (which is a just a fossil repo itself), and the catch-points for me were:</p><ul>
<li>Tickets and wiki, of course. Can be edited locally, synced, distributed, have local settings and appearance, based on <a title="TH1 language" href="http://www.sqliteconcepts.org/THManual.pdf">tcl-ish domain-specific language</a>.</li>
<li>Distributed nature, yet rational position of authors on centralization and synchronization topic.</li>
<li>All-in-one-static-binary approach! Installing hundreds of git binaries to every freebsd-to-debian-based system, was a pain, plus I've ended up with 1.4-1.7 version span and some features (like "add -p") depend on a whole lot of stuff there, like perl and damn lot of it's modules. Unix-way is cool, but that's really more portable and distributed-way-friendly.</li>
<li>Repository in a single package, and not just a binary blob, but a freely-browsable <a title="SQLite project" href="http://sqlite.org/">sqlite db</a>. It certainly is a hell lot more convenient than path with over nine thousand blobs with sha1-names, even if the actual artifact-storage here is built basically the same way. And the performance should be actually better than the fs - with just index-selects BTree-based sqlite is as fast as filesystem, but keeping different indexes on fs is by sym-/hardlinking, and that's a pain that is never done right on fs.</li>
<li>As simple as possible internal blobs' format.</li>
<li>Actual symbolics and terminology. Git is a faceless tool, Fossil have some sort of a style, and that's nice ;)</li>
</ul>
<p>Yet there are some things I don't like about it:</p><ul>
<li>HTTP-only sync. In what kind of twisted world that can be better than ssh+pam or direct access? Can be fixed with a wrapper, I guess, but really, wtf...</li>
<li>SQLite container around generic artifact storage. Artifacts are pure data with a single sha1sum-key for it, and that is simple, solid and easy to work with anytime, but wrapped into sqlite db it suddenly depends on this db format, libs, command-line tool or language bindings, etc. All the other tables can be rebuilt just from these blobs, so they should be as accessible as possible, but I guess that'd violate whole single-file design concept and would require a lot of separate management code, a pity.</li>
</ul>
<p>But that's nothing more than a few hours' tour of the docs and basic hello-world tests, guess it all will look different after I'll use it for a while, which I'm intend to do right now.<br />In the worst case it's just a distributed issue tracker + wiki with cli interface and great versioning support in one-file package (including webserver) which is more than I can say about trac, anyway.</p>-=||||||||||=-2010/4/LUKS-dm-crypt-rootfs-without-password-via-smartcard-=|||=-2010-04-25T06:46:59-=|||=-LUKS + dm-crypt rootfs without password via smartcard-=|||=-[u'Unix', u'Encryption']-=|||=-<p>While I'm on a vacation, I've decided to try out new distro I've been meaning to for quite awhile - <a title="exherbo home" href="http://www.exherbo.org/">exherbo</a>.<br />Mostly it's the same source-based <a title="gentoo linux home" href="http://www.gentoo.org/">gentoo-linux</a> derivative, yet it's not cloned from gentoo, like <a title="Daniel Robbins' (original gentoo author) funtoo" href="http://www.funtoo.org/">funtoo</a> or <a title="sabayon linux home" href="http://sabayonlinux.org/">sabayon</a>, but built from scratch by the guys who've seen gentoo and it's core concepts (like portage or baselayout) as quite a stagnant thing. While I don't share any of the disgust they have for gentoo legacy, the ideas incorporated in that distro sound quite interesting, but I digress...</p><p>I don't believe in fairly common practice of "trying out" something new on a VM - it just don't work for me, probably because I see it as a stupid and posintless thing on some subconscious level, so I've decided to put it onto one of my two laptops, which kinda needed a good cleanup anyway.</p><p>While at it, I thought it'd be a good idea to finally dump that stupid practice of entering fs-password on boot, yet I did like the idea of encrypted fs, especially in case of laptop, so I've needed to devise reasonably secure yet paswordless boot method.<br />I use in-kernel <a title="Linux Unified Key Setup specification" href="http://code.google.com/p/cryptsetup/">LUKS</a>-enabled <a title="dm-crypt linux transparent encryption subsystem" href="http://www.saout.de/misc/dm-crypt/">dm-crypt</a> (with the help of <a title="cryptsetup project" href="http://code.google.com/p/cryptsetup/">cryptsetup tool</a>), and I need some initrd (or init-fs) for <a title="linux logical volume manager subsystem" href="http://sourceware.org/lvm2/">LVM</a>-root anyway.</p><p>There are lots of guides on how to do that with a key from a flash drive but I don't see it particulary secure, since the key can always be copied from a drive just about anywhere, plus I don't trust the flash drives much as they seem to fail me quite often.<br />As an alternative to that, I have a smartcard-token, which can have a key that can't be copied in any way.</p><p>Problem is, of course, that I need to see some key to decrypt filesystem, so my idea was to use that key to sign some temporary data which then used to as an encryption secret.<br />Furthermore, I thought it'd be nice to have a "dynamic key" that'd change on every bootup, so even if anything would be able to snatch it from fs and use token to sign it, that data would be useless after a single reboot.</p><p>Initrd software is obviously a <a title="BusyBox toolkit" href="http://www.busybox.net/">busybox</a>, lvm and a smartcard-related stuff.<br />Smartcard I have is Alladin eToken PRO 64k, it works fine with <a title="OpenSC project" href="http://www.opensc-project.org/">OpenSC</a> but not via <a title="PC/SC middleware" href="http://www.linuxnet.com/middle.html">pcsc-lite</a>, which seem to be preferred hardware abstraction, but with <a title="OpenCT middleware" href="http://www.opensc-project.org/openct/">openct</a>, which seems a bit obsolete way. I haven't tried pcsc-lite in quite a while though, so maybe now it supports eToken as well, but since openct works fairly stable for me, I thought I'd stick with it anyway.</p><p>Boot sequence comes down to these:</p><ul>
<li>Mount pseudofs like proc/sys, get encrypted partition dev and real-rootfs signature (for findfs tool, like label or uuid) from cmdline.</li>
<li>Init openct, find smartcard in /sys by hardcoded product id and attach it to openct.</li>
<li>Mount persistent key-material storage (same /boot in my case).</li>
<li>Read "old" key, replace it with a hashed version, aka "new key".</li>
<li>Sign old key using smartcard, open fs with the resulting key.</li>
<li>Drop this key from LUKS storage, add a signed "new" key to it.</li>
<li>Kill openct processes, effectively severing link with smartcard.</li>
<li>Detect and activate LVM volume groups.</li>
<li>Find (findfs) and mount rootfs among currently-available partitions.</li>
<li>Umount proc/sys, pivot_root, chroot.</li>
<li>Here comes the target OS' init.</li>
</ul>
<p>Took me some time to assemble and test this stuff, although it was fun playing with linux+busybox mini-OS. Makes me somewhat wonder about what takes several GiBs of space in a full-fledged OS when BB contains pretty much everything in less than one MiB ;)</p><p>And it's probably a good idea to put some early check of /boot partition (hashes, mounts, whatever) into booted OS init-scripts to see if it was not altered in any significant way. Not really a guarantee that something nasty weren't done to it (and then cleaned up, for example) plus there's no proof that actual OS was booted up from it and the kernel isn't tainted in some malicious way, but should be enough against some lame tampering or pranks, should these ever happen.</p><p>Anyway, <a title="CIRD init-rd/fs" href="http://fraggod.net/svc/git/cird">here's the repo</a> with all the initrd stuff, should anyone need it.</p>-=|||=-<p>While I'm on a vacation, I've decided to try out new distro I've been meaning to for quite awhile - <a title="exherbo home" href="http://www.exherbo.org/">exherbo</a>.<br />Mostly it's the same source-based <a title="gentoo linux home" href="http://www.gentoo.org/">gentoo-linux</a> derivative, yet it's not cloned from gentoo, like <a title="Daniel Robbins' (original gentoo author) funtoo" href="http://www.funtoo.org/">funtoo</a> or <a title="sabayon linux home" href="http://sabayonlinux.org/">sabayon</a>, but built from scratch by the guys who've seen gentoo and it's core concepts (like portage or baselayout) as quite a stagnant thing. While I don't share any of the disgust they have for gentoo legacy, the ideas incorporated in that distro sound quite interesting, but I digress...</p><p>I don't believe in fairly common practice of "trying out" something new on a VM - it just don't work for me, probably because I see it as a stupid and posintless thing on some subconscious level, so I've decided to put it onto one of my two laptops, which kinda needed a good cleanup anyway.</p><p>While at it, I thought it'd be a good idea to finally dump that stupid practice of entering fs-password on boot, yet I did like the idea of encrypted fs, especially in case of laptop, so I've needed to devise reasonably secure yet paswordless boot method.<br />I use in-kernel <a title="Linux Unified Key Setup specification" href="http://code.google.com/p/cryptsetup/">LUKS</a>-enabled <a title="dm-crypt linux transparent encryption subsystem" href="http://www.saout.de/misc/dm-crypt/">dm-crypt</a> (with the help of <a title="cryptsetup project" href="http://code.google.com/p/cryptsetup/">cryptsetup tool</a>), and I need some initrd (or init-fs) for <a title="linux logical volume manager subsystem" href="http://sourceware.org/lvm2/">LVM</a>-root anyway.</p><p>There are lots of guides on how to do that with a key from a flash drive but I don't see it particulary secure, since the key can always be copied from a drive just about anywhere, plus I don't trust the flash drives much as they seem to fail me quite often.<br />As an alternative to that, I have a smartcard-token, which can have a key that can't be copied in any way.</p><p>Problem is, of course, that I need to see some key to decrypt filesystem, so my idea was to use that key to sign some temporary data which then used to as an encryption secret.<br />Furthermore, I thought it'd be nice to have a "dynamic key" that'd change on every bootup, so even if anything would be able to snatch it from fs and use token to sign it, that data would be useless after a single reboot.</p><p>Initrd software is obviously a <a title="BusyBox toolkit" href="http://www.busybox.net/">busybox</a>, lvm and a smartcard-related stuff.<br />Smartcard I have is Alladin eToken PRO 64k, it works fine with <a title="OpenSC project" href="http://www.opensc-project.org/">OpenSC</a> but not via <a title="PC/SC middleware" href="http://www.linuxnet.com/middle.html">pcsc-lite</a>, which seem to be preferred hardware abstraction, but with <a title="OpenCT middleware" href="http://www.opensc-project.org/openct/">openct</a>, which seems a bit obsolete way. I haven't tried pcsc-lite in quite a while though, so maybe now it supports eToken as well, but since openct works fairly stable for me, I thought I'd stick with it anyway.</p><p>Boot sequence comes down to these:</p><ul>
<li>Mount pseudofs like proc/sys, get encrypted partition dev and real-rootfs signature (for findfs tool, like label or uuid) from cmdline.</li>
<li>Init openct, find smartcard in /sys by hardcoded product id and attach it to openct.</li>
<li>Mount persistent key-material storage (same /boot in my case).</li>
<li>Read "old" key, replace it with a hashed version, aka "new key".</li>
<li>Sign old key using smartcard, open fs with the resulting key.</li>
<li>Drop this key from LUKS storage, add a signed "new" key to it.</li>
<li>Kill openct processes, effectively severing link with smartcard.</li>
<li>Detect and activate LVM volume groups.</li>
<li>Find (findfs) and mount rootfs among currently-available partitions.</li>
<li>Umount proc/sys, pivot_root, chroot.</li>
<li>Here comes the target OS' init.</li>
</ul>
<p>Took me some time to assemble and test this stuff, although it was fun playing with linux+busybox mini-OS. Makes me somewhat wonder about what takes several GiBs of space in a full-fledged OS when BB contains pretty much everything in less than one MiB ;)</p><p>And it's probably a good idea to put some early check of /boot partition (hashes, mounts, whatever) into booted OS init-scripts to see if it was not altered in any significant way. Not really a guarantee that something nasty weren't done to it (and then cleaned up, for example) plus there's no proof that actual OS was booted up from it and the kernel isn't tainted in some malicious way, but should be enough against some lame tampering or pranks, should these ever happen.</p><p>Anyway, <a title="CIRD init-rd/fs" href="http://fraggod.net/svc/git/cird">here's the repo</a> with all the initrd stuff, should anyone need it.</p>-=||||||||||=-2010/4/Exherbo-paludis-fossil-syncer-=|||=-2010-04-25T09:00:04-=|||=-Exherbo / paludis fossil syncer-=|||=-[u'SCM', u'Exherbo', u'Fossil']-=|||=-<p>So far I like <a title="exherbo project" href="http://www.exherbo.org/">exherbo</a> way of package management and base system layout.<br />I haven't migrated my desktop environment to it yet, but I expect it shouldn't be a problem, since I don't mind porting all the stuff I need either from gentoo or writing exheres for all I need from scratch.</p><p>First challenge I've faced though was due to my late addiction to <a title="fossil scm home" href="http://www.fossil-scm.org/">fossil scm</a>, which doesn't seem to neither be in any of exherbo repos listed in unavailable meta-repository, nor have a syncer for <a title="paaludis project" href="http://paludis.pioto.org/">paludis</a>, so I wrote my own dofossil syncer and created the repo.<br />Syncer should support both fossil+http:// and fossil+file:// protocols and tries to rebuild repository data from artifacts' storage, should it encounter any errors in process.</p><p>Repository, syncer and some instructions are <a title="fg_exheres fossil self-hosted repo" href="http://fraggod.net/svc/fossil/fg_exheres">here</a>.</p><p>Thought I'd give google some keywords, should someone be looking for the same thing, although I'd probably try to push it into paludis and/or "unavailable" repo, when (and if) I'll get a bit more solid grasp on exherbo concepts.</p>-=|||=-<p>So far I like <a title="exherbo project" href="http://www.exherbo.org/">exherbo</a> way of package management and base system layout.<br />I haven't migrated my desktop environment to it yet, but I expect it shouldn't be a problem, since I don't mind porting all the stuff I need either from gentoo or writing exheres for all I need from scratch.</p><p>First challenge I've faced though was due to my late addiction to <a title="fossil scm home" href="http://www.fossil-scm.org/">fossil scm</a>, which doesn't seem to neither be in any of exherbo repos listed in unavailable meta-repository, nor have a syncer for <a title="paaludis project" href="http://paludis.pioto.org/">paludis</a>, so I wrote my own dofossil syncer and created the repo.<br />Syncer should support both fossil+http:// and fossil+file:// protocols and tries to rebuild repository data from artifacts' storage, should it encounter any errors in process.</p><p>Repository, syncer and some instructions are <a title="fg_exheres fossil self-hosted repo" href="http://fraggod.net/svc/fossil/fg_exheres">here</a>.</p><p>Thought I'd give google some keywords, should someone be looking for the same thing, although I'd probably try to push it into paludis and/or "unavailable" repo, when (and if) I'll get a bit more solid grasp on exherbo concepts.</p>-=||||||||||=-2010/5/Music-collection-updates-feed-via-musicbrainz-and-lastfm-=|||=-2010-05-08T14:10:31-=|||=-Music collection updates feed via musicbrainz and last.fm-=|||=-[u'Web']-=|||=-<p>From time to time I accidentally bump into new releases from the artists/bands I listen to. Usually it happens on the web, since I don't like random radio selections much, and quite a wide variety of stuff I like seem to ensure that my <a title="last.fm project" href="http://last.fm/">last.fm</a> radio is a mess.<br />So, after accidentally seeing a few new albums for my collection, I've decided to remedy the situation somehow.</p><p>Naturally, subscribing to something like an unfiltered flow of new music releases isn't an option, but no music site other than last.fm out there knows the acts in my collection to track updates for those, and last.fm doesn't seem to have the functionality I need - just to list new studio releases from the artists I listened to beyond some reasonable threshold, or I just haven't been able to find it.</p><p>I thought of two options.<br />First is writing some script to submit my watch-list to some music site, so it'd notify me somehow about updates to these.<br />Second is to query the updates myself, either through some public-available APIs like last.fm, cddb, musicbrainz or even something like public atom feeds from a music portals. It seemed like a pretty obvious idea, btw, yet I've found no already-written software to do the job.</p><p>First one seemed easier, but not as entertaining as the second, plus I have virtually no knowledge to pick a site which will be best-suited for that (and I'd hate to pick a first thing from the google), and I'd probably have to post-process what this site feeds me anyway. I've decided to stick with the second way.</p><p>The main idea was to poll list of releases for every act in my collection, so the new additions would be instantly visible, as they weren't there before.<br />Such history can be kept in some db, and an easy way to track such flow would be just to dump db contents, ordered by addition timestamp, to an atom feed.</p><p>Object-db to a web content is a typical task for a web framework, so I chose to use <a title="Django project" href="http://djangoproject.com/">django</a> as a basic template for the task.</p><p>Obtaining list of local acts for my collection is easy, since I prefer not to rely on tags much (although I try to have them filled with the right data as well), I keep a strict "artist/year_album/num_-_track" directory tree, so it takes one readdir with minor post-processing for the names - replace underscores with spaces, "..., The" to "The ...", stuff like that.<br />Getting a list of an already-have releases then is just one more listing for each of the artists' dir.</p><p>To get all existing releases, there's cddb, musicbrainz and last.fm and co readily available.<br />I chose to use <a title="MusicBrainz database" href="http://musicbrainz.org/">musicbrainz db</a> (at least as the first source), since it seemed the most fitting to my purposes, shouldn't be as polluted as last.fm (which is formed arbitrarily from the tags ppl have in the files, afaik) and have clear studio-whateverelse distinction.<br />There's handy <a title="musicbrainz python api" href="http://musicbrainz.org/doc/python-musicbrainz2">official py-api</a> readily available, which I query by name for the act, then query it (if found) for available releases ("release" term is actually from there).</p><p>The next task is to compare two lists to drop the stuff I already have (local albums list) from the fetched list.<br />It'd also be quite helpful to get the release years, so all the releases which came before the ones in the collection can be safely dropped - they certainly aren't new, and there should actually be lots of them, much more than truly new ones. Mbz-db have "release events" for that, but I've quickly found that there's very little data in that section of db, alas. I wasn't happy about dropping such an obviously-effective filter so I've hooked much fatter last.fm db to query for found releases, fetching release year (plus some descriptive metadata), if there's any, and it actually worked quite nicely.<br />Another thing to consider here is a minor name differences - punctuation, typos and such. Luckily, python has a nice <a title="py difflib module reference" href="http://docs.python.org/library/difflib.html">difflib</a> right in the stdlib, which can compare the strings to get the fuzzy (to a defined threshold) matches, easy.</p><p>After that comes db storage, and there's not much to implement but a simple ORM-model definition with a few unique keys and the django will take care of the rest.</p><p>The last part is the data representation.<br />No surprises here either, django has <a title="django feeds module" href="http://docs.djangoproject.com/en/dev/ref/contrib/syndication/#ref-contrib-syndication">syndication feed framework module</a>, which can build db-to-feed mapping in a three lines of code, which is almost too easy and non-challenging, but oh well...<br />Another great view into db data is the <a title="django admin module reference" href="http://docs.djangoproject.com/en/dev/ref/contrib/admin/#ref-contrib-admin">django admin module</a>, allowing pretty filtering, searching and ordering, which is nice to have beside the feed.</p><p>One more thing I've thought of is the caching - no need to strain free databases with redundant queries, so the only non-cached data from these are the lists of the releases which should be updated from time to time, the rest can be kept in a single "seen" set of id's, so it'd be immediately obvious if the release was processed and queried before and is of no more interest now.</p><p>To summarize: the tools are <a title="django project" href="http://djangoproject.com/">django</a>, <a title="python bindings for musicbrainz api (REST version)" href="http://musicbrainz.org/doc/python-musicbrainz2">python-musicbrainz2</a> and <a title="py bindings for last.fm api" href="http://code.google.com/p/pylast/">pylast</a>; <a title="last.fm project" href="http://last.fm/">last.fm</a> and <a title="musicbrainz db" href="http://musicbrainz.org/">musicbrainz</a> - the data sources (although I might extend this list); direct result - <a title="updates for my music collection" href="http://fraggod.net/feeds/music/">this feed</a>.</p><p>Gave me several dozens of a great new releases for several dozen acts (out of about 150 in the collection) in the first pass, so I'm stuffed with a new yet favorite music for the next few months and probably any forseeable future (due to cron-based updates-grab).<br />Problem solved.</p><p><a title="sources for the whole thing" href="http://fraggod.net/oss/projects/mbz_feed.tar.gz">Code is here</a>, local acts' list is provided by a simple generator that should be easy to replace for any other source, while the rest is pretty simple and generic.<br />Feed (feeds.py) is hooked via django URLConf (urls.py) while the cron-updater script is bin/forager.py. Generic settings like cache and api keys are in the forager-app settings.py. Main processing code reside in models.py (info update from last.fm) and mbdb.py (release-list fetching). admin.py holds a bit of pretty-print settings for django admin module, like which fields should be displayed, made filterable, searchable or sortable. The rest are basic django templates.</p>-=|||=-<p>From time to time I accidentally bump into new releases from the artists/bands I listen to. Usually it happens on the web, since I don't like random radio selections much, and quite a wide variety of stuff I like seem to ensure that my <a title="last.fm project" href="http://last.fm/">last.fm</a> radio is a mess.<br />So, after accidentally seeing a few new albums for my collection, I've decided to remedy the situation somehow.</p><p>Naturally, subscribing to something like an unfiltered flow of new music releases isn't an option, but no music site other than last.fm out there knows the acts in my collection to track updates for those, and last.fm doesn't seem to have the functionality I need - just to list new studio releases from the artists I listened to beyond some reasonable threshold, or I just haven't been able to find it.</p><p>I thought of two options.<br />First is writing some script to submit my watch-list to some music site, so it'd notify me somehow about updates to these.<br />Second is to query the updates myself, either through some public-available APIs like last.fm, cddb, musicbrainz or even something like public atom feeds from a music portals. It seemed like a pretty obvious idea, btw, yet I've found no already-written software to do the job.</p><p>First one seemed easier, but not as entertaining as the second, plus I have virtually no knowledge to pick a site which will be best-suited for that (and I'd hate to pick a first thing from the google), and I'd probably have to post-process what this site feeds me anyway. I've decided to stick with the second way.</p><p>The main idea was to poll list of releases for every act in my collection, so the new additions would be instantly visible, as they weren't there before.<br />Such history can be kept in some db, and an easy way to track such flow would be just to dump db contents, ordered by addition timestamp, to an atom feed.</p><p>Object-db to a web content is a typical task for a web framework, so I chose to use <a title="Django project" href="http://djangoproject.com/">django</a> as a basic template for the task.</p><p>Obtaining list of local acts for my collection is easy, since I prefer not to rely on tags much (although I try to have them filled with the right data as well), I keep a strict "artist/year_album/num_-_track" directory tree, so it takes one readdir with minor post-processing for the names - replace underscores with spaces, "..., The" to "The ...", stuff like that.<br />Getting a list of an already-have releases then is just one more listing for each of the artists' dir.</p><p>To get all existing releases, there's cddb, musicbrainz and last.fm and co readily available.<br />I chose to use <a title="MusicBrainz database" href="http://musicbrainz.org/">musicbrainz db</a> (at least as the first source), since it seemed the most fitting to my purposes, shouldn't be as polluted as last.fm (which is formed arbitrarily from the tags ppl have in the files, afaik) and have clear studio-whateverelse distinction.<br />There's handy <a title="musicbrainz python api" href="http://musicbrainz.org/doc/python-musicbrainz2">official py-api</a> readily available, which I query by name for the act, then query it (if found) for available releases ("release" term is actually from there).</p><p>The next task is to compare two lists to drop the stuff I already have (local albums list) from the fetched list.<br />It'd also be quite helpful to get the release years, so all the releases which came before the ones in the collection can be safely dropped - they certainly aren't new, and there should actually be lots of them, much more than truly new ones. Mbz-db have "release events" for that, but I've quickly found that there's very little data in that section of db, alas. I wasn't happy about dropping such an obviously-effective filter so I've hooked much fatter last.fm db to query for found releases, fetching release year (plus some descriptive metadata), if there's any, and it actually worked quite nicely.<br />Another thing to consider here is a minor name differences - punctuation, typos and such. Luckily, python has a nice <a title="py difflib module reference" href="http://docs.python.org/library/difflib.html">difflib</a> right in the stdlib, which can compare the strings to get the fuzzy (to a defined threshold) matches, easy.</p><p>After that comes db storage, and there's not much to implement but a simple ORM-model definition with a few unique keys and the django will take care of the rest.</p><p>The last part is the data representation.<br />No surprises here either, django has <a title="django feeds module" href="http://docs.djangoproject.com/en/dev/ref/contrib/syndication/#ref-contrib-syndication">syndication feed framework module</a>, which can build db-to-feed mapping in a three lines of code, which is almost too easy and non-challenging, but oh well...<br />Another great view into db data is the <a title="django admin module reference" href="http://docs.djangoproject.com/en/dev/ref/contrib/admin/#ref-contrib-admin">django admin module</a>, allowing pretty filtering, searching and ordering, which is nice to have beside the feed.</p><p>One more thing I've thought of is the caching - no need to strain free databases with redundant queries, so the only non-cached data from these are the lists of the releases which should be updated from time to time, the rest can be kept in a single "seen" set of id's, so it'd be immediately obvious if the release was processed and queried before and is of no more interest now.</p><p>To summarize: the tools are <a title="django project" href="http://djangoproject.com/">django</a>, <a title="python bindings for musicbrainz api (REST version)" href="http://musicbrainz.org/doc/python-musicbrainz2">python-musicbrainz2</a> and <a title="py bindings for last.fm api" href="http://code.google.com/p/pylast/">pylast</a>; <a title="last.fm project" href="http://last.fm/">last.fm</a> and <a title="musicbrainz db" href="http://musicbrainz.org/">musicbrainz</a> - the data sources (although I might extend this list); direct result - <a title="updates for my music collection" href="http://fraggod.net/feeds/music/">this feed</a>.</p><p>Gave me several dozens of a great new releases for several dozen acts (out of about 150 in the collection) in the first pass, so I'm stuffed with a new yet favorite music for the next few months and probably any forseeable future (due to cron-based updates-grab).<br />Problem solved.</p><p><a title="sources for the whole thing" href="http://fraggod.net/oss/projects/mbz_feed.tar.gz">Code is here</a>, local acts' list is provided by a simple generator that should be easy to replace for any other source, while the rest is pretty simple and generic.<br />Feed (feeds.py) is hooked via django URLConf (urls.py) while the cron-updater script is bin/forager.py. Generic settings like cache and api keys are in the forager-app settings.py. Main processing code reside in models.py (info update from last.fm) and mbdb.py (release-list fetching). admin.py holds a bit of pretty-print settings for django admin module, like which fields should be displayed, made filterable, searchable or sortable. The rest are basic django templates.</p>-=||||||||||=-2010/6/Getting-rid-of-dead-bittorrent-trackers-for-rtorrent-by-scrubbing-torrent-files-=|||=-2010-06-05T08:33:14-=|||=-Getting rid of dead bittorrent trackers for rtorrent by scrubbing .torrent files-=|||=-[u'Web', u'p2p', u'Python']-=|||=-<p>If you're downloading stuff off the 'net via bt like me, then <a title="The Pirate Bay" href="http://thepiratebay.org/">TPB</a> is probably quite a familiar place to you.</p><p>Ever since the '09 trial there were a problems with TPB trackers (tracker.thepiratebay.org) - the name gets inserted into every .torrent yet it points to 127.0.0.1.<br />Lately, TPB offshot, tracker.openbittorrent.com suffers from the same problem and actually there's a lot of other trackers in .torrent files that point either at 0.0.0.0 or 127.0.0.1 these days.</p><p>As I use <a title="rtorrent project" href="http://libtorrent.rakshasa.no/">rtorrent</a>, I have an issue with this - rtorrent seem pretty dumb when it comes to tracker filtering so it queries all of them on a round-robin basis, without checking where it's name points to or if it's down for the whole app uptime, and queries take quite a lot of time to timeout, so that means at least two-minute delay in starting download (as there's TPB trackers first), plus it breaks a lot of other things like manual tracker-cycling ("t" key), since it seem to prefer only top-tier trackers and these are 100% down.<br />Now the problem with http to localhost can be solved with the firewall, of course, although it's an ugly solution, and 0.0.0.0 seem to fail pretty fast by itself, but stateless udp is still a problem.<br />Another way to tackle the issue is probably just to use a client that is capable of filtering the trackers by ip address, but that probably means some heavy-duty stuff like azureus or vanilla bittorrent which I've found pretty buggy and also surprisingly heavy in the past.</p><p>So the easiest generic solution (which will, of course, work for rtorrent) I've found is just to process the .torrent files before feeding them to the leecher app. Since I'm downloading these via firefox exclusively, and there I use <a title="FlashGot firefox extension" href="http://flashgot.net/">FlashGot</a> (not the standard "open with" interface since I also use it to download large files on remote machine w/o firefox, and afaik it's just not the way "open with" works) to drop them into an torrent bin via script, it's actually a matter of updating the link-receiving script.</p><p><a title="bencode algorithm" href="http://en.wikipedia.org/wiki/Bencode">Bencode</a> is not a mystery, plus it's pure-py implementation is actually the reference one, since it's a part of original python bittorrent client, so all I basically had to do is to rip bencode.py from it and paste the relevant part into the script.<br />The right way might've been to add dependency on the whole bittorrent client, but it's an overkill for such a simple task plus the api itself seem to be purely internal and probably a subject to change with client releases anyway.</p><p>So, to the script itself...</p><pre name="code" class="python"># URL checker
def trak_check(trak):
	if not trak: return False
	try: ip = gethostbyname(urlparse(trak).netloc.split(':', 1)[0])
	except gaierror: return True # prehaps it will resolve later on
	else: return ip not in ('127.0.0.1', '0.0.0.0')

# Actual processing
torrent = bdecode(torrent)
for tier in list(torrent['announce-list']):
	for trak in list(tier):
		if not trak_check(trak):
			tier.remove(trak)
			# print >>sys.stderr, 'Dropped:', trak
	if not tier: torrent['announce-list'].remove(tier)
# print >>sys.stderr, 'Result:', torrent['announce-list']
if not trak_check(torrent['announce']):
	torrent['announce'] = torrent['announce-list'][0][0]
torrent = bencode(torrent)
</pre><p>That, plus the simple "fetch-dump" part, if needed.<br />No magic of any kind, just a plain "announce-list" and "announce" urls check, dropping each only if it resolves to that bogus placeholder IPs.</p><p>I've wanted to make it as light as possible so no logging or optparse/argparse stuff I tend cram everywhere I can, and the extra and heavy imports like urllib/urllib2 are conditional as well. The only dependency is python (>=2.6) itself.</p><p>Basic use-case is one of these:</p>brecode.py &lt; /path/to/file.torrent &gt; /path/to/proccessed.torrent<br />brecode.py http://tpb.org/torrents/myfile.torrent &gt; /path/to/proccessed.torrent<br />brecode.py http://tpb.org/torrents/myfile.torrent -r http://tpb.org/ -c "...some cookies..." -d /path/to/torrents-bin/<br /><p>All the extra headers like cookies and referer are optional, so is the destination path (dir, basename is generated from URL). My use-case in FlashGot is this: "[URL] -r [REFERER] -c [COOKIE] -d /mnt/p2p/bt/torrents"</p><p>And <a title="brecode.py" href="http://fraggod.net/oss/bin_scrz/brecode.py">there's the script itself</a>.</p><p>Quick, dirty and inconclusive testing showed almost 100 KB/s -> 600 KB/s increase on several different (two successive tests on the same file even with clean session are obviously wrong) popular and unrelated .torrent files.<br />That's pretty inspiring. Guess now I can waste even more time on the TV-era crap than before, oh joy ;)</p>-=|||=-<p>If you're downloading stuff off the 'net via bt like me, then <a title="The Pirate Bay" href="http://thepiratebay.org/">TPB</a> is probably quite a familiar place to you.</p><p>Ever since the '09 trial there were a problems with TPB trackers (tracker.thepiratebay.org) - the name gets inserted into every .torrent yet it points to 127.0.0.1.<br />Lately, TPB offshot, tracker.openbittorrent.com suffers from the same problem and actually there's a lot of other trackers in .torrent files that point either at 0.0.0.0 or 127.0.0.1 these days.</p><p>As I use <a title="rtorrent project" href="http://libtorrent.rakshasa.no/">rtorrent</a>, I have an issue with this - rtorrent seem pretty dumb when it comes to tracker filtering so it queries all of them on a round-robin basis, without checking where it's name points to or if it's down for the whole app uptime, and queries take quite a lot of time to timeout, so that means at least two-minute delay in starting download (as there's TPB trackers first), plus it breaks a lot of other things like manual tracker-cycling ("t" key), since it seem to prefer only top-tier trackers and these are 100% down.<br />Now the problem with http to localhost can be solved with the firewall, of course, although it's an ugly solution, and 0.0.0.0 seem to fail pretty fast by itself, but stateless udp is still a problem.<br />Another way to tackle the issue is probably just to use a client that is capable of filtering the trackers by ip address, but that probably means some heavy-duty stuff like azureus or vanilla bittorrent which I've found pretty buggy and also surprisingly heavy in the past.</p><p>So the easiest generic solution (which will, of course, work for rtorrent) I've found is just to process the .torrent files before feeding them to the leecher app. Since I'm downloading these via firefox exclusively, and there I use <a title="FlashGot firefox extension" href="http://flashgot.net/">FlashGot</a> (not the standard "open with" interface since I also use it to download large files on remote machine w/o firefox, and afaik it's just not the way "open with" works) to drop them into an torrent bin via script, it's actually a matter of updating the link-receiving script.</p><p><a title="bencode algorithm" href="http://en.wikipedia.org/wiki/Bencode">Bencode</a> is not a mystery, plus it's pure-py implementation is actually the reference one, since it's a part of original python bittorrent client, so all I basically had to do is to rip bencode.py from it and paste the relevant part into the script.<br />The right way might've been to add dependency on the whole bittorrent client, but it's an overkill for such a simple task plus the api itself seem to be purely internal and probably a subject to change with client releases anyway.</p><p>So, to the script itself...</p><pre name="code" class="python"># URL checker<br />def trak_check(trak):<br />	if not trak: return False<br />	try: ip = gethostbyname(urlparse(trak).netloc.split(':', 1)[0])<br />	except gaierror: return True # prehaps it will resolve later on<br />	else: return ip not in ('127.0.0.1', '0.0.0.0')<br /><br /># Actual processing<br />torrent = bdecode(torrent)<br />for tier in list(torrent['announce-list']):<br />	for trak in list(tier):<br />		if not trak_check(trak):<br />			tier.remove(trak)<br />			# print >>sys.stderr, 'Dropped:', trak<br />	if not tier: torrent['announce-list'].remove(tier)<br /># print >>sys.stderr, 'Result:', torrent['announce-list']<br />if not trak_check(torrent['announce']):<br />	torrent['announce'] = torrent['announce-list'][0][0]<br />torrent = bencode(torrent)<br /></pre><p>That, plus the simple "fetch-dump" part, if needed.<br />No magic of any kind, just a plain "announce-list" and "announce" urls check, dropping each only if it resolves to that bogus placeholder IPs.</p><p>I've wanted to make it as light as possible so no logging or optparse/argparse stuff I tend cram everywhere I can, and the extra and heavy imports like urllib/urllib2 are conditional as well. The only dependency is python (>=2.6) itself.</p><p>Basic use-case is one of these:</p>brecode.py &lt; /path/to/file.torrent &gt; /path/to/proccessed.torrent<br />brecode.py http://tpb.org/torrents/myfile.torrent &gt; /path/to/proccessed.torrent<br />brecode.py http://tpb.org/torrents/myfile.torrent -r http://tpb.org/ -c "...some cookies..." -d /path/to/torrents-bin/<br /><p>All the extra headers like cookies and referer are optional, so is the destination path (dir, basename is generated from URL). My use-case in FlashGot is this: "[URL] -r [REFERER] -c [COOKIE] -d /mnt/p2p/bt/torrents"</p><p>And <a title="brecode.py" href="http://fraggod.net/oss/bin_scrz/brecode.py">there's the script itself</a>.</p><p>Quick, dirty and inconclusive testing showed almost 100 KB/s -> 600 KB/s increase on several different (two successive tests on the same file even with clean session are obviously wrong) popular and unrelated .torrent files.<br />That's pretty inspiring. Guess now I can waste even more time on the TV-era crap than before, oh joy ;)</p>-=||||||||||=-2010/6/Drop-in-ccrypt-replacement-for-bournal-=|||=-2010-06-13T13:35:05-=|||=-Drop-in ccrypt replacement for bournal-=|||=-[u'Bash', u'Encryption', u'Desktop']-=|||=-<p>There's one great app - <a title="bournal home" href="http://becauseinter.net/bournal/">bournal</a> ("when nobody cares what you have to say!"). Essentialy it's a bash script, providing a simple interface to edit and encrypt journal entries.<br />Idea behind it is quite opposite of blogging - keep your thoughts as far away from everyone as possible. I've used the app for quite a while, ever since I've noticed it among freshmeat release announcements. It's useful to keep some thoughts or secrets (like keys or passwords) somewhere, aside from the head, even if you'd never read these again.</p><p>Anyway, encryption there is done by the means of <a title="ccrypt util" href="http://ccrypt.sourceforge.net/">ccrypt utility</a>, which is sorta CLI for <a title="OpenSSL project" href="http://www.openssl.org/">openssl</a>. I don't get the rationale behind using it instead of openssl directly (like "openssl enc ..."), and there are actually even better options, like <a title="GnuPG project" href="http://www.gnupg.org/">gnupg</a>, which won't need a special logic to keep separate stream-cipher password, like it's done in bournal.</p><p>So today, as I needed bournal on exherbo laptop, I've faced the need to get ccrypt binary just for that purpose again. Worse yet, I have to recall and enter a password I've used there, and I don't actually need it to just encrypt an entry... as if assymetric encryption, gpg-agent, smartcards and all the other cool santa helpers don't exist yet.<br />I've decided to hack up my "ccrypt" which will use all-too-familiar gpg and won't ask me for any passwords my agent or scd already know, and in an hour or so, I've succeeded.</p><p>And <a title="drop-in ccrypt replacement via gpg" href="http://fraggod.net/oss/bin_scrz/ccrypt">here goes</a> - ccrypt, relying only on "gpg -e -r $EMAIL" and "gpg -d". EMAIL should be in the env, btw.<br />It actually works as ccencrypt, ccdecrypt, ccat as well, and can do recursive ops just like vanilla ccrypt, which is enough for bournal.</p>-=|||=-<p>There's one great app - <a title="bournal home" href="http://becauseinter.net/bournal/">bournal</a> ("when nobody cares what you have to say!"). Essentialy it's a bash script, providing a simple interface to edit and encrypt journal entries.<br />Idea behind it is quite opposite of blogging - keep your thoughts as far away from everyone as possible. I've used the app for quite a while, ever since I've noticed it among freshmeat release announcements. It's useful to keep some thoughts or secrets (like keys or passwords) somewhere, aside from the head, even if you'd never read these again.</p><p>Anyway, encryption there is done by the means of <a title="ccrypt util" href="http://ccrypt.sourceforge.net/">ccrypt utility</a>, which is sorta CLI for <a title="OpenSSL project" href="http://www.openssl.org/">openssl</a>. I don't get the rationale behind using it instead of openssl directly (like "openssl enc ..."), and there are actually even better options, like <a title="GnuPG project" href="http://www.gnupg.org/">gnupg</a>, which won't need a special logic to keep separate stream-cipher password, like it's done in bournal.</p><p>So today, as I needed bournal on exherbo laptop, I've faced the need to get ccrypt binary just for that purpose again. Worse yet, I have to recall and enter a password I've used there, and I don't actually need it to just encrypt an entry... as if assymetric encryption, gpg-agent, smartcards and all the other cool santa helpers don't exist yet.<br />I've decided to hack up my "ccrypt" which will use all-too-familiar gpg and won't ask me for any passwords my agent or scd already know, and in an hour or so, I've succeeded.</p><p>And <a title="drop-in ccrypt replacement via gpg" href="http://fraggod.net/oss/bin_scrz/ccrypt">here goes</a> - ccrypt, relying only on "gpg -e -r $EMAIL" and "gpg -d". EMAIL should be in the env, btw.<br />It actually works as ccencrypt, ccdecrypt, ccat as well, and can do recursive ops just like vanilla ccrypt, which is enough for bournal.</p>-=||||||||||=-2010/6/No-IPSec-on-a-stick-for-me--=|||=-2010-06-14T10:41:53-=|||=-No IPSec on-a-stick for me ;(-=|||=-[u'IPSec', u'Unix', u'Encryption']-=|||=-<p>Guess being a long user of stuff like <a title="OpenSSH project" href="http://www.openssh.org/">OpenSSH</a>, <a title="iproute2 tools" href="http://www.linuxfoundation.org/collaborate/workgroups/networking/iproute2">iproute2</a> and <a title="Virtual Distributed Ethernet" href="http://vde.sourceforge.net/">VDE</a> rots your brain - you start thinking that building any sort of tunnel is a bliss. Well, it's not. At least not "any sort".</p><p>This day I've dedicated to set up some basic IPSec tunnel and at first that seemed an easy task - it's long ago in kernel (<a title="KAME project" href="http://www.kame.net/">kame</a> implementation, and it's not the only one for linux), native for IPv6 (which I use in a local network), has quite a lot of publicity (and guides), it's open (and is quite simple, even with IKE magic) and there are at least three major userspace implementations: <a title="openswan project" href="http://www.openswan.org/">openswan</a>, <a title="ipsec-tools from KAME" href="http://ipsec-tools.sourceforge.net/">ipsec-tools</a> (racoon, kame) and Isakmpd. Hell, it's even supported on Windows. What's more to ask for?<br />Well, prehaps I made a bad decision starting with openswan and "native" kame NETKEY, but my experience wasn't quite a nice one.</p><p>I chose openswan because it looks like more extensive implementation than the rest, and is supported by folks like Red Hat, plus it is fairly up to date and looks constantly developed. Another cherry in it was apparent smartcard support via <a title="Mozilla Network Security Services" href="http://www.mozilla.org/projects/security/pki/nss/">nss</a> now and <a title="OpenSC project" href="http://www.opensc-project.org/">opensc</a> in the past.</p><p>First alarm bell should've been the fact that openswan actually doesn't compile without <a title="my notes on the build process" href="http://bugs.gentoo.org/show_bug.cgi?id=301813#c8">quite extensive patching</a>.<br />Latest version of it in ebuild form (which isn't quite enough for me anyway, since I use exheres these days) is 2.6.23. That's more than half a year old, and even that one is masked in gentoo due to apparent bugs and the ebuild is obviously blind-bumped from some previous version, since it doesn't take things like opensc->nss move (finalized in 2.6.23) into account.<br />Okay, hacking my own ebuild and exheres for it was fun enough, at least I've got a firm grasp of what it's capable of, but seeing pure-Makefile build system and hard-coded paths in such a package was a bit unexpected. Took me some time to deal with include paths, then lib paths, then it turned out to had an <a title="Won't build with KAME instead of KLIPS or MAST" href="https://bugs.xelerance.com/issues/1112">open bug</a> which prevents it's build on linux (wtf!?), and then it crashes on install phase due to some ever-crappy XML stuff.</p><p>At least the docs are good enough (even though it's not easy to build them), so I set up an nss db, linked smartcard to it, and got a... segfault? Right on ipsec showhostkey? Right, there's <a title="ipsec showhostkey debian bug" href="http://www.mail-archive.com/debian-bugs-closed@lists.debian.org/msg282979.html">this bug</a> in 2.6.26, although in my case it's probably another one, since the patch doesn't fixes the problem. Great!<br />Ok, gdb showed that it's something like get-nss-password failing (although it should be quite a generic interface, delegated from nss), even with nsspassword in place and nss itself working perfectly.</p><p>Scratch that, simple nss-generated keys (not even certificates) as described in the <a title="openswan configuration tutorial" href="http://wiki.openswan.org/index.php/Openswan/Configure">most basic tutorial</a>, and now it's pluto daemon crashing with just a "Jun 14 15:40:25 daemon.err&lt;27&gt; ipsec__plutorun[-]:  /usr/lib/ipsec/_plutorun: line 244:  6229 Aborted ..." line in syslog as soon as both ends off tunnel are up.<br />Oh, and of course it messes up the connection between hosts in question, so it wouldn't be too easy to ssh between them and debug the problem.</p><p>Comparing to ssh or pretty much any tunneling I've encountered to this point, it's still quite a remarkably epic fail. Guess I'll waste a bit more time on this crap, since success seems so close, but it's quite amazing how crappy such projects can still be these days. Of course, at least it's free, right?</p>-=|||=-<p>Guess being a long user of stuff like <a title="OpenSSH project" href="http://www.openssh.org/">OpenSSH</a>, <a title="iproute2 tools" href="http://www.linuxfoundation.org/collaborate/workgroups/networking/iproute2">iproute2</a> and <a title="Virtual Distributed Ethernet" href="http://vde.sourceforge.net/">VDE</a> rots your brain - you start thinking that building any sort of tunnel is a bliss. Well, it's not. At least not "any sort".</p><p>This day I've dedicated to set up some basic IPSec tunnel and at first that seemed an easy task - it's long ago in kernel (<a title="KAME project" href="http://www.kame.net/">kame</a> implementation, and it's not the only one for linux), native for IPv6 (which I use in a local network), has quite a lot of publicity (and guides), it's open (and is quite simple, even with IKE magic) and there are at least three major userspace implementations: <a title="openswan project" href="http://www.openswan.org/">openswan</a>, <a title="ipsec-tools from KAME" href="http://ipsec-tools.sourceforge.net/">ipsec-tools</a> (racoon, kame) and Isakmpd. Hell, it's even supported on Windows. What's more to ask for?<br />Well, prehaps I made a bad decision starting with openswan and "native" kame NETKEY, but my experience wasn't quite a nice one.</p><p>I chose openswan because it looks like more extensive implementation than the rest, and is supported by folks like Red Hat, plus it is fairly up to date and looks constantly developed. Another cherry in it was apparent smartcard support via <a title="Mozilla Network Security Services" href="http://www.mozilla.org/projects/security/pki/nss/">nss</a> now and <a title="OpenSC project" href="http://www.opensc-project.org/">opensc</a> in the past.</p><p>First alarm bell should've been the fact that openswan actually doesn't compile without <a title="my notes on the build process" href="http://bugs.gentoo.org/show_bug.cgi?id=301813#c8">quite extensive patching</a>.<br />Latest version of it in ebuild form (which isn't quite enough for me anyway, since I use exheres these days) is 2.6.23. That's more than half a year old, and even that one is masked in gentoo due to apparent bugs and the ebuild is obviously blind-bumped from some previous version, since it doesn't take things like opensc->nss move (finalized in 2.6.23) into account.<br />Okay, hacking my own ebuild and exheres for it was fun enough, at least I've got a firm grasp of what it's capable of, but seeing pure-Makefile build system and hard-coded paths in such a package was a bit unexpected. Took me some time to deal with include paths, then lib paths, then it turned out to had an <a title="Won't build with KAME instead of KLIPS or MAST" href="https://bugs.xelerance.com/issues/1112">open bug</a> which prevents it's build on linux (wtf!?), and then it crashes on install phase due to some ever-crappy XML stuff.</p><p>At least the docs are good enough (even though it's not easy to build them), so I set up an nss db, linked smartcard to it, and got a... segfault? Right on ipsec showhostkey? Right, there's <a title="ipsec showhostkey debian bug" href="http://www.mail-archive.com/debian-bugs-closed@lists.debian.org/msg282979.html">this bug</a> in 2.6.26, although in my case it's probably another one, since the patch doesn't fixes the problem. Great!<br />Ok, gdb showed that it's something like get-nss-password failing (although it should be quite a generic interface, delegated from nss), even with nsspassword in place and nss itself working perfectly.</p><p>Scratch that, simple nss-generated keys (not even certificates) as described in the <a title="openswan configuration tutorial" href="http://wiki.openswan.org/index.php/Openswan/Configure">most basic tutorial</a>, and now it's pluto daemon crashing with just a "Jun 14 15:40:25 daemon.err&lt;27&gt; ipsec__plutorun[-]:  /usr/lib/ipsec/_plutorun: line 244:  6229 Aborted ..." line in syslog as soon as both ends off tunnel are up.<br />Oh, and of course it messes up the connection between hosts in question, so it wouldn't be too easy to ssh between them and debug the problem.</p><p>Comparing to ssh or pretty much any tunneling I've encountered to this point, it's still quite a remarkably epic fail. Guess I'll waste a bit more time on this crap, since success seems so close, but it's quite amazing how crappy such projects can still be these days. Of course, at least it's free, right?</p>-=||||||||||=-2010/8/Home-brewed-NAS-gluster-with-sensible-replication-=|||=-2010-08-15T09:13:16-=|||=-Home-brewed NAS gluster with sensible replication-=|||=-[u'SysAdmin', u'NFS', u'Caching', u'Replication']-=|||=-<p></p><h3>Hardware</h3><p>I'd say "every sufficiently advanced user is indistinguishable from a sysadmin" (yes, it's a play on famous Arthur C Clarke's quote), and it doesn't take much of "advanced" to come to a home-server idea.<br />And I bet the main purpose for most of these aren't playground, p2p client or some http/ftp server - it's just a storage. Serving and updating the stored stuff is kinda secondary.<br />And I guess it's some sort of nature law that any storage runs outta free space sooner or later. And when this happens, just buying more space seem to be a better option than cleanup because a) "hey, there's dirt-cheap 2TB harddisks out there!" b) you just get used to having all that stuff at packet's reach.</p><p>Going down this road I found myself out of connectors on the motherboard (which is fairly spartan <a title="D510MO MB specs" href="http://www.intel.com/products/desktop/motherboards/D510MO/D510MO-overview.htm">D510MO miniITX</a>) and the slots for an extension cards (the only PCI is used by dual-port nic).<br />So I hooked up two harddisks via usb, but either the usb-sata controllers or usb's on the motherboard were faulty and controllers just hang with both leds on, vanishing from the system. Not that it's a problem - just mdraid'ed them into raid1 and when one fails like that, I just have to replug it and start raid recovery, never losing access to the data itself.</p><p>Then, to extend the storage capacity a bit further (and to provide a backup to that media content) I just bought +1 miniITX unit.<br />Now, I could've mouned two NFS'es from both units, but this approach has several disadvantages:</p><ul>
<li>Two mounts instead of one. Ok, not a big deal by itself.</li>
<li>I'd have to manage free space on these by hand, shuffling subtrees between them.</li>
<li>I need replication for some subtrees, and that complicates the previous point a bit further.</li>
<li>Some sort of HA would be nice, so I'd be able to shutdown one replica and switch to using another automatically.</li>
</ul>
<p>The obvious improvement would be to use some distributed network filesystem, and pondering on the possibilities I've decided to stick with the glusterfs due to it's flexible yet very simple "layer cake" configuration model.<br />Oh, and the most important reason to set this whole thing up - it's just fun ;)</p><p></p><h3>The Setup</h3><p>Ok, what I have is:</p><ul>
<li>Node1</li>
<li><ul>
<li>physical storage (raid1) "disk11", 300G, old and fairly "valuable" data (well, of course it's not "valuable", since I can just re-download it all from p2p, but I'd hate to do that)</li>
<li>physical disk "disk12", 150G, same stuff as disk11</li>
</ul></li>
<li>Node2</li>
<li><ul>
<li>physical disk "disk2", 1.5T, blank, to-be-filled</li>
</ul></li>
</ul>
<p>What I want is one single network storage, with db1 (disk11 + disk12) data available from any node (replicated) and new stuff which won't fit onto this storage should be writen to db2 (what's left of disk2).</p><p>With glusterfs there are several ways to do this:</p><p></p><h4>Scenario 1: fully network-aware client.</h4><p><img alt="scenario1" title="scenario1" src="http://blog.fraggod.net/static/embed/gluster_sc1.png" align="right" />That's actually the simpliest scenario - glusterfsd.vol files on "nodes" should just export local disks and client configuration ties it all together.</p><p>Pros:</p><ul>
<li>Fault tolerance. Client is fully aware of the storage hierarchy, so if one node with db1 is down, it will just use the other one.</li>
<li>If the bandwidth is better than disk i/o, reading from db1 can be potentially faster (dunno if glusterfs allows that, actually), but that's not the case, since "client" is one of the laptops and it's a slow wifi link.</li>
</ul>
<p>Cons:</p><ul>
<li>Write performance is bad - client has to push data to both nodes, and that's a big minus with my link.</li>
</ul>
<p></p><h4>Scenario 2: server-side replication.</h4><p><img alt="scenario2_1" title="scenario2_1" src="http://blog.fraggod.net/static/embed/gluster_sc2_1.png" align="right" /><img alt="" src="http://blog.fraggod.net/static/embed/gluster_sc2_2.png" align="right" />Here, "nodes" export local disks for each other and gather local+remote db1 into cluster/replicate and then export this already-replicated volume. Client just ties db2 and one of the replicated-db1 together via nufa or distribute layer.</p><p>Pros:</p><ul>
<li>Better performance.</li>
</ul>
<p>Cons:</p><ul>
<li>Single point of failure, not only for db2 (which is natural, since it's not replicated), but for db1 as well.</li>
</ul>
<p></p><h4>Scenario 3: server-side replication + fully-aware client.</h4><p><img alt="" src="http://blog.fraggod.net/static/embed/gluster_sc3.png" align="right" />db1 replicas are synced by "nodes" and client mounts all three volumes (2 x db1, 1 x db2) with either cluster/unify layer and nufa scheduler (listing both db1 replicas in "local-volume-name") or cluster/nufa.</p><p>That's the answer to obvious question I've asked myself after implementing scenario 2: "why not get rid of this single_point_of_failure just by using not single, but both replicated-db1 volumes in nufa?"<br />In this case, if node1 goes down, client won't even notice it! And if that happens to node2, files from db2 just disappear from hierarchy, but db1 will still remain fully-accessible.</p><p>But there is a problem: cluster/nufa has no support for multiple 
local-volume-name specs. cluster/unify has this support, but requires 
it's ugly "namespace-volume" hack. The solution would be to aggregate 
both db1's into a distribute layer and use it as a single volume 
alongside db2.</p><p>With aforementioned physical layout this seem to be just the best all-around case.</p><p>Pros:</p><ul>
<li>Best performance and network utilization.</li>
</ul>
<p>Cons:</p><ul>
<li>None?</li>
</ul>
<p></p><h4>Implementation</h4><p>So, scenarios 2 and 3 in terms of glusterfs, with the omission of different performance, lock layers and a few options, for the sake of clarity:</p><p><strong>node1 glusterfsd.vol:</strong></p><pre>## db1: replicated node1/node2<br />volume local-db1<br />	type storage/posix<br />	option directory /srv/storage/db1<br />end-volume<br /><br /># No client-caches here, because ops should already come aggregated<br /># from the client, and the link between servers is much fatter than the client's<br />volume node2-db1<br />	type protocol/client<br />	option remote-host node2<br />	option remote-subvolume local-db1<br />end-volume<br /><br />volume composite-db1<br />	type cluster/replicate<br />	subvolumes local-db1 node2-db1<br />end-volume<br /><br />## db: linear (nufa) db1 + db2<br />## export: local-db1 (for peers), composite-db1 (for clients)<br />volume export<br />	type protocol/server<br />	subvolumes local-db1 composite-db1<br />end-volume<br /></pre><p><strong>node2 glusterfsd.vol:</strong></p><pre>## db1: replicated node1/node2<br />volume local-db1<br />	type storage/posix<br />	option directory /srv/storage/db1<br />end-volume<br /><br /># No client-caches here, because ops should already come aggregated<br /># from the client, and the link between servers is much fatter than the client's<br />volume node1-db1<br />	type protocol/client<br />	option remote-host node1<br />	option remote-subvolume local-db1<br />end-volume<br /><br />volume composite-db1<br />	type cluster/replicate<br />	subvolumes local-db1 node1-db1<br />end-volume<br /><br />## db2: node2<br />volume db2<br />	type storage/posix<br />	option directory /srv/storage/db2<br />end-volume<br /><br />## db: linear (nufa) db1 + db2<br />## export: local-db1 (for peers), composite-db1 (for clients)<br />volume export<br />	type protocol/server<br />	subvolumes local-db1 composite-db1<br />end-volume<br /></pre><p><strong>client (replicated to both nodes):</strong></p><pre>volume node1-db1<br />	type protocol/client<br />	option remote-host node1<br />	option remote-subvolume composite-db1<br />end-volume<br /><br />volume node2-db1<br />	type protocol/client<br />	option remote-host node2<br />	option remote-subvolume composite-db1<br />end-volume<br /><br />volume db1<br />	type cluster/distribute<br />	option remote-subvolume node1-db1 node2-db1<br />end-volume<br /><br />volume db2<br />	type protocol/client<br />	option remote-host node2<br />	option remote-subvolume db2<br />end-volume<br /><br />volume db<br />	type cluster/nufa<br />	option local-volume-name db1<br />	subvolumes db1 db2<br />end-volume<br /></pre><p>Actually there's one more scenario I thought of for non-local clients - same as 2, but pushing nufa into glusterfsd.vol on "nodes", thus making client mount single unified volume on a single host via single port in a single connection.<br />Not that I really need this one, since all I need to mount from external networks is just music 99.9% of time, and <a title="NFS + FS-Cache setup" href="http://blog.fraggod.net/2010/2/Listening-to-music-over-the-net-with-authentication-and-cache">NFS + FS-Cache</a> offer more advantages there, although I might resort to it in the future, when music won't fit to db1 anymore (doubt that'll happen anytime soon).</p><p>P.S.<br />Configs are fine, but the most important thing for setting up glusterfs are these lines:</p><pre>node# /usr/sbin/glusterfsd --debug --volfile=/etc/glusterfs/glusterfsd.vol<br />client# /usr/sbin/glusterfs --debug --volfile-server=somenode /mnt/tmp<br /></pre><p></p>-=|||=-<p></p><h3>Hardware</h3><p>I'd say "every sufficiently advanced user is indistinguishable from a sysadmin" (yes, it's a play on famous Arthur C Clarke's quote), and it doesn't take much of "advanced" to come to a home-server idea.<br />And I bet the main purpose for most of these aren't playground, p2p client or some http/ftp server - it's just a storage. Serving and updating the stored stuff is kinda secondary.<br />And I guess it's some sort of nature law that any storage runs outta free space sooner or later. And when this happens, just buying more space seem to be a better option than cleanup because a) "hey, there's dirt-cheap 2TB harddisks out there!" b) you just get used to having all that stuff at packet's reach.</p><p>Going down this road I found myself out of connectors on the motherboard (which is fairly spartan <a title="D510MO MB specs" href="http://www.intel.com/products/desktop/motherboards/D510MO/D510MO-overview.htm">D510MO miniITX</a>) and the slots for an extension cards (the only PCI is used by dual-port nic).<br />So I hooked up two harddisks via usb, but either the usb-sata controllers or usb's on the motherboard were faulty and controllers just hang with both leds on, vanishing from the system. Not that it's a problem - just mdraid'ed them into raid1 and when one fails like that, I just have to replug it and start raid recovery, never losing access to the data itself.</p><p>Then, to extend the storage capacity a bit further (and to provide a backup to that media content) I just bought +1 miniITX unit.<br />Now, I could've mouned two NFS'es from both units, but this approach has several disadvantages:</p><ul>
<li>Two mounts instead of one. Ok, not a big deal by itself.</li>
<li>I'd have to manage free space on these by hand, shuffling subtrees between them.</li>
<li>I need replication for some subtrees, and that complicates the previous point a bit further.</li>
<li>Some sort of HA would be nice, so I'd be able to shutdown one replica and switch to using another automatically.</li>
</ul>
<p>The obvious improvement would be to use some distributed network filesystem, and pondering on the possibilities I've decided to stick with the glusterfs due to it's flexible yet very simple "layer cake" configuration model.<br />Oh, and the most important reason to set this whole thing up - it's just fun ;)</p><p></p><h3>The Setup</h3><p>Ok, what I have is:</p><ul>
<li>Node1</li>
<li><ul>
<li>physical storage (raid1) "disk11", 300G, old and fairly "valuable" data (well, of course it's not "valuable", since I can just re-download it all from p2p, but I'd hate to do that)</li>
<li>physical disk "disk12", 150G, same stuff as disk11</li>
</ul></li>

<li>Node2</li>
<li><ul>
<li>physical disk "disk2", 1.5T, blank, to-be-filled</li>
</ul></li>

</ul>
<p>What I want is one single network storage, with db1 (disk11 + disk12) data available from any node (replicated) and new stuff which won't fit onto this storage should be writen to db2 (what's left of disk2).</p><p>With glusterfs there are several ways to do this:</p><p></p><h4>Scenario 1: fully network-aware client.</h4><p><img alt="scenario1" title="scenario1" src="http://blog.fraggod.net/static/embed/gluster_sc1.png" align="right" />That's actually the simpliest scenario - glusterfsd.vol files on "nodes" should just export local disks and client configuration ties it all together.</p><p>Pros:</p><ul>
<li>Fault tolerance. Client is fully aware of the storage hierarchy, so if one node with db1 is down, it will just use the other one.</li>
<li>If the bandwidth is better than disk i/o, reading from db1 can be potentially faster (dunno if glusterfs allows that, actually), but that's not the case, since "client" is one of the laptops and it's a slow wifi link.</li>
</ul>
<p>Cons:</p><ul>
<li>Write performance is bad - client has to push data to both nodes, and that's a big minus with my link.</li>
</ul>
<p></p><h4>Scenario 2: server-side replication.</h4><p><img alt="scenario2_1" title="scenario2_1" src="http://blog.fraggod.net/static/embed/gluster_sc2_1.png" align="right" /><img alt="" src="http://blog.fraggod.net/static/embed/gluster_sc2_2.png" align="right" />Here, "nodes" export local disks for each other and gather local+remote db1 into cluster/replicate and then export this already-replicated volume. Client just ties db2 and one of the replicated-db1 together via nufa or distribute layer.</p><p>Pros:</p><ul>
<li>Better performance.</li>
</ul>
<p>Cons:</p><ul>
<li>Single point of failure, not only for db2 (which is natural, since it's not replicated), but for db1 as well.</li>
</ul>
<p></p><h4>Scenario 3: server-side replication + fully-aware client.</h4><p><img alt="" src="http://blog.fraggod.net/static/embed/gluster_sc3.png" align="right" />db1 replicas are synced by "nodes" and client mounts all three volumes (2 x db1, 1 x db2) with either cluster/unify layer and nufa scheduler (listing both db1 replicas in "local-volume-name") or cluster/nufa.</p><p>That's the answer to obvious question I've asked myself after implementing scenario 2: "why not get rid of this single_point_of_failure just by using not single, but both replicated-db1 volumes in nufa?"<br />In this case, if node1 goes down, client won't even notice it! And if that happens to node2, files from db2 just disappear from hierarchy, but db1 will still remain fully-accessible.</p><p>But there is a problem: cluster/nufa has no support for multiple 
local-volume-name specs. cluster/unify has this support, but requires 
it's ugly "namespace-volume" hack. The solution would be to aggregate 
both db1's into a distribute layer and use it as a single volume 
alongside db2.</p><p>With aforementioned physical layout this seem to be just the best all-around case.</p><p>Pros:</p><ul>
<li>Best performance and network utilization.</li>
</ul>
<p>Cons:</p><ul>
<li>None?</li>
</ul>
<p></p><h4>Implementation</h4><p>So, scenarios 2 and 3 in terms of glusterfs, with the omission of different performance, lock layers and a few options, for the sake of clarity:</p><p><strong>node1 glusterfsd.vol:</strong></p><pre>## db1: replicated node1/node2<br />volume local-db1<br />	type storage/posix<br />	option directory /srv/storage/db1<br />end-volume<br /><br /># No client-caches here, because ops should already come aggregated<br /># from the client, and the link between servers is much fatter than the client's<br />volume node2-db1<br />	type protocol/client<br />	option remote-host node2<br />	option remote-subvolume local-db1<br />end-volume<br /><br />volume composite-db1<br />	type cluster/replicate<br />	subvolumes local-db1 node2-db1<br />end-volume<br /><br />## db: linear (nufa) db1 + db2<br />## export: local-db1 (for peers), composite-db1 (for clients)<br />volume export<br />	type protocol/server<br />	subvolumes local-db1 composite-db1<br />end-volume<br /></pre><p><strong>node2 glusterfsd.vol:</strong></p><pre>## db1: replicated node1/node2<br />volume local-db1<br />	type storage/posix<br />	option directory /srv/storage/db1<br />end-volume<br /><br /># No client-caches here, because ops should already come aggregated<br /># from the client, and the link between servers is much fatter than the client's<br />volume node1-db1<br />	type protocol/client<br />	option remote-host node1<br />	option remote-subvolume local-db1<br />end-volume<br /><br />volume composite-db1<br />	type cluster/replicate<br />	subvolumes local-db1 node1-db1<br />end-volume<br /><br />## db2: node2<br />volume db2<br />	type storage/posix<br />	option directory /srv/storage/db2<br />end-volume<br /><br />## db: linear (nufa) db1 + db2<br />## export: local-db1 (for peers), composite-db1 (for clients)<br />volume export<br />	type protocol/server<br />	subvolumes local-db1 composite-db1<br />end-volume<br /></pre><p><strong>client (replicated to both nodes):</strong></p><pre>volume node1-db1<br />	type protocol/client<br />	option remote-host node1<br />	option remote-subvolume composite-db1<br />end-volume<br /><br />volume node2-db1<br />	type protocol/client<br />	option remote-host node2<br />	option remote-subvolume composite-db1<br />end-volume<br /><br />volume db1<br />	type cluster/distribute<br />	option remote-subvolume node1-db1 node2-db1<br />end-volume<br /><br />volume db2<br />	type protocol/client<br />	option remote-host node2<br />	option remote-subvolume db2<br />end-volume<br /><br />volume db<br />	type cluster/nufa<br />	option local-volume-name db1<br />	subvolumes db1 db2<br />end-volume<br /></pre><p>Actually there's one more scenario I thought of for non-local clients - same as 2, but pushing nufa into glusterfsd.vol on "nodes", thus making client mount single unified volume on a single host via single port in a single connection.<br />Not that I really need this one, since all I need to mount from external networks is just music 99.9% of time, and <a title="NFS + FS-Cache setup" href="http://blog.fraggod.net/2010/2/Listening-to-music-over-the-net-with-authentication-and-cache">NFS + FS-Cache</a> offer more advantages there, although I might resort to it in the future, when music won't fit to db1 anymore (doubt that'll happen anytime soon).</p><p>P.S.<br />Configs are fine, but the most important thing for setting up glusterfs are these lines:</p><pre>node# /usr/sbin/glusterfsd --debug --volfile=/etc/glusterfs/glusterfsd.vol<br />client# /usr/sbin/glusterfs --debug --volfile-server=somenode /mnt/tmp<br /></pre><p></p>-=||||||||||=-2010/9/Distributed-fault-tolerant-fs-take-2-MooseFS-=|||=-2010-09-09T15:39:56-=|||=-Distributed fault-tolerant fs take 2: MooseFS-=|||=-[u'SysAdmin', u'NFS', u'Replication']-=|||=-<p>Ok, <a title="my glusterfs setup" href="http://blog.fraggod.net/2010/8/Home-brewed-NAS-gluster-with-sensible-replication">almost one month of glusterfs</a> was too much for me to handle. That was an epic fail ;)</p><p>Random errors on start (yeah, just restart nodes a few times and it'll be fine) and during operation (same ESTALE, EIOs for whole mount, half of files just vanishing) seem to be a norm for it. I mean, that's with a perfectly sane and calm conditions - everything works, links stable.<br />A bit complicated configurations like server-side replication seem to be the cause of these, sometimes to the point when the stuff just gives ESTALE in 100% cases right from the start w/o any reason I can comprehend. And adding a third node to the system just made things worse and configuration files are even more scary.</p><p>Well, maybe I'm just out of luck or too brain-dead for it, whatever.</p><p>So, moving on, I've tried (although briefly) <a title="ceph fs" href="http://ceph.newdream.net/">ceph</a>.</p><p>Being in mainline kernel, and not just the staging part, I'd expected it to be much less work-in-progress, but as it is, it's very raw, to the point that x86_64 monitor daemon just crashes upon receiving data from plain x86. Interface is a bunch of simple shell scripts, fairly opaque operation, and the whole thing is built on such crap as boost.</p><p>Managed to get it running with two nodes, but it feels like the end of the world - one more kick and it all falls to pieces. Confirmed by the reports all over the mailing list and #ceph.</p><p>In-kernel and seemingly fast is a good mix though, so I may get back to it eventually, but now I'd rather prefer to settle on something that actually works.</p><p>Next thing in my sight was <a title="Tahoe LAFS" href="http://tahoe-lafs.org/">tahoe-lafs</a>, but it still lacks normal posix-fs interface layer, sftp interface being totally unusable on 1.8.0c3 - no permissions, cp -R fails w/ I/O error, displayed data in inconsistent even with locally-made changes, and so on. A pity, whole system design looks very cool, with raid5-like "parity" instead of plain chunk replication, and it's python!</p><p>Thus I ended up with <a title="MooseFS home" href="http://www.moosefs.org/">MooseFS</a>.</p><p>First thing to note here is incredibly simple and yet infinitely more powerful interface that probably sold me the fs right from the start. None of this configuration layers hell of gluster, just a line in hosts (so there's no need to tweak configs at all!) plus a few about what to export (subtrees-to-nets, nfs style) and where to put chunks (any fs as a simple backend key-value storage), and that's about it.</p><p>Replication? Piece a cake, and it's configured on per-tree basis, so important or compact stuff can have one replication "goal" and some heavy trash in the neighbor path have no replication at all. No chance of anything like this with gluster and it's not even documented for ceph.</p><p>Performance is totally I/O and network bound (which is totally not-the-case with tahoe, for instance), so no complaints here as well.</p><p>One more amazing thing is how simple and transparent it is:</p><pre>fraggod@anathema:~% mfsgetgoal tmp/db/softCore/_nix/os/systemrescuecd-x86-1.5.8.iso<br />tmp/db/softCore/_nix/os/systemrescuecd-x86-1.5.8.iso: 2<br /><br /></pre><pre>fraggod@anathema:~% mfsfileinfo tmp/db/softCore/_nix/os/systemrescuecd-x86-1.5.8.iso<br />tmp/db/softCore/_nix/os/systemrescuecd-x86-1.5.8.iso:<br /> chunk 0: 000000000000CE78_00000001 / (id:52856 ver:1)<br />  copy 1: 192.168.0.8:9422<br />  copy 2: 192.168.0.11:9422<br /> chunk 1: 000000000000CE79_00000001 / (id:52857 ver:1)<br />  copy 1: 192.168.0.10:9422<br />  copy 2: 192.168.0.11:9422<br /> chunk 2: 000000000000CE7A_00000001 / (id:52858 ver:1)<br />  copy 1: 192.168.0.10:9422<br />  copy 2: 192.168.0.11:9422<br /> chunk 3: 000000000000CE7B_00000001 / (id:52859 ver:1)<br />  copy 1: 192.168.0.8:9422<br />  copy 2: 192.168.0.10:9422<br /> chunk 4: 000000000000CE7C_00000001 / (id:52860 ver:1)<br />  copy 1: 192.168.0.10:9422<br />  copy 2: 192.168.0.11:9422<br /><br /></pre><pre>fraggod@anathema:~% mfsdirinfo tmp/db/softCore/_nix/os<br />tmp/db/softCore/_nix/os:<br /> inodes: 12<br /> directories: 1<br /> files: 11<br /> chunks: 175<br /> length: 11532174263<br /> size: 11533462528<br /> realsize: 23066925056<br /></pre><p>And if that's not enough, there's even a cow snaphots, trash bin with a customizable grace period and a special attributes for file caching and ownership, all totally documented along with the architectural details in manpages and on the project site.<br />Code is plain C, no shitload of deps like boost and lib*whatevermagic*, and it's really lite. Whole thing feels like a simple and solid design, not some polished turd of a *certified professionals*.<br />Yes, it's not truly-scalable, as there's a master host (with optional metalogger failover backups) with fs metadata, but there's no chance it'll be a bottleneck in my setup and comparing to a "no-way" bottlenecks of other stuff, I'd rather stick with this one.</p><p>MooseFS has yet to pass the trial of time on my makeshift "cluster", yet none of the other setups went (even remotely) as smooth as this one so far, thus I feel pretty optimistic about it.</p>-=|||=-<p>Ok, <a title="my glusterfs setup" href="http://blog.fraggod.net/2010/8/Home-brewed-NAS-gluster-with-sensible-replication">almost one month of glusterfs</a> was too much for me to handle. That was an epic fail ;)</p><p>Random errors on start (yeah, just restart nodes a few times and it'll be fine) and during operation (same ESTALE, EIOs for whole mount, half of files just vanishing) seem to be a norm for it. I mean, that's with a perfectly sane and calm conditions - everything works, links stable.<br />A bit complicated configurations like server-side replication seem to be the cause of these, sometimes to the point when the stuff just gives ESTALE in 100% cases right from the start w/o any reason I can comprehend. And adding a third node to the system just made things worse and configuration files are even more scary.</p><p>Well, maybe I'm just out of luck or too brain-dead for it, whatever.</p><p>So, moving on, I've tried (although briefly) <a title="ceph fs" href="http://ceph.newdream.net/">ceph</a>.</p><p>Being in mainline kernel, and not just the staging part, I'd expected it to be much less work-in-progress, but as it is, it's very raw, to the point that x86_64 monitor daemon just crashes upon receiving data from plain x86. Interface is a bunch of simple shell scripts, fairly opaque operation, and the whole thing is built on such crap as boost.</p><p>Managed to get it running with two nodes, but it feels like the end of the world - one more kick and it all falls to pieces. Confirmed by the reports all over the mailing list and #ceph.</p><p>In-kernel and seemingly fast is a good mix though, so I may get back to it eventually, but now I'd rather prefer to settle on something that actually works.</p><p>Next thing in my sight was <a title="Tahoe LAFS" href="http://tahoe-lafs.org/">tahoe-lafs</a>, but it still lacks normal posix-fs interface layer, sftp interface being totally unusable on 1.8.0c3 - no permissions, cp -R fails w/ I/O error, displayed data in inconsistent even with locally-made changes, and so on. A pity, whole system design looks very cool, with raid5-like "parity" instead of plain chunk replication, and it's python!</p><p>Thus I ended up with <a title="MooseFS home" href="http://www.moosefs.org/">MooseFS</a>.</p><p>First thing to note here is incredibly simple and yet infinitely more powerful interface that probably sold me the fs right from the start. None of this configuration layers hell of gluster, just a line in hosts (so there's no need to tweak configs at all!) plus a few about what to export (subtrees-to-nets, nfs style) and where to put chunks (any fs as a simple backend key-value storage), and that's about it.</p><p>Replication? Piece a cake, and it's configured on per-tree basis, so important or compact stuff can have one replication "goal" and some heavy trash in the neighbor path have no replication at all. No chance of anything like this with gluster and it's not even documented for ceph.</p><p>Performance is totally I/O and network bound (which is totally not-the-case with tahoe, for instance), so no complaints here as well.</p><p>One more amazing thing is how simple and transparent it is:</p><pre>fraggod@anathema:~% mfsgetgoal tmp/db/softCore/_nix/os/systemrescuecd-x86-1.5.8.iso<br />tmp/db/softCore/_nix/os/systemrescuecd-x86-1.5.8.iso: 2<br /><br /></pre><pre>fraggod@anathema:~% mfsfileinfo tmp/db/softCore/_nix/os/systemrescuecd-x86-1.5.8.iso<br />tmp/db/softCore/_nix/os/systemrescuecd-x86-1.5.8.iso:<br /> chunk 0: 000000000000CE78_00000001 / (id:52856 ver:1)<br />  copy 1: 192.168.0.8:9422<br />  copy 2: 192.168.0.11:9422<br /> chunk 1: 000000000000CE79_00000001 / (id:52857 ver:1)<br />  copy 1: 192.168.0.10:9422<br />  copy 2: 192.168.0.11:9422<br /> chunk 2: 000000000000CE7A_00000001 / (id:52858 ver:1)<br />  copy 1: 192.168.0.10:9422<br />  copy 2: 192.168.0.11:9422<br /> chunk 3: 000000000000CE7B_00000001 / (id:52859 ver:1)<br />  copy 1: 192.168.0.8:9422<br />  copy 2: 192.168.0.10:9422<br /> chunk 4: 000000000000CE7C_00000001 / (id:52860 ver:1)<br />  copy 1: 192.168.0.10:9422<br />  copy 2: 192.168.0.11:9422<br /><br /></pre><pre>fraggod@anathema:~% mfsdirinfo tmp/db/softCore/_nix/os<br />tmp/db/softCore/_nix/os:<br /> inodes: 12<br /> directories: 1<br /> files: 11<br /> chunks: 175<br /> length: 11532174263<br /> size: 11533462528<br /> realsize: 23066925056<br /></pre><p>And if that's not enough, there's even a cow snaphots, trash bin with a customizable grace period and a special attributes for file caching and ownership, all totally documented along with the architectural details in manpages and on the project site.<br />Code is plain C, no shitload of deps like boost and lib*whatevermagic*, and it's really lite. Whole thing feels like a simple and solid design, not some polished turd of a *certified professionals*.<br />Yes, it's not truly-scalable, as there's a master host (with optional metalogger failover backups) with fs metadata, but there's no chance it'll be a bottleneck in my setup and comparing to a "no-way" bottlenecks of other stuff, I'd rather stick with this one.</p><p>MooseFS has yet to pass the trial of time on my makeshift "cluster", yet none of the other setups went (even remotely) as smooth as this one so far, thus I feel pretty optimistic about it.</p>-=||||||||||=-2010/9/Info-feeds-=|||=-2010-09-12T10:54:29-=|||=-Info feeds-=|||=-[u'Web', u'Syndication']-=|||=-<p>Thanks to <a title="my fork of it" href="http://fraggod.net/svc/fossil/feedjack">feedjack</a>, I'm able to keep in sync with 120 feeds (many of them, like <a title="/." href="http://slashdot.org/">slashdot</a> or <a title="Reddit" href="http://www.reddit.com/">reddit</a>, being an aggregates as well), as of today. Quite a lot of stuff I couldn't even imagine handling a year ago, and a good aggregator definitely helps, keeping all the info just one click away.</p><p>And every workstation-based (desktop) aggregator I've seen is a fail:</p><ul>
<li><a title="RSSOwl project" href="http://www.rssowl.org/">RSSOwl</a>. Really nice interface and very powerful. That said, it eats more ram than a firefox!!! Hogs CPU till the whole system stutters, and eats more of it than every other app I use combined (yes, including firefox). Just keeping it in the background costs 20-30% of dualcore cpu. Changing "show new" to "show all" kills the system ;)</li>
<li><a href="http://liferea.sf.net/">liferea</a>. Horribly slow, interface hangs on any action (like fetching feed "in the background"), hogs cpu just as RSSOwl and not quite as feature-packed.</li>
<li><a href="http://www.claws-mail.org/plugin.php?plugin=rssyl">Claws-mail's RSSyl</a>. Quite nice resource-wise and very responsive, unlike dedicated software (beats me why). Pity it's also very limited interface-wise and can't reliably keep track of many of feeds by itself, constantly loosing a few if closed non-properly (most likely it's a claws-mail fault, since it affects stuff like nntp as well).</li>
<li>Emacs' gnus and newsticker. Good for a feed or two, epic fail in every way with more dozen of them.</li>
<li>Various terminal-based readers. Simply intolerable.</li>
</ul>
<p>Server-based aggregator on the other hand is a bliss - any hoards of stuff as you want it, filtered, processed, categorized and re-exported to any format (same rss, but not a hundred of them, for any other reader works as well) and I don't give a damn about how many CPU-hours it spends doing so (yet it tend to be very few, since processing and storage is done via production-grade database and modules, not some crappy ad-hoc wheel re-invention).<br />And it's simple as a doorknob, so any extra functionality can be added with no effort.</p><p>Maybe someday I'll get around to use something like Google Reader, but it's still one hell of a mess, and it's no worse than similar web-based services out there. So much for the cloud services. *sigh*</p>-=|||=-<p>Thanks to <a title="my fork of it" href="http://fraggod.net/svc/fossil/feedjack">feedjack</a>, I'm able to keep in sync with 120 feeds (many of them, like <a title="/." href="http://slashdot.org/">slashdot</a> or <a title="Reddit" href="http://www.reddit.com/">reddit</a>, being an aggregates as well), as of today. Quite a lot of stuff I couldn't even imagine handling a year ago, and a good aggregator definitely helps, keeping all the info just one click away.</p><p>And every workstation-based (desktop) aggregator I've seen is a fail:</p><ul>
<li><a title="RSSOwl project" href="http://www.rssowl.org/">RSSOwl</a>. Really nice interface and very powerful. That said, it eats more ram than a firefox!!! Hogs CPU till the whole system stutters, and eats more of it than every other app I use combined (yes, including firefox). Just keeping it in the background costs 20-30% of dualcore cpu. Changing "show new" to "show all" kills the system ;)</li>
<li><a href="http://liferea.sf.net/">liferea</a>. Horribly slow, interface hangs on any action (like fetching feed "in the background"), hogs cpu just as RSSOwl and not quite as feature-packed.</li>
<li><a href="http://www.claws-mail.org/plugin.php?plugin=rssyl">Claws-mail's RSSyl</a>. Quite nice resource-wise and very responsive, unlike dedicated software (beats me why). Pity it's also very limited interface-wise and can't reliably keep track of many of feeds by itself, constantly loosing a few if closed non-properly (most likely it's a claws-mail fault, since it affects stuff like nntp as well).</li>
<li>Emacs' gnus and newsticker. Good for a feed or two, epic fail in every way with more dozen of them.</li>
<li>Various terminal-based readers. Simply intolerable.</li>
</ul>
<p>Server-based aggregator on the other hand is a bliss - any hoards of stuff as you want it, filtered, processed, categorized and re-exported to any format (same rss, but not a hundred of them, for any other reader works as well) and I don't give a damn about how many CPU-hours it spends doing so (yet it tend to be very few, since processing and storage is done via production-grade database and modules, not some crappy ad-hoc wheel re-invention).<br />And it's simple as a doorknob, so any extra functionality can be added with no effort.</p><p>Maybe someday I'll get around to use something like Google Reader, but it's still one hell of a mess, and it's no worse than similar web-based services out there. So much for the cloud services. *sigh*</p>-=||||||||||=-2010/11/From-Baselayout-to-Systemd-setup-on-Exherbo-=|||=-2010-11-05T13:27:14-=|||=-From Baselayout to Systemd setup on Exherbo-=|||=-[u'Desktop', u'Exherbo', u'SysAdmin', u'Unix', u'Systemd']-=|||=-<p>It's been more than a week since I've migrated from sysvinit and gentoo'ish baselayout scripts to <a title="systemd project" href="http://0pointer.de/blog/projects/systemd.html">systemd</a> with it's units, and aside from few initial todos it's been surprisingly easy.<br />Nice guide for migration (which actually tipped me into trying systemd) <a title='Philantrop&squot;s "HowTo: systemd on Exherbo"' href="http://www.mailstation.de/wordpress/?p=48">can be found here</a>, in this post I'd rather summarize my experiences.</p><p>Most distributions seem to take "the legacy" way of migration, starting all the old initscripts from systemd just as sysinit did before that.<br />It makes some sense, since all the actions necessary to start the service are already written there, but most of them are no longer necessary with systemd - you don't need pidfiles, daemonization, killing code, LSB headers and most checks for other stuff... which kinda leaves nothing at all for 95% of software I've encountered!<br />I haven't really tried to adapt fedora or debian init for systemd (since my setup runs <a title="exherbo linux" href="http://www.exherbo.org/">exherbo</a>), so I may be missing some crucial points here, but it looks like even in these systems initscripts, written in simple unaugmented *sh, are unnecessary evil, each one doing the same thing in it's own crappy way.</p><p>With exherbo (or gentoo, for that matter), which has a bit more advanced init system, it's even harder to find some sense in keeping these scripts. Baselayout <a title="gentoo handbook on baselayout init" href="http://www.gentoo.org/doc/en/handbook/handbook-x86.xml?part=2&amp;chap=4">allows some cool stuff beyond simple LSB headers</a>, but does so in it's own way, typical initscript here looks like this:</p><pre>#!/sbin/runscript<br /><br />depend() {<br />	use logger<br />	need clock hostname<br />	provide cron<br />}<br /><br />start() {<br />	ebegin "Starting ${SVCNAME}"<br />	start-stop-daemon --start --pidfile ${FCRON_PIDFILE} --exec /usr/sbin/fcron -- -c ${FCRON_CONF}<br />	eend $?<br />}<br /><br />stop() {<br />	ebegin "Stopping ${SVCNAME}"<br />	start-stop-daemon --stop --pidfile ${FCRON_PIDFILE}<br />	eend $?<br />}<br /></pre><p>...with $SVCNAME taken from the script name and other vars from complimentary <em>/etc/conf.d/someservice</em> file (with sensible defaults in initscript itself).<br />Such script already allows nice and logged output (with e* commands) and clearly-defined, relatively uncluttered sections for startup and shutdown. You don't have to parse commandline arguments (although it's perfectly possible), since baselayout scripts will do that, and every daemon is accounted for via "start-stop-daemon" wrapper - it has a few simple ways to check their status via passed --pidfile or --exec lines, plus it handles forking (if necessary), IO redirection, dropping privileges and stuff like that.</p><p>All these feats lead to much more consistent init and control over services' state:</p><pre>root@damnation:~# rc-status -a<br />Runlevel: shutdown<br /> killprocs        [ stopped ]<br /> savecache        [ stopped ]<br /> mount-ro        [ stopped ]<br />Runlevel: single<br />Runlevel: nonetwork<br /> local        [ started ]<br />Runlevel: cryptinit<br /> rsyslog        [ started ]<br /> ip6tables        [ started ]<br />...<br /> twistd        [ started ]<br /> local        [ started ]<br />Runlevel: sysinit<br /> dmesg        [ started ]<br /> udev        [ started ]<br /> devfs        [ started ]<br />Runlevel: boot<br /> hwclock        [ started ]<br /> lvm        [ started ]<br />...<br /> wdd        [ started ]<br /> keymaps        [ started ]<br />Runlevel: default<br /> rsyslog        [ started ]<br /> ip6tables        [ started ]<br />...<br /> twistd        [ started ]<br /> local        [ started ]<br />Dynamic Runlevel: hotplugged<br />Dynamic Runlevel: needed<br /> sysfs        [ started ]<br /> rpc.pipefs       [ started ]<br />...<br /> rpcbind        [ started ]<br /> rpc.idmapd       [ started ]<br />Dynamic Runlevel: manual<br /></pre><p>One nice colored list of everything that should be running, is running, failed to start, crashed and whatever. One look and you know if unscheduled reboot has any surprises for you. Weird that such long-lived and supported distros as debian and fedora make these simple tasks so much harder (<em>chkconfig --list</em>? You can keep it! ;).<br />Furthermore, it provides as many custom and named runlevels as you want, as a way to flip the state of the whole system with a painless one-liner.</p><p>Now, systemd provides all of these features, in a cleaner nicer form and much more, but that makes migration from one to the other actually harder.</p><p>Systemd is developed/tested mainly on and for fedora, so abscence of LSB headers in these scripts is a problem (no dependency information), and presence of other headers (which start another scripts w/o systemd help or permission) is even more serious problem.<br />start-stop-daemon interference is also redundant and actually harmful and so is e* (and other special bl-commands and wrappers), and they won't work w/o baselayout framework.</p><p>Thus, it makes sense for systemd on exherbo to be totally independent of baselayout and it's scripts, and having a separate package option to install systemd and baselayout-specific init stuff:</p><pre>root@sacrilege:~# cave show -f acpid<br />* sys-power/acpid<br /> ::arbor   2.0.6-r2* {:0}<br /> ::installed  2.0.6-r2 {:0}<br /> sys-power/acpid-2.0.6-r2:0::installed<br /> Description<br />acpid is designed to notify user-space programs of ACPI events. It will<br />will attempt to connect to the Linux kernel via the input layer and<br />netlink. When an ACPI event is received from one of these sources, acpid<br />will examine a list of rules, and execute the rules that match the event.<br /> Homepage  http://tedfelix.com/linux/acpid-netlink.html<br /> Summary  A configurable ACPI policy daemon for Linux<br /> From repositories arbor<br /> Installed time Thu Oct 21 23:11:55 YEKST 2010<br /> Installed using paludis-0.55.0-git-0.54.2-44-g203a470<br /> Licences  GPL-2<br /> Options  (-baselayout) (systemd) build_options: -trace<br /> sys-power/acpid-2.0.6-r2:0::arbor<br /> Homepage  http://tedfelix.com/linux/acpid-netlink.html<br /> Summary  A configurable ACPI policy daemon for Linux<br /> Description  acpid is designed to notify user-space programs of ACPI events. It will will attempt to connect to the Linux kernel via the input layer and netlink. When an ACPI event is received from one of these sources, acpid will examine a list of rules, and execute the rules that match the event.<br /> Options  -baselayout systemd build_options: -recommended_tests split strip jobs -trace -preserve_work<br /> Overridden Masks<br />  Supported platforms ~amd64 ~x86<br /></pre><p>So, basically, the migration to systemd consists of enabling the option and flipping the "eclectic init" switch:</p><pre>root@sacrilege:~# eclectic init list<br />Available providers for init:<br /> [1] systemd *<br /> [2] sysvinit<br /></pre><p>Of course, in reality things are little more complicated, and breaking init is quite undesirable prospect, so I took advantage of virtualization capabilities of cpu on my new laptop and made a complete virtual replica of the system.<br />Things got a bit more complicated since <a title="initrd for dm-crypt/lvm setup" href="http://blog.fraggod.net/2010/4/LUKS-dm-crypt-rootfs-without-password-via-smartcard">dm-crypt/lvm setup I've described before</a>, but overally creating such a vm is trivial:</p><ul>
<li>A dedicated lv for whole setup.</li>
<li>luksFormat it, so it'd represent an encrypted "raw" partition.</li>
<li>pvcreate / vgcreate / lvcreate / mkfs on top of it, identical (although much smaller) to original system.</li>
<li>A script to mount all these and rsync the "original" system to this replica, with a few post-sync hooks to make some vm-specific changes - different vg name, no extra devices for media content, simplier passwords.</li>
</ul>
<p><a title="script to manage/update/run vm replica of a system" href="http://fraggod.net/oss/bin_scrz/quasictl.sh">I have this script here</a>, list of "exclusions" for rsync is actually taken from backup scripts, since it's designed to omit various heavy and non-critical paths like caches, spools and debugging info, plus there's not much point syncing most /home contents. All in all, whole setup is about 2-3G and rsync makes a fast job of updating it.<br />vm (qemu-kvm) startup is right there in the <a title="script to manage/update/run vm replica of a system" href="http://fraggod.net/oss/bin_scrz/quasictl.sh">script</a> and uses exactly the same kernel/initrd as the host machine, although I skip encryption part (via kernel cmdline) for faster bootup.</p><p>And the first launch gave quite a mixed result: systemd fired a bunch of basic stuff at once, then hanged for about a minute before presenting a getty. After login, it turned out that none of the filesystems in <em>/etc/fstab</em> got mounted.<br />Systemd handles mounts in quite a clever (and fully documented) way - from each device in fstab it creates a "XXX.device" unit, "fsck@XXX.service", and either "XXX.mount" or "XXX.automount" from mountpoints (depending on optional "comment=" mount opts). All the autogenerated "XXX.mount" units without explicit "noauto" option will get started on boot.<br />And they do get started, hence that hang. Each .mount, naturally, depends on corresponding .device unit (with fsck in between), and these are considered started when udev issues an event.<br />In my case, even after exherbo-specific lvm2.service, which does <em>vgscan</em> and <em>vgchange -ay</em> stuff, these events are never generated, so .device units hang for 60 seconds and systemd marks them as "failed" as well as dependent .mount units.<br />It looks like my local problem, since I actually activate and use these in initrd, so I just worked around it by adding "ExecStart=-/sbin/udevadm trigger --subsystem-match=block --sysname-match=dm-*" line to lvm2.service. That generated the event in parallel to still-waiting .device units, so they got started, then fsck, then just mounted.</p><p>While this may look a bit like a problem, it's quite surprising how transparent and easy-to-debug whole process is, regardless of it's massively-parallel nature - all the information is available via "systemctl" and it's show/status commands, all the services are organized (and monitored) in systemd-cgls tree, and can be easily debugged with systemd monitoring and console/dmesg-logging features:</p><pre>root@sacrilege:~# systemd-cgls<br />├ 2 [kthreadd]<br />├ 3 [ksoftirqd/0]<br />├ 6 [migration/0]<br />├ 7 [migration/1]<br />├ 9 [ksoftirqd/1]<br />├ 10 [kworker/0:1]<br />...<br />├ 2688 [kworker/0:2]<br />├ 2700 [kworker/u:0]<br />├ 2728 [kworker/u:2]<br />├ 2729 [kworker/u:4]<br />├ user<br />│ └ fraggod<br />│ └ no-session<br />│ ├ 1444 /bin/sh /usr/bin/startx<br />│ ├ 1462 xinit /home/fraggod/.xinitrc -- /etc/X11/xinit/xserverrc :0 -auth /home/fraggod/.serveraut...<br />...<br />│ ├ 2407 ssh root@anathema -Y<br />│ └ 2751 systemd-cgls<br />└ systemd-1<br /> ├ 1 /sbin/init<br /> ├ var-src.mount<br /> ├ var-tmp.mount<br /> ├ ipsec.service<br /> │ ├ 1059 /bin/sh /usr/lib/ipsec/_plutorun --debug --uniqueids yes --force_busy no --nocrsend no --str...<br /> │ ├ 1060 logger -s -p daemon.error -t ipsec__plutorun<br /> │ ├ 1061 /bin/sh /usr/lib/ipsec/_plutorun --debug --uniqueids yes --force_busy no --nocrsend no --str...<br /> │ ├ 1062 /bin/sh /usr/lib/ipsec/_plutoload --wait no --post<br /> │ ├ 1064 /usr/libexec/ipsec/pluto --nofork --secretsfile /etc/ipsec.secrets --ipsecdir /etc/ipsec.d -...<br /> │ ├ 1069 pluto helper # 0<br /> │ ├ 1070 pluto helper # 1<br /> │ ├ 1071 pluto helper # 2<br /> │ └ 1223 _pluto_adns<br /> ├ sys-kernel-debug.mount<br /> ├ var-cache-fscache.mount<br /> ├ net@.service<br /> ├ rpcidmapd.service<br /> │ └ 899 /usr/sbin/rpc.idmapd -f<br /> ├ rpcstatd.service<br /> │ └ 892 /sbin/rpc.statd -F<br /> ├ rpcbind.service<br /> │ └ 890 /sbin/rpcbind -d<br /> ├ wpa_supplicant.service<br /> │ └ 889 /usr/sbin/wpa_supplicant -c /etc/wpa_supplicant/wpa_supplicant.conf -u -Dwext -iwlan0<br /> ├ cachefilesd.service<br /> │ └ 883 /sbin/cachefilesd -n<br /> ├ dbus.service<br /> │ └ 784 /usr/bin/dbus-daemon --system --address=systemd: --nofork --systemd-activation<br /> ├ acpid.service<br /> │ └ 775 /usr/sbin/acpid -f<br /> ├ openct.service<br /> │ └ 786 /usr/sbin/ifdhandler -H -p etoken64 usb /dev/bus/usb/002/003<br /> ├ ntpd.service<br /> │ └ 772 /usr/sbin/ntpd -u ntp:ntp -n -g -p /var/run/ntpd.pid<br /> ├ bluetooth.service<br /> │ ├ 771 /usr/sbin/bluetoothd -n<br /> │ └ 1469 [khidpd_046db008]<br /> ├ syslog.service<br /> │ └ 768 /usr/sbin/rsyslogd -n -c5 -6<br /> ├ getty@.service<br /> │ ├ tty1<br /> │ │ └ 1451 /sbin/agetty 38400 tty1<br /> │ ├ tty3<br /> │ │ └ 766 /sbin/agetty 38400 tty3<br /> │ ├ tty6<br /> │ │ └ 765 /sbin/agetty 38400 tty6<br /> │ ├ tty5<br /> │ │ └ 763 /sbin/agetty 38400 tty5<br /> │ ├ tty4<br /> │ │ └ 762 /sbin/agetty 38400 tty4<br /> │ └ tty2<br /> │ └ 761 /sbin/agetty 38400 tty2<br /> ├ postfix.service<br /> │ ├ 872 /usr/lib/postfix/master<br /> │ ├ 877 qmgr -l -t fifo -u<br /> │ └ 2631 pickup -l -t fifo -u<br /> ├ fcron.service<br /> │ └ 755 /usr/sbin/fcron -f<br /> ├ var-cache.mount<br /> ├ var-run.mount<br /> ├ var-lock.mount<br /> ├ var-db-paludis.mount<br /> ├ home-fraggod-.spring.mount<br /> ├ etc-core.mount<br /> ├ var.mount<br /> ├ home.mount<br /> ├ boot.mount<br /> ├ fsck@.service<br /> ├ dev-mapper-prime\x2dswap.swap<br /> ├ dev-mqueue.mount<br /> ├ dev-hugepages.mount<br /> ├ udev.service<br /> │ ├ 240 /sbin/udevd<br /> │ ├ 639 /sbin/udevd<br /> │ └ 640 /sbin/udevd<br /> ├ systemd-logger.service<br /> │ └ 228 //lib/systemd/systemd-logger<br /> └ tmp.mount<br /><br /></pre><pre>root@sacrilege:~# systemctl status ipsec.service<br />ipsec.service - IPSec (openswan)<br />  Loaded: loaded (/etc/systemd/system/ipsec.service)<br />  Active: active (running) since Fri, 05 Nov 2010 15:16:54 +0500; 2h 16min ago<br />  Process: 981 (/usr/sbin/ipsec setup start, code=exited, status=0/SUCCESS)<br />  Process: 974 (/bin/sleep 10, code=exited, status=0/SUCCESS)<br />  CGroup: name=systemd:/systemd-1/ipsec.service<br />   ├ 1059 /bin/sh /usr/lib/ipsec/_plutorun --debug --uniqueids yes --force_busy no --noc...<br />   ├ 1060 logger -s -p daemon.error -t ipsec__plutorun<br />   ├ 1061 /bin/sh /usr/lib/ipsec/_plutorun --debug --uniqueids yes --force_busy no --noc...<br />   ├ 1062 /bin/sh /usr/lib/ipsec/_plutoload --wait no --post<br />   ├ 1064 /usr/libexec/ipsec/pluto --nofork --secretsfile /etc/ipsec.secrets --ipsecdir ...<br />   ├ 1069 pluto helper # 0<br />   ├ 1070 pluto helper # 1<br />   ├ 1071 pluto helper # 2<br />   └ 1223 _pluto_adns<br /></pre><p>It's not just hacking at some opaque *sh hacks (like debian init or even interactive-mode baselayout) and takes so little effort to the point that it's really enjoyable process.</p><p>But making it mount and start all the default (and available) stuff is not the end of it, because there are plenty of services not yet adapted to systemd.<br />I actually expected some (relatively) hard work here, because there are quite a few initscripts in /etc/init.d, even on a desktop machine, but once again, I was in for a nice surprise, since systemd just makes all the work go away. All you need to do is to decide on the ordering (or copy it from baselayout scripts) and put an appropriate "Type=" and "ExecStart=" lines in .service file. That's all there is, really.<br />After that, of course, complete bootup-shutdown test on a vm is in order, and everything "just works" as it is supposed to.<br />Bootup on a real hardware is exactly the same as vm, no surprises here. "udevadm trigger" seem to be necessary as well, proving validity of vm model.</p><p>Systemd boot time is way faster than sysvinit, as it is supposed to, although I don't really care, since reboot is seldom necessary here.</p><p>As a summary, I'd recommend everyone to give systemd a try, or at least get familiar with <a title="systemd rationale" href="http://0pointer.de/blog/projects/systemd.html">it's rationale</a> and <a title="systemd manpages" href="http://0pointer.de/public/systemd-man/">features</a> (plus this three-part blog series: <a title="systemd for admins 1" href="http://0pointer.de/blog/projects/systemd-for-admins-1.html">one</a>, <a title="systemd for admins 2" href="http://0pointer.de/blog/projects/systemd-for-admins-2.html">two</a>, <a title="systemd for admins 3" href="http://0pointer.de/blog/projects/systemd-for-admins-3.html">three</a>).<br /><a title="my units for systemd" href="http://fraggod.net/svc/git/systemd">My units</a> aren't perfect (and I'll probably update network-related stuff to use <a title="ConnMan project" href="http://connman.net/">ConnMan</a>), but if you're lazy, <a title="my units for systemd" href="http://fraggod.net/svc/git/systemd">grab them here</a>. Also, <a title="systemd-arch-units" href="http://github.com/falconindy/systemd-arch-units">here is a repo with units for archlinux</a>, which I loosely used as a reference point along with <em>/lib/systemd</em> contents.</p>-=|||=-<p>It's been more than a week since I've migrated from sysvinit and gentoo'ish baselayout scripts to <a title="systemd project" href="http://0pointer.de/blog/projects/systemd.html">systemd</a> with it's units, and aside from few initial todos it's been surprisingly easy.<br />Nice guide for migration (which actually tipped me into trying systemd) <a title='Philantrop&squot;s "HowTo: systemd on Exherbo"' href="http://www.mailstation.de/wordpress/?p=48">can be found here</a>, in this post I'd rather summarize my experiences.</p><p>Most distributions seem to take "the legacy" way of migration, starting all the old initscripts from systemd just as sysinit did before that.<br />It makes some sense, since all the actions necessary to start the service are already written there, but most of them are no longer necessary with systemd - you don't need pidfiles, daemonization, killing code, LSB headers and most checks for other stuff... which kinda leaves nothing at all for 95% of software I've encountered!<br />I haven't really tried to adapt fedora or debian init for systemd (since my setup runs <a title="exherbo linux" href="http://www.exherbo.org/">exherbo</a>), so I may be missing some crucial points here, but it looks like even in these systems initscripts, written in simple unaugmented *sh, are unnecessary evil, each one doing the same thing in it's own crappy way.</p><p>With exherbo (or gentoo, for that matter), which has a bit more advanced init system, it's even harder to find some sense in keeping these scripts. Baselayout <a title="gentoo handbook on baselayout init" href="http://www.gentoo.org/doc/en/handbook/handbook-x86.xml?part=2&amp;chap=4">allows some cool stuff beyond simple LSB headers</a>, but does so in it's own way, typical initscript here looks like this:</p><pre>#!/sbin/runscript<br /><br />depend() {<br />	use logger<br />	need clock hostname<br />	provide cron<br />}<br /><br />start() {<br />	ebegin "Starting ${SVCNAME}"<br />	start-stop-daemon --start --pidfile ${FCRON_PIDFILE} --exec /usr/sbin/fcron -- -c ${FCRON_CONF}<br />	eend $?<br />}<br /><br />stop() {<br />	ebegin "Stopping ${SVCNAME}"<br />	start-stop-daemon --stop --pidfile ${FCRON_PIDFILE}<br />	eend $?<br />}<br /></pre><p>...with $SVCNAME taken from the script name and other vars from complimentary <em>/etc/conf.d/someservice</em> file (with sensible defaults in initscript itself).<br />Such script already allows nice and logged output (with e* commands) and clearly-defined, relatively uncluttered sections for startup and shutdown. You don't have to parse commandline arguments (although it's perfectly possible), since baselayout scripts will do that, and every daemon is accounted for via "start-stop-daemon" wrapper - it has a few simple ways to check their status via passed --pidfile or --exec lines, plus it handles forking (if necessary), IO redirection, dropping privileges and stuff like that.</p><p>All these feats lead to much more consistent init and control over services' state:</p><pre>root@damnation:~# rc-status -a<br />Runlevel: shutdown<br /> killprocs        [ stopped ]<br /> savecache        [ stopped ]<br /> mount-ro        [ stopped ]<br />Runlevel: single<br />Runlevel: nonetwork<br /> local        [ started ]<br />Runlevel: cryptinit<br /> rsyslog        [ started ]<br /> ip6tables        [ started ]<br />...<br /> twistd        [ started ]<br /> local        [ started ]<br />Runlevel: sysinit<br /> dmesg        [ started ]<br /> udev        [ started ]<br /> devfs        [ started ]<br />Runlevel: boot<br /> hwclock        [ started ]<br /> lvm        [ started ]<br />...<br /> wdd        [ started ]<br /> keymaps        [ started ]<br />Runlevel: default<br /> rsyslog        [ started ]<br /> ip6tables        [ started ]<br />...<br /> twistd        [ started ]<br /> local        [ started ]<br />Dynamic Runlevel: hotplugged<br />Dynamic Runlevel: needed<br /> sysfs        [ started ]<br /> rpc.pipefs       [ started ]<br />...<br /> rpcbind        [ started ]<br /> rpc.idmapd       [ started ]<br />Dynamic Runlevel: manual<br /></pre><p>One nice colored list of everything that should be running, is running, failed to start, crashed and whatever. One look and you know if unscheduled reboot has any surprises for you. Weird that such long-lived and supported distros as debian and fedora make these simple tasks so much harder (<em>chkconfig --list</em>? You can keep it! ;).<br />Furthermore, it provides as many custom and named runlevels as you want, as a way to flip the state of the whole system with a painless one-liner.</p><p>Now, systemd provides all of these features, in a cleaner nicer form and much more, but that makes migration from one to the other actually harder.</p><p>Systemd is developed/tested mainly on and for fedora, so abscence of LSB headers in these scripts is a problem (no dependency information), and presence of other headers (which start another scripts w/o systemd help or permission) is even more serious problem.<br />start-stop-daemon interference is also redundant and actually harmful and so is e* (and other special bl-commands and wrappers), and they won't work w/o baselayout framework.</p><p>Thus, it makes sense for systemd on exherbo to be totally independent of baselayout and it's scripts, and having a separate package option to install systemd and baselayout-specific init stuff:</p><pre>root@sacrilege:~# cave show -f acpid<br />* sys-power/acpid<br /> ::arbor   2.0.6-r2* {:0}<br /> ::installed  2.0.6-r2 {:0}<br /> sys-power/acpid-2.0.6-r2:0::installed<br /> Description<br />acpid is designed to notify user-space programs of ACPI events. It will<br />will attempt to connect to the Linux kernel via the input layer and<br />netlink. When an ACPI event is received from one of these sources, acpid<br />will examine a list of rules, and execute the rules that match the event.<br /> Homepage  http://tedfelix.com/linux/acpid-netlink.html<br /> Summary  A configurable ACPI policy daemon for Linux<br /> From repositories arbor<br /> Installed time Thu Oct 21 23:11:55 YEKST 2010<br /> Installed using paludis-0.55.0-git-0.54.2-44-g203a470<br /> Licences  GPL-2<br /> Options  (-baselayout) (systemd) build_options: -trace<br /> sys-power/acpid-2.0.6-r2:0::arbor<br /> Homepage  http://tedfelix.com/linux/acpid-netlink.html<br /> Summary  A configurable ACPI policy daemon for Linux<br /> Description  acpid is designed to notify user-space programs of ACPI events. It will will attempt to connect to the Linux kernel via the input layer and netlink. When an ACPI event is received from one of these sources, acpid will examine a list of rules, and execute the rules that match the event.<br /> Options  -baselayout systemd build_options: -recommended_tests split strip jobs -trace -preserve_work<br /> Overridden Masks<br />  Supported platforms ~amd64 ~x86<br /></pre><p>So, basically, the migration to systemd consists of enabling the option and flipping the "eclectic init" switch:</p><pre>root@sacrilege:~# eclectic init list<br />Available providers for init:<br /> [1] systemd *<br /> [2] sysvinit<br /></pre><p>Of course, in reality things are little more complicated, and breaking init is quite undesirable prospect, so I took advantage of virtualization capabilities of cpu on my new laptop and made a complete virtual replica of the system.<br />Things got a bit more complicated since <a title="initrd for dm-crypt/lvm setup" href="http://blog.fraggod.net/2010/4/LUKS-dm-crypt-rootfs-without-password-via-smartcard">dm-crypt/lvm setup I've described before</a>, but overally creating such a vm is trivial:</p><ul>
<li>A dedicated lv for whole setup.</li>
<li>luksFormat it, so it'd represent an encrypted "raw" partition.</li>
<li>pvcreate / vgcreate / lvcreate / mkfs on top of it, identical (although much smaller) to original system.</li>
<li>A script to mount all these and rsync the "original" system to this replica, with a few post-sync hooks to make some vm-specific changes - different vg name, no extra devices for media content, simplier passwords.</li>
</ul>
<p><a title="script to manage/update/run vm replica of a system" href="http://fraggod.net/oss/bin_scrz/quasictl.sh">I have this script here</a>, list of "exclusions" for rsync is actually taken from backup scripts, since it's designed to omit various heavy and non-critical paths like caches, spools and debugging info, plus there's not much point syncing most /home contents. All in all, whole setup is about 2-3G and rsync makes a fast job of updating it.<br />vm (qemu-kvm) startup is right there in the <a title="script to manage/update/run vm replica of a system" href="http://fraggod.net/oss/bin_scrz/quasictl.sh">script</a> and uses exactly the same kernel/initrd as the host machine, although I skip encryption part (via kernel cmdline) for faster bootup.</p><p>And the first launch gave quite a mixed result: systemd fired a bunch of basic stuff at once, then hanged for about a minute before presenting a getty. After login, it turned out that none of the filesystems in <em>/etc/fstab</em> got mounted.<br />Systemd handles mounts in quite a clever (and fully documented) way - from each device in fstab it creates a "XXX.device" unit, "fsck@XXX.service", and either "XXX.mount" or "XXX.automount" from mountpoints (depending on optional "comment=" mount opts). All the autogenerated "XXX.mount" units without explicit "noauto" option will get started on boot.<br />And they do get started, hence that hang. Each .mount, naturally, depends on corresponding .device unit (with fsck in between), and these are considered started when udev issues an event.<br />In my case, even after exherbo-specific lvm2.service, which does <em>vgscan</em> and <em>vgchange -ay</em> stuff, these events are never generated, so .device units hang for 60 seconds and systemd marks them as "failed" as well as dependent .mount units.<br />It looks like my local problem, since I actually activate and use these in initrd, so I just worked around it by adding "ExecStart=-/sbin/udevadm trigger --subsystem-match=block --sysname-match=dm-*" line to lvm2.service. That generated the event in parallel to still-waiting .device units, so they got started, then fsck, then just mounted.</p><p>While this may look a bit like a problem, it's quite surprising how transparent and easy-to-debug whole process is, regardless of it's massively-parallel nature - all the information is available via "systemctl" and it's show/status commands, all the services are organized (and monitored) in systemd-cgls tree, and can be easily debugged with systemd monitoring and console/dmesg-logging features:</p><pre>root@sacrilege:~# systemd-cgls<br />├ 2 [kthreadd]<br />├ 3 [ksoftirqd/0]<br />├ 6 [migration/0]<br />├ 7 [migration/1]<br />├ 9 [ksoftirqd/1]<br />├ 10 [kworker/0:1]<br />...<br />├ 2688 [kworker/0:2]<br />├ 2700 [kworker/u:0]<br />├ 2728 [kworker/u:2]<br />├ 2729 [kworker/u:4]<br />├ user<br />│ └ fraggod<br />│ └ no-session<br />│ ├ 1444 /bin/sh /usr/bin/startx<br />│ ├ 1462 xinit /home/fraggod/.xinitrc -- /etc/X11/xinit/xserverrc :0 -auth /home/fraggod/.serveraut...<br />...<br />│ ├ 2407 ssh root@anathema -Y<br />│ └ 2751 systemd-cgls<br />└ systemd-1<br /> ├ 1 /sbin/init<br /> ├ var-src.mount<br /> ├ var-tmp.mount<br /> ├ ipsec.service<br /> │ ├ 1059 /bin/sh /usr/lib/ipsec/_plutorun --debug --uniqueids yes --force_busy no --nocrsend no --str...<br /> │ ├ 1060 logger -s -p daemon.error -t ipsec__plutorun<br /> │ ├ 1061 /bin/sh /usr/lib/ipsec/_plutorun --debug --uniqueids yes --force_busy no --nocrsend no --str...<br /> │ ├ 1062 /bin/sh /usr/lib/ipsec/_plutoload --wait no --post<br /> │ ├ 1064 /usr/libexec/ipsec/pluto --nofork --secretsfile /etc/ipsec.secrets --ipsecdir /etc/ipsec.d -...<br /> │ ├ 1069 pluto helper # 0<br /> │ ├ 1070 pluto helper # 1<br /> │ ├ 1071 pluto helper # 2<br /> │ └ 1223 _pluto_adns<br /> ├ sys-kernel-debug.mount<br /> ├ var-cache-fscache.mount<br /> ├ net@.service<br /> ├ rpcidmapd.service<br /> │ └ 899 /usr/sbin/rpc.idmapd -f<br /> ├ rpcstatd.service<br /> │ └ 892 /sbin/rpc.statd -F<br /> ├ rpcbind.service<br /> │ └ 890 /sbin/rpcbind -d<br /> ├ wpa_supplicant.service<br /> │ └ 889 /usr/sbin/wpa_supplicant -c /etc/wpa_supplicant/wpa_supplicant.conf -u -Dwext -iwlan0<br /> ├ cachefilesd.service<br /> │ └ 883 /sbin/cachefilesd -n<br /> ├ dbus.service<br /> │ └ 784 /usr/bin/dbus-daemon --system --address=systemd: --nofork --systemd-activation<br /> ├ acpid.service<br /> │ └ 775 /usr/sbin/acpid -f<br /> ├ openct.service<br /> │ └ 786 /usr/sbin/ifdhandler -H -p etoken64 usb /dev/bus/usb/002/003<br /> ├ ntpd.service<br /> │ └ 772 /usr/sbin/ntpd -u ntp:ntp -n -g -p /var/run/ntpd.pid<br /> ├ bluetooth.service<br /> │ ├ 771 /usr/sbin/bluetoothd -n<br /> │ └ 1469 [khidpd_046db008]<br /> ├ syslog.service<br /> │ └ 768 /usr/sbin/rsyslogd -n -c5 -6<br /> ├ getty@.service<br /> │ ├ tty1<br /> │ │ └ 1451 /sbin/agetty 38400 tty1<br /> │ ├ tty3<br /> │ │ └ 766 /sbin/agetty 38400 tty3<br /> │ ├ tty6<br /> │ │ └ 765 /sbin/agetty 38400 tty6<br /> │ ├ tty5<br /> │ │ └ 763 /sbin/agetty 38400 tty5<br /> │ ├ tty4<br /> │ │ └ 762 /sbin/agetty 38400 tty4<br /> │ └ tty2<br /> │ └ 761 /sbin/agetty 38400 tty2<br /> ├ postfix.service<br /> │ ├ 872 /usr/lib/postfix/master<br /> │ ├ 877 qmgr -l -t fifo -u<br /> │ └ 2631 pickup -l -t fifo -u<br /> ├ fcron.service<br /> │ └ 755 /usr/sbin/fcron -f<br /> ├ var-cache.mount<br /> ├ var-run.mount<br /> ├ var-lock.mount<br /> ├ var-db-paludis.mount<br /> ├ home-fraggod-.spring.mount<br /> ├ etc-core.mount<br /> ├ var.mount<br /> ├ home.mount<br /> ├ boot.mount<br /> ├ fsck@.service<br /> ├ dev-mapper-prime\x2dswap.swap<br /> ├ dev-mqueue.mount<br /> ├ dev-hugepages.mount<br /> ├ udev.service<br /> │ ├ 240 /sbin/udevd<br /> │ ├ 639 /sbin/udevd<br /> │ └ 640 /sbin/udevd<br /> ├ systemd-logger.service<br /> │ └ 228 //lib/systemd/systemd-logger<br /> └ tmp.mount<br /><br /></pre><pre>root@sacrilege:~# systemctl status ipsec.service<br />ipsec.service - IPSec (openswan)<br />  Loaded: loaded (/etc/systemd/system/ipsec.service)<br />  Active: active (running) since Fri, 05 Nov 2010 15:16:54 +0500; 2h 16min ago<br />  Process: 981 (/usr/sbin/ipsec setup start, code=exited, status=0/SUCCESS)<br />  Process: 974 (/bin/sleep 10, code=exited, status=0/SUCCESS)<br />  CGroup: name=systemd:/systemd-1/ipsec.service<br />   ├ 1059 /bin/sh /usr/lib/ipsec/_plutorun --debug --uniqueids yes --force_busy no --noc...<br />   ├ 1060 logger -s -p daemon.error -t ipsec__plutorun<br />   ├ 1061 /bin/sh /usr/lib/ipsec/_plutorun --debug --uniqueids yes --force_busy no --noc...<br />   ├ 1062 /bin/sh /usr/lib/ipsec/_plutoload --wait no --post<br />   ├ 1064 /usr/libexec/ipsec/pluto --nofork --secretsfile /etc/ipsec.secrets --ipsecdir ...<br />   ├ 1069 pluto helper # 0<br />   ├ 1070 pluto helper # 1<br />   ├ 1071 pluto helper # 2<br />   └ 1223 _pluto_adns<br /></pre><p>It's not just hacking at some opaque *sh hacks (like debian init or even interactive-mode baselayout) and takes so little effort to the point that it's really enjoyable process.</p><p>But making it mount and start all the default (and available) stuff is not the end of it, because there are plenty of services not yet adapted to systemd.<br />I actually expected some (relatively) hard work here, because there are quite a few initscripts in /etc/init.d, even on a desktop machine, but once again, I was in for a nice surprise, since systemd just makes all the work go away. All you need to do is to decide on the ordering (or copy it from baselayout scripts) and put an appropriate "Type=" and "ExecStart=" lines in .service file. That's all there is, really.<br />After that, of course, complete bootup-shutdown test on a vm is in order, and everything "just works" as it is supposed to.<br />Bootup on a real hardware is exactly the same as vm, no surprises here. "udevadm trigger" seem to be necessary as well, proving validity of vm model.</p><p>Systemd boot time is way faster than sysvinit, as it is supposed to, although I don't really care, since reboot is seldom necessary here.</p><p>As a summary, I'd recommend everyone to give systemd a try, or at least get familiar with <a title="systemd rationale" href="http://0pointer.de/blog/projects/systemd.html">it's rationale</a> and <a title="systemd manpages" href="http://0pointer.de/public/systemd-man/">features</a> (plus this three-part blog series: <a title="systemd for admins 1" href="http://0pointer.de/blog/projects/systemd-for-admins-1.html">one</a>, <a title="systemd for admins 2" href="http://0pointer.de/blog/projects/systemd-for-admins-2.html">two</a>, <a title="systemd for admins 3" href="http://0pointer.de/blog/projects/systemd-for-admins-3.html">three</a>).<br /><a title="my units for systemd" href="http://fraggod.net/svc/git/systemd">My units</a> aren't perfect (and I'll probably update network-related stuff to use <a title="ConnMan project" href="http://connman.net/">ConnMan</a>), but if you're lazy, <a title="my units for systemd" href="http://fraggod.net/svc/git/systemd">grab them here</a>. Also, <a title="systemd-arch-units" href="http://github.com/falconindy/systemd-arch-units">here is a repo with units for archlinux</a>, which I loosely used as a reference point along with <em>/lib/systemd</em> contents.</p>-=||||||||||=-2010/11/Moar-free-time-=|||=-2010-11-12T13:33:26-=|||=-Moar free time!-=|||=-[u'RL']-=|||=-<p>As of today, I'm unemployed once again.</p><p>Guess now I'll have time to debug and report a btrfs-systemd crash, read all the feeds, fix some long-standing issues on my home servers, update an antique web setup, write a few watch-notify scripts there, deploy/update a configuration management systems, update/finish/publish a few of my spare-time projects, start playing with a lot of new ideas, check out networking tools like connman, wicd, nm and a bunch of other cool-stuff oss projects, write a few hooks for plotting and graphing stuff in real-time, adapt emacs mail/rss tools, update other elisp stuff, emacs itself, a few symbian-to pc event hooks, check out gnustep environment, ltu lang articles, pybrain and a few other simple machine-learning implementations, some lua-ai for spring, play a lot of games I've missed in past few years, read a few dozen books I've already uploaded but never had a time to, study linear and geometric algebra... maybe find a new job, even, before I starve?</p><p>Nah, nobody in the world have that much time... ;)</p>-=|||=-<p>As of today, I'm unemployed once again.</p><p>Guess now I'll have time to debug and report a btrfs-systemd crash, read all the feeds, fix some long-standing issues on my home servers, update an antique web setup, write a few watch-notify scripts there, deploy/update a configuration management systems, update/finish/publish a few of my spare-time projects, start playing with a lot of new ideas, check out networking tools like connman, wicd, nm and a bunch of other cool-stuff oss projects, write a few hooks for plotting and graphing stuff in real-time, adapt emacs mail/rss tools, update other elisp stuff, emacs itself, a few symbian-to pc event hooks, check out gnustep environment, ltu lang articles, pybrain and a few other simple machine-learning implementations, some lua-ai for spring, play a lot of games I've missed in past few years, read a few dozen books I've already uploaded but never had a time to, study linear and geometric algebra... maybe find a new job, even, before I starve?</p><p>Nah, nobody in the world have that much time... ;)</p>-=||||||||||=-2010/12/MooseFS-usage-experiences-=|||=-2010-12-07T22:22:20-=|||=-MooseFS usage experiences-=|||=-[u'SysAdmin', u'NFS', u'Replication']-=|||=-<p>It's been three months since I've <a title="Distributed-fault-tolerant-fs-take-2-MooseFS" href="/2010/9/Distributed-fault-tolerant-fs-take-2-MooseFS">replaced gluster with moose</a> and I've had a few questions about it's performance so far.<br />Info on the subject in the internets is a bit scarce, so here goes my case. Keep in mind however that it's not a stress-benchmark of any kind and actually rather degenerate use-case, since loads aren't pushing hardware to any limits.</p><p>Guess I can say that it's quite remarkable in a way that it's really unremarkable - I just kinda forgot it's there, which is probably the best thing one can expect from a filesystem.</p><p>My setup is 4 physical nodes at most, with 1.3 TiB of data in fairly large files (3361/52862 dirs/files, calculated average is about 250 MiB for a file).<br />Spanned fs hosts a storage for distfiles, media content, vm images and pretty much anything that comes in the compressed form and worth keeping for further usage. Hence the access to files is highly sequential in most cases (as in reading gzip, listening to mp3, watching a movie, etc).<br />OS on the nodes is gentoo/exherbo/fedora mix and is a subject to constant software updates, breakages and rebuilds. Naturally, mfs is proven to be quite resilent in these conditions, since it doesn't depend on boost or other volatile crap and just consists of several binaries and configuration files, which work fine even with defaults right out of the box.<br />One node is a "master", eating 100 MiB RAM (115 VSZ) on the few-month average (according to atop logs). Others have metalogger slaves which cost virtually nothing (&lt;3 MiB VSZ), so it's not a big deal to keep metadata fully-replicated just in case.<br />Chunkservers have 500 GiB - 3 TiB space on btrfs. These usually hang on 10 MiB RAM, occasional 50-100 MiB in VSZ, though it's not swapped-out, just unused.<br />Cpu usage for each is negligible, even though mfsmaster + mfsmount + mfschunkserver node is Atom D510 on miniITX board.</p><p>mfsmount maintains persistent connection to master and on-demand to chunkservers.<br />It doesn't seem to mind if some of them are down though, so I guess it's perfectly possible to upload files via mfsmount to one (the only accessible) node and they'll be replicated to others from there (more details on that below), although I'm unsure what will happen when you'll try to retrieve chunks, stored exclusively on inaccessible nodes (guess it's easy enough to test, anyway).</p><p>I use only one mfsmount on the same machine as master, and re-export (mostly for reading) it over NFS, SFTP, WebDAV and plain HTTP to other machines.<br />Re-export is there because that way I don't need access to all machines in cluster, which can be in a separate network (i.e. if I access fs from work), plus stuff like NFS comes out of the box (no need for separate client) and have a nice FS-Cache support, which saves a lot of bandwidth, webdav/sftp works for ms-os machines as well and server-based replication saves more precious bandwidth all by itself.</p><p>FS bandwidth in my case in constant ~1 MiB read 24/7 plus any on-demand reading on speeds, which are usually slower than any single hdd (over slower network links like 100 Mbps LAN and WiFi), and using only a few threads as well, so I'm afraid I can't give any real-world stress results here.<br />On a local bulk-copy operations to/from mfs mount though, disk always seem to be a bottleneck, with all other parameters far below any possible limitations, but in my case it's a simple "wd green" low-speed/noise high-capacity disks or seagate/hitachi disks with AAM threshold set to lowest level via "hdparm -M" (works well for sound, but I never really cared about how it affects speed to check).</p><p>Chunkservers' storage consists of idexed (AA/AABCD...) paths, according to chunk names, which can be easily retreived from master. They rely on fs scanning to determine which chunks they have, so I've been able to successfully merge two nodes into one w/o storing the chunks on different filesystems/paths (which is also perfectly possible).</p><p>Chunkservers talk to each other on p2p-basis (doesn't imply that they don't need connection to master, but bandwidth there doesn't seem to be an issue at all) to maintain requested replication goal and auto-balance disk space between themselves, so the free percentage tries to be equal on all nodes (w/o compromising the goal, of course), so with goal=2 and 4 nodes I have 30% space usage on backend-fs on both 500 GiB node and 3 TiB one.</p><p>Balancing seem to be managed by every chunkserver in background (not quite sure if I've seen it in any docs, but there's a "chunk testing" process, which seem to imply that, and can be tuned btw), according to info about chunk and other currently-available nodes' space utilization from master.</p><p>Hence, adding/removing nodes is a bliss - just turn it on/off, no configuration changes for other nodes are necessary - master sees the change (new/lost connection) and all the chunkservers start relocating/getting the chunks to restore the balance and maintain the requested goal. In a few hours everything will be balanced again.</p><p>Whole approach seem superior to dumb round-robin of the chunks on creation or rehashing and relocating every one of them on single node failure, and suggests that it might be easy to implement custom replication and balancing scheme just by rsync'ing chunks between nodes as necessary (i.e. to make most of small ssd buffer, putting most-demanded files' chunks there).<br />And indeed I've utilized that feature twice to merge different nodes and filesystems, although the latter is not really necessary, since chunkserver can work with several storage paths on different filesystems, but it's just seem irrational to keep several btrfs trees these days, as they can even span to multiple devices.</p><p>But the best part, enabling me not to look further for alternatives, is the simple fact that I've yet to see any problem in the stability department - it still just works. mfsmount never refused to give or receive a file, node daemons never crashed or came back up with a weird inconsistency (which I don't think is easy to produce with such simple setup/design, anyway).<br />Connection between nodes has failed quite often - sometimes my NIC/switch/cables went to 30% packet loss for no apparent reason, sometimes I've messed up openswan and ipsec or some other network setup, shut down and hard-rebooted the nodes as necessary, but such failures were always unnoticeable here, without any need to restart anything on the mfs level - chunkservers just reconnect, forget obsolete chunks and keep going about their business.</p><p>Well, there *was* one exception: one time I've managed to hard-reboot a master machine and noticed that mfsmaster has failed to start.<br />Problem was missing metadata.mfs file in /var/lib, which I believe is created on mfsmaster stop and checkpointed every hour to .back file, so, knowing there was no changes to fs in the last few minutes, I just removed the .back suffix and everything started just fine.<br />Doing it The Right Way would've involved stopping any of the metalogger nodes (or signaling it somehow) and retreiving this file from there, or just starting master on that node, updating the mfsmaster ns entry, since they're identical.</p><p>Of course, it's just a commodity hardware and lighter loads, but it's still way above other stuff I've tried here in virtually every aspect, so thumbs up for <a title="MooseFS home" href="http://www.moosefs.org/">moose</a>.</p>-=|||=-<p>It's been three months since I've <a title="Distributed-fault-tolerant-fs-take-2-MooseFS" href="/2010/9/Distributed-fault-tolerant-fs-take-2-MooseFS">replaced gluster with moose</a> and I've had a few questions about it's performance so far.<br />Info on the subject in the internets is a bit scarce, so here goes my case. Keep in mind however that it's not a stress-benchmark of any kind and actually rather degenerate use-case, since loads aren't pushing hardware to any limits.</p><p>Guess I can say that it's quite remarkable in a way that it's really unremarkable - I just kinda forgot it's there, which is probably the best thing one can expect from a filesystem.</p><p>My setup is 4 physical nodes at most, with 1.3 TiB of data in fairly large files (3361/52862 dirs/files, calculated average is about 250 MiB for a file).<br />Spanned fs hosts a storage for distfiles, media content, vm images and pretty much anything that comes in the compressed form and worth keeping for further usage. Hence the access to files is highly sequential in most cases (as in reading gzip, listening to mp3, watching a movie, etc).<br />OS on the nodes is gentoo/exherbo/fedora mix and is a subject to constant software updates, breakages and rebuilds. Naturally, mfs is proven to be quite resilent in these conditions, since it doesn't depend on boost or other volatile crap and just consists of several binaries and configuration files, which work fine even with defaults right out of the box.<br />One node is a "master", eating 100 MiB RAM (115 VSZ) on the few-month average (according to atop logs). Others have metalogger slaves which cost virtually nothing (&lt;3 MiB VSZ), so it's not a big deal to keep metadata fully-replicated just in case.<br />Chunkservers have 500 GiB - 3 TiB space on btrfs. These usually hang on 10 MiB RAM, occasional 50-100 MiB in VSZ, though it's not swapped-out, just unused.<br />Cpu usage for each is negligible, even though mfsmaster + mfsmount + mfschunkserver node is Atom D510 on miniITX board.</p><p>mfsmount maintains persistent connection to master and on-demand to chunkservers.<br />It doesn't seem to mind if some of them are down though, so I guess it's perfectly possible to upload files via mfsmount to one (the only accessible) node and they'll be replicated to others from there (more details on that below), although I'm unsure what will happen when you'll try to retrieve chunks, stored exclusively on inaccessible nodes (guess it's easy enough to test, anyway).</p><p>I use only one mfsmount on the same machine as master, and re-export (mostly for reading) it over NFS, SFTP, WebDAV and plain HTTP to other machines.<br />Re-export is there because that way I don't need access to all machines in cluster, which can be in a separate network (i.e. if I access fs from work), plus stuff like NFS comes out of the box (no need for separate client) and have a nice FS-Cache support, which saves a lot of bandwidth, webdav/sftp works for ms-os machines as well and server-based replication saves more precious bandwidth all by itself.</p><p>FS bandwidth in my case in constant ~1 MiB read 24/7 plus any on-demand reading on speeds, which are usually slower than any single hdd (over slower network links like 100 Mbps LAN and WiFi), and using only a few threads as well, so I'm afraid I can't give any real-world stress results here.<br />On a local bulk-copy operations to/from mfs mount though, disk always seem to be a bottleneck, with all other parameters far below any possible limitations, but in my case it's a simple "wd green" low-speed/noise high-capacity disks or seagate/hitachi disks with AAM threshold set to lowest level via "hdparm -M" (works well for sound, but I never really cared about how it affects speed to check).</p><p>Chunkservers' storage consists of idexed (AA/AABCD...) paths, according to chunk names, which can be easily retreived from master. They rely on fs scanning to determine which chunks they have, so I've been able to successfully merge two nodes into one w/o storing the chunks on different filesystems/paths (which is also perfectly possible).</p><p>Chunkservers talk to each other on p2p-basis (doesn't imply that they don't need connection to master, but bandwidth there doesn't seem to be an issue at all) to maintain requested replication goal and auto-balance disk space between themselves, so the free percentage tries to be equal on all nodes (w/o compromising the goal, of course), so with goal=2 and 4 nodes I have 30% space usage on backend-fs on both 500 GiB node and 3 TiB one.</p><p>Balancing seem to be managed by every chunkserver in background (not quite sure if I've seen it in any docs, but there's a "chunk testing" process, which seem to imply that, and can be tuned btw), according to info about chunk and other currently-available nodes' space utilization from master.</p><p>Hence, adding/removing nodes is a bliss - just turn it on/off, no configuration changes for other nodes are necessary - master sees the change (new/lost connection) and all the chunkservers start relocating/getting the chunks to restore the balance and maintain the requested goal. In a few hours everything will be balanced again.</p><p>Whole approach seem superior to dumb round-robin of the chunks on creation or rehashing and relocating every one of them on single node failure, and suggests that it might be easy to implement custom replication and balancing scheme just by rsync'ing chunks between nodes as necessary (i.e. to make most of small ssd buffer, putting most-demanded files' chunks there).<br />And indeed I've utilized that feature twice to merge different nodes and filesystems, although the latter is not really necessary, since chunkserver can work with several storage paths on different filesystems, but it's just seem irrational to keep several btrfs trees these days, as they can even span to multiple devices.</p><p>But the best part, enabling me not to look further for alternatives, is the simple fact that I've yet to see any problem in the stability department - it still just works. mfsmount never refused to give or receive a file, node daemons never crashed or came back up with a weird inconsistency (which I don't think is easy to produce with such simple setup/design, anyway).<br />Connection between nodes has failed quite often - sometimes my NIC/switch/cables went to 30% packet loss for no apparent reason, sometimes I've messed up openswan and ipsec or some other network setup, shut down and hard-rebooted the nodes as necessary, but such failures were always unnoticeable here, without any need to restart anything on the mfs level - chunkservers just reconnect, forget obsolete chunks and keep going about their business.</p><p>Well, there *was* one exception: one time I've managed to hard-reboot a master machine and noticed that mfsmaster has failed to start.<br />Problem was missing metadata.mfs file in /var/lib, which I believe is created on mfsmaster stop and checkpointed every hour to .back file, so, knowing there was no changes to fs in the last few minutes, I just removed the .back suffix and everything started just fine.<br />Doing it The Right Way would've involved stopping any of the metalogger nodes (or signaling it somehow) and retreiving this file from there, or just starting master on that node, updating the mfsmaster ns entry, since they're identical.</p><p>Of course, it's just a commodity hardware and lighter loads, but it's still way above other stuff I've tried here in virtually every aspect, so thumbs up for <a title="MooseFS home" href="http://www.moosefs.org/">moose</a>.</p>-=||||||||||=-2010/12/Further-improvements-on-notification-daemon-=|||=-2010-12-09T03:58:41-=|||=-Further improvements for notification-daemon-=|||=-[u'Python', u'Desktop', u'Unix', u'Notification', u'Rate-limiting', u'Lisp']-=|||=-<p>It's been a while since I <a title="libnotify / notification-daemon shortcomings and my solution" href="/2010/2/libnotify-notification-daemon-shortcomings-and-my-solution">augmented libnotify / notification-daemon stack</a> to better suit my (maybe not-so-) humble needs, and it certainly was an improvement, but there's no limit to perfection and since then I felt the urge to upgrade it every now and then.</p><p>One early fix was to let messages with priority=critical through without delays and aggregation. I've learned it the hard way when my laptop shut down because of drained battery without me registering any notification about it.<br />Other good candidates for high priority seem to be real-time messages like <a title="Emacs Multimedia System" href="http://www.gnu.org/software/emms/">emms</a> track updates and network connectivity loss events which either too important to ignore or just don't make much sense after delay.<br />Implementation here is straightforward - just check urgency level and pass these unhindered to notification-daemon.</p><p>Another important feature which seem to be missing in reference daemon is the ability to just cleanup the screen of notifications. Sometimes you just need to dismiss them to see the content beneath, or you just read them and don't want them drawing any more attention.<br />The only available interface for that seem to be CloseNotification method, which can only close notification message using it's id, hence only useful from the application that created the note. Kinda makes sense to avoid apps stepping on each others toes, but since id's in question are sequential, it won't be much of a problem to an app to abuse this mechanism anyway.<br />Proxy script, sitting in the middle of dbus communication as it is, don't have to guess these ids, as can just keep track of them.<br />So, to clean up the occasional notification-mess I extended the CloseNotification method to accept 0 as a "special" id, closing all the currently-displayed notifications.<br />Binding it to a key is just a matter of (a bit inelegant, but powerful) dbus-send tool invocation:</p><pre>dbus-send --type=method_call\<br /> --dest=org.freedesktop.Notifications\<br /> /org/freedesktop/Notifications\<br /> org.freedesktop.Notifications.CloseNotification uint32:0</pre><p>Expanding the idea of occasional distraction-free needs, I found the idea of the ability to "plug" the notification system - collecting the notifications into the same digest behind the scenes (yet passing urgent ones, if this behavior is enabled) - when necessary quite appealing, so I just added a flag akin to "fullscreen" check, forcing notification aggregation regardless of rate when it's set.<br />Of course, some means of control over this flag was necessary, so another extension of the interface was to add "Set" method to control notification-proxy options. Method was also useful to occasionally toggle special "urgent" messages treatment, so I empowered it to do so as well by making it accept a key-value array of parameters to apply.<br />And since now there is a plug, I also found handy to have a complimentary "Flush" method to dump last digested notifications.<br />Same handy dbus-send tool comes to rescue again, when these need to be toggled or set via cli:</p><pre>dbus-send --type=method_call\<br /> --dest=org.freedesktop.Notifications <br /> /org/freedesktop/Notifications\<br /> org.freedesktop.Notifications.Set\<br /> dict:string:boolean:plug_toggle,true</pre><p>In contrast to cleanup, I occasionally found myself monitoring low-traffic IRC conversations entirely through notification boxes - no point switching the apps if you can read the whole lines right there, but there was a catch of course - you have to immediately switch attention from whatever you're doing to a notification box to be able to read it before it times out and disappears, which of course is a quite inconvenient.<br />Easy solution is to just override "timeout" value in notification boxes to make them stay as long as you need to, so one more flag for the "Set" method to handle plus one-liner check and there it is.<br />Now it's possible to read them with minimum distraction from the current activity and dismiss via mentioned above extended CloseNotification method.<br />
</p>
<p>As if the above was not enough, sometimes I found myself willing to read and react to the stuff from one set of sources, while temporarily ignoring the traffic from the others, like when you're working at some hack, discussing it (and the current implications / situation) in parallel over jabber or irc, while heated discussion (but interesting none the less) starts in another channel.<br />Shutting down the offending channel in <a title="Emacs IRC Client" href="http://www.emacswiki.org/emacs/ERC">ERC</a>, leaving <a title="Bouncer Software" href="http://en.wikipedia.org/wiki/BNC_%28software%29">BNC</a> to monitor the conversation or just supress notifications with some ERC command would probably be the right way to handle that, yet it's not always that simple, especially since every notification-enabled app then would have to implement some way of doing that, which of course is not the case at all.<br />Remedy is in the customizable filters for notifications, which can be a simple set of regex'es, dumped into some specific dot-file, but even as I started to implement the idea, I could think of several different validation scenarios like "match summary against several regexes", "match message body", "match simple regex with a list of exceptions" or even some counting and more complex logic for them.<br />Idea of inventing yet another perlish (poorly-designed, minimal, ambiguous, write-only) DSL for filtering rules didn't struck me as an exactly bright one, so I thought for looking for some lib implementation of clearly-defined and thought-through syntax for such needs, yet found nothing designed purely for such filtering task (could be one of the reasons why every tool and daemon hard-codes it's own DSL for that *sigh*).</p><p>On that note I thought of some generic yet easily extensible syntax for such rules, and came to realization that simple SICP-like subset of scheme/lisp with regex support would be exactly what I need.<br />Luckily, there are plenty implementations of such embedded languages in python, and since I needed a really simple and customizabe one, I've decided to stick with extended 90-line "<a title="lis.py code" href="http://norvig.com/lis.py">lis.py</a>", described by Peter Norvig <a title="(How to Write a (Lisp) Interpreter (in Python))" href="http://norvig.com/lispy.html">here</a> and <a title="(An ((Even Better) Lisp) Interpreter (in Python))" href="http://norvig.com/lispy2.html">extended here</a>. Out goes unnecessary file-handling, plus regexes and some minor fixes and the result is "make it into whatever you need" language.<br />Just added a stat and mtime check on a dotfile, reading and compiling the matcher-function from it on any change. Contents may look like this:</p><pre>(define-macro define-matcher (lambda<br /> (name comp last rev-args)<br /> `(define ,name (lambda args<br /> (if (= (length args) 1) ,last<br /> (let ((atom (car args)) (args (cdr args)))<br /> (,comp<br /> (~ ,@(if rev-args '((car args) atom) '(atom (car args))))<br /> (apply ,name (cons atom (cdr args))))))))))<br /><br />(define-matcher ~all and #t #f)<br />(define-matcher all~ and #t #t)<br />(define-matcher ~any or #f #f)<br />(define-matcher any~ or #f #t)<br /><br />(lambda (summary body)<br /> (not (and<br /> (~ "^erc: #\S+" summary)<br /> (~ "^\*\*\* #\S+ (was created on|modes:) " body))<br /> (all~ summary "^erc: #pulseaudio$" "^mail:")))<br /></pre><p>Which kinda shows what can you do with it, making your own syntax as you go along (note that stuff like "and" is also a macro, just defined on a higher level).<br />Even with weird macros I find it much more comprehensible than rsync filters, apache/lighttpd rewrite magic or pretty much any pseudo-simple magic set of string-matching rules I had to work with.<br />I considered using python itself to the same end, but found that it's syntax is both more verbose and less flexible/extensible for such goal, plus it allows to do far too much for a simple filtering script which can potentially be evaluated by process with elevated privileges, hence would need some sort of sandboxing anyway.</p><p>In my case all this stuff is bound to convenient key shortcuts via <a title="fluxbox wm" href="http://www.fluxbox.org/">fluxbox wm</a>:</p><pre># Notification-proxy control<br /><br />Print :Exec dbus-send --type=method_call\<br /> --dest=org.freedesktop.Notifications\<br /> /org/freedesktop/Notifications org.freedesktop.Notifications.Set\<br /> dict:string:boolean:plug_toggle,true<br /><br />Shift Print :Exec dbus-send --type=method_call\<br /> --dest=org.freedesktop.Notifications\<br /> /org/freedesktop/Notifications org.freedesktop.Notifications.Set\<br /> dict:string:boolean:cleanup_toggle,true<br /><br />Pause :Exec dbus-send --type=method_call\<br /> --dest=org.freedesktop.Notifications\<br /> /org/freedesktop/Notifications\<br /> org.freedesktop.Notifications.CloseNotification\<br /> uint32:0<br /><br />Shift Pause :Exec dbus-send --type=method_call\<br /> --dest=org.freedesktop.Notifications\<br /> /org/freedesktop/Notifications\<br /> org.freedesktop.Notifications.Flush<br /></pre><p>Pretty sure there's more room for improvement in this aspect, so I'd have to extend the system once again, which is fun all by itself.</p><p>Resulting (and maybe further extended) script is <a title="notification-proxy source" href="http://fraggod.net/oss/projects/notification-proxy.py">here</a>, now linked against <a title="fgc.scheme" href="http://fraggod.net/svc/git/fgc/tree/fgc/scheme.py">a bit revised lis.py scheme implementation</a>.</p>-=|||=-<p>It's been a while since I <a title="libnotify / notification-daemon shortcomings and my solution" href="/2010/2/libnotify-notification-daemon-shortcomings-and-my-solution">augmented libnotify / notification-daemon stack</a> to better suit my (maybe not-so-) humble needs, and it certainly was an improvement, but there's no limit to perfection and since then I felt the urge to upgrade it every now and then.</p><p>One early fix was to let messages with priority=critical through without delays and aggregation. I've learned it the hard way when my laptop shut down because of drained battery without me registering any notification about it.<br />Other good candidates for high priority seem to be real-time messages like <a title="Emacs Multimedia System" href="http://www.gnu.org/software/emms/">emms</a> track updates and network connectivity loss events which either too important to ignore or just don't make much sense after delay.<br />Implementation here is straightforward - just check urgency level and pass these unhindered to notification-daemon.</p><p>Another important feature which seem to be missing in reference daemon is the ability to just cleanup the screen of notifications. Sometimes you just need to dismiss them to see the content beneath, or you just read them and don't want them drawing any more attention.<br />The only available interface for that seem to be CloseNotification method, which can only close notification message using it's id, hence only useful from the application that created the note. Kinda makes sense to avoid apps stepping on each others toes, but since id's in question are sequential, it won't be much of a problem to an app to abuse this mechanism anyway.<br />Proxy script, sitting in the middle of dbus communication as it is, don't have to guess these ids, as can just keep track of them.<br />So, to clean up the occasional notification-mess I extended the CloseNotification method to accept 0 as a "special" id, closing all the currently-displayed notifications.<br />Binding it to a key is just a matter of (a bit inelegant, but powerful) dbus-send tool invocation:</p><pre>dbus-send --type=method_call\<br /> --dest=org.freedesktop.Notifications\<br /> /org/freedesktop/Notifications\<br /> org.freedesktop.Notifications.CloseNotification uint32:0</pre><p>Expanding the idea of occasional distraction-free needs, I found the idea of the ability to "plug" the notification system - collecting the notifications into the same digest behind the scenes (yet passing urgent ones, if this behavior is enabled) - when necessary quite appealing, so I just added a flag akin to "fullscreen" check, forcing notification aggregation regardless of rate when it's set.<br />Of course, some means of control over this flag was necessary, so another extension of the interface was to add "Set" method to control notification-proxy options. Method was also useful to occasionally toggle special "urgent" messages treatment, so I empowered it to do so as well by making it accept a key-value array of parameters to apply.<br />And since now there is a plug, I also found handy to have a complimentary "Flush" method to dump last digested notifications.<br />Same handy dbus-send tool comes to rescue again, when these need to be toggled or set via cli:</p><pre>dbus-send --type=method_call\<br /> --dest=org.freedesktop.Notifications <br /> /org/freedesktop/Notifications\<br /> org.freedesktop.Notifications.Set\<br /> dict:string:boolean:plug_toggle,true</pre><p>In contrast to cleanup, I occasionally found myself monitoring low-traffic IRC conversations entirely through notification boxes - no point switching the apps if you can read the whole lines right there, but there was a catch of course - you have to immediately switch attention from whatever you're doing to a notification box to be able to read it before it times out and disappears, which of course is a quite inconvenient.<br />Easy solution is to just override "timeout" value in notification boxes to make them stay as long as you need to, so one more flag for the "Set" method to handle plus one-liner check and there it is.<br />Now it's possible to read them with minimum distraction from the current activity and dismiss via mentioned above extended CloseNotification method.<br />
</p>
<p>As if the above was not enough, sometimes I found myself willing to read and react to the stuff from one set of sources, while temporarily ignoring the traffic from the others, like when you're working at some hack, discussing it (and the current implications / situation) in parallel over jabber or irc, while heated discussion (but interesting none the less) starts in another channel.<br />Shutting down the offending channel in <a title="Emacs IRC Client" href="http://www.emacswiki.org/emacs/ERC">ERC</a>, leaving <a title="Bouncer Software" href="http://en.wikipedia.org/wiki/BNC_%28software%29">BNC</a> to monitor the conversation or just supress notifications with some ERC command would probably be the right way to handle that, yet it's not always that simple, especially since every notification-enabled app then would have to implement some way of doing that, which of course is not the case at all.<br />Remedy is in the customizable filters for notifications, which can be a simple set of regex'es, dumped into some specific dot-file, but even as I started to implement the idea, I could think of several different validation scenarios like "match summary against several regexes", "match message body", "match simple regex with a list of exceptions" or even some counting and more complex logic for them.<br />Idea of inventing yet another perlish (poorly-designed, minimal, ambiguous, write-only) DSL for filtering rules didn't struck me as an exactly bright one, so I thought for looking for some lib implementation of clearly-defined and thought-through syntax for such needs, yet found nothing designed purely for such filtering task (could be one of the reasons why every tool and daemon hard-codes it's own DSL for that *sigh*).</p><p>On that note I thought of some generic yet easily extensible syntax for such rules, and came to realization that simple SICP-like subset of scheme/lisp with regex support would be exactly what I need.<br />Luckily, there are plenty implementations of such embedded languages in python, and since I needed a really simple and customizabe one, I've decided to stick with extended 90-line "<a title="lis.py code" href="http://norvig.com/lis.py">lis.py</a>", described by Peter Norvig <a title="(How to Write a (Lisp) Interpreter (in Python))" href="http://norvig.com/lispy.html">here</a> and <a title="(An ((Even Better) Lisp) Interpreter (in Python))" href="http://norvig.com/lispy2.html">extended here</a>. Out goes unnecessary file-handling, plus regexes and some minor fixes and the result is "make it into whatever you need" language.<br />Just added a stat and mtime check on a dotfile, reading and compiling the matcher-function from it on any change. Contents may look like this:</p><pre>(define-macro define-matcher (lambda<br /> (name comp last rev-args)<br /> `(define ,name (lambda args<br /> (if (= (length args) 1) ,last<br /> (let ((atom (car args)) (args (cdr args)))<br /> (,comp<br /> (~ ,@(if rev-args '((car args) atom) '(atom (car args))))<br /> (apply ,name (cons atom (cdr args))))))))))<br /><br />(define-matcher ~all and #t #f)<br />(define-matcher all~ and #t #t)<br />(define-matcher ~any or #f #f)<br />(define-matcher any~ or #f #t)<br /><br />(lambda (summary body)<br /> (not (and<br /> (~ "^erc: #\S+" summary)<br /> (~ "^\*\*\* #\S+ (was created on|modes:) " body))<br /> (all~ summary "^erc: #pulseaudio$" "^mail:")))<br /></pre><p>Which kinda shows what can you do with it, making your own syntax as you go along (note that stuff like "and" is also a macro, just defined on a higher level).<br />Even with weird macros I find it much more comprehensible than rsync filters, apache/lighttpd rewrite magic or pretty much any pseudo-simple magic set of string-matching rules I had to work with.<br />I considered using python itself to the same end, but found that it's syntax is both more verbose and less flexible/extensible for such goal, plus it allows to do far too much for a simple filtering script which can potentially be evaluated by process with elevated privileges, hence would need some sort of sandboxing anyway.</p><p>In my case all this stuff is bound to convenient key shortcuts via <a title="fluxbox wm" href="http://www.fluxbox.org/">fluxbox wm</a>:</p><pre># Notification-proxy control<br /><br />Print :Exec dbus-send --type=method_call\<br /> --dest=org.freedesktop.Notifications\<br /> /org/freedesktop/Notifications org.freedesktop.Notifications.Set\<br /> dict:string:boolean:plug_toggle,true<br /><br />Shift Print :Exec dbus-send --type=method_call\<br /> --dest=org.freedesktop.Notifications\<br /> /org/freedesktop/Notifications org.freedesktop.Notifications.Set\<br /> dict:string:boolean:cleanup_toggle,true<br /><br />Pause :Exec dbus-send --type=method_call\<br /> --dest=org.freedesktop.Notifications\<br /> /org/freedesktop/Notifications\<br /> org.freedesktop.Notifications.CloseNotification\<br /> uint32:0<br /><br />Shift Pause :Exec dbus-send --type=method_call\<br /> --dest=org.freedesktop.Notifications\<br /> /org/freedesktop/Notifications\<br /> org.freedesktop.Notifications.Flush<br /></pre><p>Pretty sure there's more room for improvement in this aspect, so I'd have to extend the system once again, which is fun all by itself.</p><p>Resulting (and maybe further extended) script is <a title="notification-proxy source" href="http://fraggod.net/oss/projects/notification-proxy.py">here</a>, now linked against <a title="fgc.scheme" href="http://fraggod.net/svc/git/fgc/tree/fgc/scheme.py">a bit revised lis.py scheme implementation</a>.</p>-=||||||||||=-2010/12/zcat-bzcat-lzcat-xzcat-Arrrgh-Autodetection-rocks-=|||=-2010-12-11T06:04:33-=|||=-zcat, bzcat, lzcat, xzcat... Arrrgh! Autodetection rocks-=|||=-[u'Unix', u'Compression']-=|||=-<p>Playing with <a title="Dracut project" href="http://sourceforge.net/apps/trac/dracut/wiki">dracut</a> today, noticed that it can create lzma-compressed initrd's without problem, but it's "lsinitrd" script uses zcat to access initrd data, thus failing for lzma or bzip2 compression.</p><p>Of course the "problem" is nothing new, and I've bumped against it a zillion times in the past, although it looks like today I was a bit less (or more?) lazy than usual and tried to seek a solution - some *cat tool, which would be able to read any compressed format without the need to specify it explicitly.</p><p>Finding nothing of the /usr/bin persuasion, I noticed that there's a fine <a title="libarchive" href="http://code.google.com/p/libarchive/">libarchive project</a>, which can do all sort of magic just for this purpose, alas there seem to be no cli client for it to utilize this magic, so I got around to write my own one.</p><p>These few minutes of happy-hacking probably saved me a lot of time in the long run, guess the result may as well be useful to someone else:</p><pre name="code" class="c">#include &lt;archive.h&gt;
#include &lt;archive_entry.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;

const int BS = 16384;

int main(int argc, const char **argv) {
	if (argc &gt; 2) {
		fprintf(stderr, "Usage: %s [file]\n", argv[0]);
		exit(1); }

	struct archive *a = archive_read_new();
	archive_read_support_compression_all(a);
	archive_read_support_format_raw(a);

	int err;
	if (argc == 2) err = archive_read_open_filename(a, argv[1], BS);
	else err = archive_read_open_fd(a, 0, BS);
	if (err != ARCHIVE_OK) {
		fprintf(stderr, "Broken archive (1)\n");
		exit(1); }

	struct archive_entry *ae;
	err = archive_read_next_header(a, &ae;);
	if (err != ARCHIVE_OK) {
		fprintf(stderr, "Broken archive (2)\n");
		exit(1); }

	(void) archive_read_data_into_fd(a, 1);

	archive_read_finish(a);
	exit(0);
}
</pre><p>Build it with "gcc -larchive excat.c -o excat" and use as "excat /path/to/something.{xz,gz,bz2,...}".<br />List of formats, supported by libarchive can be found <a title="list of formats, supported by libarchive" href="http://code.google.com/p/libarchive/wiki/LibarchiveFormats">here</a>, note that it can also unpack something like file.gz.xz, although I have no idea why'd someont want to create such thing.</p><p>I've also created a <a title='"excat" project' href="http://excat.sf.net/">project on sourceforge</a> for it, in hopes that it'd save someone like me a bit of time with google-fu, but I doubt I'll add any new features here.</p>-=|||=-<p>Playing with <a title="Dracut project" href="http://sourceforge.net/apps/trac/dracut/wiki">dracut</a> today, noticed that it can create lzma-compressed initrd's without problem, but it's "lsinitrd" script uses zcat to access initrd data, thus failing for lzma or bzip2 compression.</p><p>Of course the "problem" is nothing new, and I've bumped against it a zillion times in the past, although it looks like today I was a bit less (or more?) lazy than usual and tried to seek a solution - some *cat tool, which would be able to read any compressed format without the need to specify it explicitly.</p><p>Finding nothing of the /usr/bin persuasion, I noticed that there's a fine <a title="libarchive" href="http://code.google.com/p/libarchive/">libarchive project</a>, which can do all sort of magic just for this purpose, alas there seem to be no cli client for it to utilize this magic, so I got around to write my own one.</p><p>These few minutes of happy-hacking probably saved me a lot of time in the long run, guess the result may as well be useful to someone else:</p><pre name="code" class="c">#include &lt;archive.h&gt;<br />#include &lt;archive_entry.h&gt;<br />#include &lt;stdio.h&gt;<br />#include &lt;stdlib.h&gt;<br /><br />const int BS = 16384;<br /><br />int main(int argc, const char **argv) {<br />	if (argc &gt; 2) {<br />		fprintf(stderr, "Usage: %s [file]\n", argv[0]);<br />		exit(1); }<br /><br />	struct archive *a = archive_read_new();<br />	archive_read_support_compression_all(a);<br />	archive_read_support_format_raw(a);<br /><br />	int err;<br />	if (argc == 2) err = archive_read_open_filename(a, argv[1], BS);<br />	else err = archive_read_open_fd(a, 0, BS);<br />	if (err != ARCHIVE_OK) {<br />		fprintf(stderr, "Broken archive (1)\n");<br />		exit(1); }<br /><br />	struct archive_entry *ae;<br />	err = archive_read_next_header(a, &ae);<br />	if (err != ARCHIVE_OK) {<br />		fprintf(stderr, "Broken archive (2)\n");<br />		exit(1); }<br /><br />	(void) archive_read_data_into_fd(a, 1);<br /><br />	archive_read_finish(a);<br />	exit(0);<br />}<br /></pre><p>Build it with "gcc -larchive excat.c -o excat" and use as "excat /path/to/something.{xz,gz,bz2,...}".<br />List of formats, supported by libarchive can be found <a title="list of formats, supported by libarchive" href="http://code.google.com/p/libarchive/wiki/LibarchiveFormats">here</a>, note that it can also unpack something like file.gz.xz, although I have no idea why'd someont want to create such thing.</p><p>I've also created a <a title='"excat" project' href="http://excat.sf.net/">project on sourceforge</a> for it, in hopes that it'd save someone like me a bit of time with google-fu, but I doubt I'll add any new features here.</p>-=||||||||||=-2010/12/oslistdir-and-oswalk-in-python-without-lists-by-the-grace-of-c-api-generator-and-recursion-custom-stack-=|||=-2010-12-15T19:11:33-=|||=-os.listdir and os.walk in python without lists (by the grace of c-api generator) and recursion (custom stack)-=|||=-[u'Unix', u'Python']-=|||=-<p>As I got around to update some older crap in the my shared codebase (I mean mostly <a title="fgc toolkit" href="http://fraggod.net/svc/git/fgc/">fgc</a> by that), I've noticed that I use <a title="os.walk docs" href="http://docs.python.org/library/os.html#os.walk">os.walk</a> (although in most cases indirectly) in quite a few places, and it's implementation leaves a few things to be desired:</p><p>First of all, it's recursive, so it has to mirror fs nesing via python call stack, creating a frame objects for every level.<br />I've yet to see (or... I'd rather not see it, ever!) path structure deep enough to cause OOM problems or depleting stack-depth though, but I suppose fs limitations should be well above python's here.</p><p>Second thing is that it uses os.listdir, which, contrary to glibc/posix design of opendir(3), returns a list with all nodes in the given path.<br />Most modern filesystems have fairly loose limits on a number of nodes in a single path, and I actually tested how they handle creation and stat hits/misses for a paths with millions of entries (to check index-paths performance benefits) and the only filesystems with radically degrading performance in such cases were venerable ext2 (on linux, via jbd driver), ufs2 (on freeBSD) and similar, so it's not altogether impossible to stumble upon such path with os.walk and get a 1e6+ element list.</p><p>And another annoyance I've found there is it's weird interface - in nearly all cases I need to get nodes just one-by-one, so I'd be able to work with such pipeline with itertools or any other iterable-oriented stuff, but string and two lists is definitely not what I need in any case.</p><p>One good thing about the current os.walk I can see though, is that it shouldn't hold the dentry in cache any longer than necessary for a single scan, plus then it goes on to probe all the inodes there, which should be quite cache-friendly behavior, not taking into acount further usage of these entries.</p><p>Anyway, to put my mind at ease on the subject, and as a kind of exercise, I thought I'd fix all these issues.</p><p>At the lowest level, that's os.listdir, which I thought I'd replace with a simple generator. Alas, generators in py c-api aren't very simple, but certainly nothing to be afraid of either. Most (and probably the only) helpful info on the subject (with non-native C ppl in mind) was <a title='"How to create a generator/iterator with the Python C API?"' href="http://stackoverflow.com/questions/1815812/how-to-create-a-generator-iterator-with-the-python-c-api/1816961#1816961">this answer on stack overflow</a>, giving the great sample code.</p><p>In my case half of the work was done with opendir(3) in the initialization function, and the rest is just readdir(3) with '.' and '..' filtering and to-unicode conversion with PyObject struct holding the DIR pointer. Code can be found <a title="fgc.os_ext module source" href="http://fraggod.net/svc/git/fgc/tree/os_ext.c">here</a>.<br />Hopefully, it will be another working example of a more complex yet thin c-api usage to augment the python, if not the most elegant or killer-feature-implementing kind.</p><p>Recursion, on the other hand, can be solved entirely in python, all that's needed is to maintain the custom processing stack, mirroring the natural recursion pattern. "Depth" ordering control can be easily implemented by making stack double-edged (as <a title="collections.deque documentation" href="http://docs.python.org/library/collections.html#collections.deque">collections.deque</a>) and the rest is just a simple logic excercise.</p><p>Whole python-side implementation is in fgc.sh module <a title="fgc.sh source" href="http://fraggod.net/svc/git/fgc/tree/fgc/sh.py">here</a>, just look for "def walk" in the source.<br />End-result is efficient iteration and simple clean iterable interface.</p><p>For some use-cases though, just a blind generator is suboptimal, including quite common ones like filtering - you don't need to recurse into some (and usually the most crowded) paths' contents if the filter already blocks the path itself.<br />And thanks to python's <a title="feature description (pep-342)" href="http://docs.python.org/release/2.5.4/whatsnew/pep-342.html">coroutine-like generators</a>, it's not only possible, but trivial to implement - just check yield feedback value for the path, determining the further direction on it's basis (<span><a title="fgc.sh source" href="http://fraggod.net/svc/git/fgc/tree/fgc/sh.py">fgc.sh.crawl</a> function,</span> along with the regex-based filtering).</p><p>Don't get me wrong though, the whole thing doesn't really solve any real problem, thus is little more than a puritan excercise aka brainfart, although of course I'd prefer this implementation over the one in stdlib anyday.<br />Oh, and don't mind the title, I just wanted to give more keywords to the eye-of-google, since generators with python c-api aren't the most elegant and obvious thing, and google don't seem to be very knowledgeable on the subject atm.</p>-=|||=-<p>As I got around to update some older crap in the my shared codebase (I mean mostly <a title="fgc toolkit" href="http://fraggod.net/svc/git/fgc/">fgc</a> by that), I've noticed that I use <a title="os.walk docs" href="http://docs.python.org/library/os.html#os.walk">os.walk</a> (although in most cases indirectly) in quite a few places, and it's implementation leaves a few things to be desired:</p><p>First of all, it's recursive, so it has to mirror fs nesing via python call stack, creating a frame objects for every level.<br />I've yet to see (or... I'd rather not see it, ever!) path structure deep enough to cause OOM problems or depleting stack-depth though, but I suppose fs limitations should be well above python's here.</p><p>Second thing is that it uses os.listdir, which, contrary to glibc/posix design of opendir(3), returns a list with all nodes in the given path.<br />Most modern filesystems have fairly loose limits on a number of nodes in a single path, and I actually tested how they handle creation and stat hits/misses for a paths with millions of entries (to check index-paths performance benefits) and the only filesystems with radically degrading performance in such cases were venerable ext2 (on linux, via jbd driver), ufs2 (on freeBSD) and similar, so it's not altogether impossible to stumble upon such path with os.walk and get a 1e6+ element list.</p><p>And another annoyance I've found there is it's weird interface - in nearly all cases I need to get nodes just one-by-one, so I'd be able to work with such pipeline with itertools or any other iterable-oriented stuff, but string and two lists is definitely not what I need in any case.</p><p>One good thing about the current os.walk I can see though, is that it shouldn't hold the dentry in cache any longer than necessary for a single scan, plus then it goes on to probe all the inodes there, which should be quite cache-friendly behavior, not taking into acount further usage of these entries.</p><p>Anyway, to put my mind at ease on the subject, and as a kind of exercise, I thought I'd fix all these issues.</p><p>At the lowest level, that's os.listdir, which I thought I'd replace with a simple generator. Alas, generators in py c-api aren't very simple, but certainly nothing to be afraid of either. Most (and probably the only) helpful info on the subject (with non-native C ppl in mind) was <a title='"How to create a generator/iterator with the Python C API?"' href="http://stackoverflow.com/questions/1815812/how-to-create-a-generator-iterator-with-the-python-c-api/1816961#1816961">this answer on stack overflow</a>, giving the great sample code.</p><p>In my case half of the work was done with opendir(3) in the initialization function, and the rest is just readdir(3) with '.' and '..' filtering and to-unicode conversion with PyObject struct holding the DIR pointer. Code can be found <a title="fgc.os_ext module source" href="http://fraggod.net/svc/git/fgc/tree/os_ext.c">here</a>.<br />Hopefully, it will be another working example of a more complex yet thin c-api usage to augment the python, if not the most elegant or killer-feature-implementing kind.</p><p>Recursion, on the other hand, can be solved entirely in python, all that's needed is to maintain the custom processing stack, mirroring the natural recursion pattern. "Depth" ordering control can be easily implemented by making stack double-edged (as <a title="collections.deque documentation" href="http://docs.python.org/library/collections.html#collections.deque">collections.deque</a>) and the rest is just a simple logic excercise.</p><p>Whole python-side implementation is in fgc.sh module <a title="fgc.sh source" href="http://fraggod.net/svc/git/fgc/tree/fgc/sh.py">here</a>, just look for "def walk" in the source.<br />End-result is efficient iteration and simple clean iterable interface.</p><p>For some use-cases though, just a blind generator is suboptimal, including quite common ones like filtering - you don't need to recurse into some (and usually the most crowded) paths' contents if the filter already blocks the path itself.<br />And thanks to python's <a title="feature description (pep-342)" href="http://docs.python.org/release/2.5.4/whatsnew/pep-342.html">coroutine-like generators</a>, it's not only possible, but trivial to implement - just check yield feedback value for the path, determining the further direction on it's basis (<span><a title="fgc.sh source" href="http://fraggod.net/svc/git/fgc/tree/fgc/sh.py">fgc.sh.crawl</a> function,</span> along with the regex-based filtering).</p><p>Don't get me wrong though, the whole thing doesn't really solve any real problem, thus is little more than a puritan excercise aka brainfart, although of course I'd prefer this implementation over the one in stdlib anyday.<br />Oh, and don't mind the title, I just wanted to give more keywords to the eye-of-google, since generators with python c-api aren't the most elegant and obvious thing, and google don't seem to be very knowledgeable on the subject atm.</p>-=||||||||||=-2010/12/Commandline-pulseaudio-mixer-tool-=|||=-2010-12-25T21:55:11-=|||=-Commandline pulseaudio mixer tool-=|||=-[u'Desktop', u'Python']-=|||=-<p>Some time ago I decided to check out <a title="pulse" href="http://pulseaudio.org/">pulseaudio project</a>, and after a few docs it was controlling all the sound flow in my system, since then I've never really looked back to pure alsa.<br />At first I just needed the sound-over-network feature, which I've extensively used a few years ago with esd, and pulse offered full transparency here, not just limited support. Hell, it even has simple-client netcat'able stream support, so there's no need to look for a client on alien OS'es.<br />Controllable and centralized resampling was the next nice feat, because some apps (notably, audacious and aqualung) seemed to waste quite a lot of resources on it in the past, either because of unreasonably-high quality or just suboptimal alghorithms, I've never really cared to check. Alsa should be capable to do that as well, but for some reason it failed me in this regard before.</p><p>One major annoyance though was the abscence of a simple tool to control volume levels.<br />pactl seem to be only good for muting the output, while the rest of pa-stuff on the net seem to be based on either gtk or qt, while I needed something to bind to a hotkeys and quickly run inside a readily-available terminal.<br />Maybe it's just an old habit of using alsamixer for this, but replacing it with heavy gnome/kde tools for such a simple task seem unreasonable, so I thought: since it's modern daemon with a lot of well-defined interfaces, why not write my own?</p><p>I considered writing a simple hack around pacmd/pacli, but they aren't much machine-oriented and regex-parsing is not fun, so I found that newer (git or v1.0-dev) pulse versions have a <a title="new dbus interface specs" href="http://pulseaudio.org/wiki/DBusInterface">nice dbus interface</a> to everything.<br />Only problem there is that it doesn't really work, crashing pulse on any attempt to get some list from properties. Had to track down <a title="ticket in pa trac" href="http://pulseaudio.org/ticket/887">the issue</a>, good thing it's fairly trivial to fix (just a simple revert), and then just hacked-up simple non-interactive tool to adjust sink volume by some percentage, specified on command line.</p><p>It was good enough for hotkeys, but I still wanted some nice alsamixer-like bars and thought it might be a good place to implement control per-stream sound levels as well, which is really another nice feature, but only as long as there's a way to actually adjust these levels, which there wasn't.</p><p>A few hours of python and learning curses and there we go:</p><pre> ALSA plug-in [aplay] (fraggod@sacrilege:1424)  [ #######################################---------- ]<br /> MPlayer (fraggod@sacrilege:1298)      [ ################################################# ]<br /> Simple client (TCP/IP client from 192.168.0.5:49162) [ #########---------------------------------------- ]<br /></pre><p>Result was quite responsive and solid, which I kinda didn't expect from any sort of interactive interface.</p><p>Guess I may be not the only person in the world looking for a cli mixer, so I'd probably put the project up somewhere, meanwhile the script is available <a title="tool itself" href="http://fraggod.net/oss/projects/pa_mixer.py">here</a>.<br />The only deps are python-2.7 with curses support and dbus-python, which should come out of the box on any decent desktop system these days, anyway. List of command-line parameters to control sink level is available via traditional "-h" or "--help" option, although interactive stream levels tuning doesn't need any of them.</p>-=|||=-<p>Some time ago I decided to check out <a title="pulse" href="http://pulseaudio.org/">pulseaudio project</a>, and after a few docs it was controlling all the sound flow in my system, since then I've never really looked back to pure alsa.<br />At first I just needed the sound-over-network feature, which I've extensively used a few years ago with esd, and pulse offered full transparency here, not just limited support. Hell, it even has simple-client netcat'able stream support, so there's no need to look for a client on alien OS'es.<br />Controllable and centralized resampling was the next nice feat, because some apps (notably, audacious and aqualung) seemed to waste quite a lot of resources on it in the past, either because of unreasonably-high quality or just suboptimal alghorithms, I've never really cared to check. Alsa should be capable to do that as well, but for some reason it failed me in this regard before.</p><p>One major annoyance though was the abscence of a simple tool to control volume levels.<br />pactl seem to be only good for muting the output, while the rest of pa-stuff on the net seem to be based on either gtk or qt, while I needed something to bind to a hotkeys and quickly run inside a readily-available terminal.<br />Maybe it's just an old habit of using alsamixer for this, but replacing it with heavy gnome/kde tools for such a simple task seem unreasonable, so I thought: since it's modern daemon with a lot of well-defined interfaces, why not write my own?</p><p>I considered writing a simple hack around pacmd/pacli, but they aren't much machine-oriented and regex-parsing is not fun, so I found that newer (git or v1.0-dev) pulse versions have a <a title="new dbus interface specs" href="http://pulseaudio.org/wiki/DBusInterface">nice dbus interface</a> to everything.<br />Only problem there is that it doesn't really work, crashing pulse on any attempt to get some list from properties. Had to track down <a title="ticket in pa trac" href="http://pulseaudio.org/ticket/887">the issue</a>, good thing it's fairly trivial to fix (just a simple revert), and then just hacked-up simple non-interactive tool to adjust sink volume by some percentage, specified on command line.</p><p>It was good enough for hotkeys, but I still wanted some nice alsamixer-like bars and thought it might be a good place to implement control per-stream sound levels as well, which is really another nice feature, but only as long as there's a way to actually adjust these levels, which there wasn't.</p><p>A few hours of python and learning curses and there we go:</p><pre> ALSA plug-in [aplay] (fraggod@sacrilege:1424)  [ #######################################---------- ]<br /> MPlayer (fraggod@sacrilege:1298)      [ ################################################# ]<br /> Simple client (TCP/IP client from 192.168.0.5:49162) [ #########---------------------------------------- ]<br /></pre><p>Result was quite responsive and solid, which I kinda didn't expect from any sort of interactive interface.</p><p>Guess I may be not the only person in the world looking for a cli mixer, so I'd probably put the project up somewhere, meanwhile the script is available <a title="tool itself" href="http://fraggod.net/oss/projects/pa_mixer.py">here</a>.<br />The only deps are python-2.7 with curses support and dbus-python, which should come out of the box on any decent desktop system these days, anyway. List of command-line parameters to control sink level is available via traditional "-h" or "--help" option, although interactive stream levels tuning doesn't need any of them.</p>-=||||||||||=-2010/12/Sane-playback-for-online-streaming-video-and-via-stream-dumping-=|||=-2010-12-29T11:09:01-=|||=-Sane playback for online streaming video via stream dumping-=|||=-[u'Desktop', u'Python', u'Caching', u'Web']-=|||=-<p>I rarely watch footage from various conferences online, usually because I have some work to do and video takes much more dedicated time than the same thing just written on a webpage, making it basically a waste of time, but sometimes it's just fun.<br />Watching familiar "desktop linux complexity" holywar right on the stage of "Desktop on the Linux..." presentation of 27c3 (<a title='"Desktop on the Linux" talk dump' href="http://c3.ex23.de/saal2-2010-12-27_20-04-47.wmv">here's the dump</a>, available atm, better one should probably appear in the <a title="27c3 recordings (empty atm)" href="https://events.ccc.de/congress/2010/wiki/Conference_Recordings">Recordings section</a>) certainly was, and since there are few other interesting topics on schedule (like DJB's talk about high-speed network security) and I have some extra time, I decided not to miss the fun.</p><p>Problem is, "watching stuff online" is even worse than just "watching stuff" - either you pay attention or you just miss it, so I set up recording as a sort of "fs-based cache", at the very least to watch the recorded streams right as they get written, being able to pause or rewind, should I need to do so.</p><p>Natural tool to do the job is mplayer, with it's "-streamdump" flag.<br />It works well up until some network (remote or local) error or just mplayer glitch, which seem to happen quite often.<br />That's when mplayer crashes with funny "Core dumped ;)" line and if you're watching the recorded stream atm, you'll certainly miss it at the time, noticing the fuckup when whatever you're watching ends aburptly and the real-time talk is already finished.<br />Somehow, I managed to forget about the issue and got bit by it soon enough.</p><p>So, mplayer needs to be wrapped in a while loop, but then you also need to give dump files unique names to keep mplayer from overwriting them, and actually do several such loops for several streams you're recording (different halls, different talks, same time), and then probably because of strain on the streaming servers mplayer tend to reconnect several times in a row, producing lots of small dumps, which aren't really good for anything, and you'd also like to get some feedback on what's happening, and... so on.</p><p>Well, I just needed a better tool, and it looks like there aren't much simple non-gui dumpers for video+audio streams and not many libs to connect to http video streams from python, existing one being vlc bindings, which isn't probably any better than mplayer, provided all I need is just to dump a stream to a file, without any reconnection or rotation or multi-stream handling mechanism.<br />To cut the story short I ended up writing a bit more complicated eventloop script to control several mplayer instances, aggregating (and marking each accordingly) their output, restarting failed ones, discarding failed dumps and making up sensible names for the produced files.<br />It was a quick ad-hoc hack, so I thought to implement it straight through signal handling and poll loop for the output, but thinking about all the async calls and state-tracking it'd involve I quickly reconsidered and just used twisted to shove all this mess under the rug, ending up with quick and simple 100-liner.<br /><a title="script itself" href="http://fraggod.net/oss/projects/streamrip.py">Script code</a>, <a title="Twisted framework" href="http://twistedmatrix.com/">twisted</a> is required.</p><p>And now, back to the ongoing talks of day 3.</p>-=|||=-<p>I rarely watch footage from various conferences online, usually because I have some work to do and video takes much more dedicated time than the same thing just written on a webpage, making it basically a waste of time, but sometimes it's just fun.<br />Watching familiar "desktop linux complexity" holywar right on the stage of "Desktop on the Linux..." presentation of 27c3 (<a title='"Desktop on the Linux" talk dump' href="http://c3.ex23.de/saal2-2010-12-27_20-04-47.wmv">here's the dump</a>, available atm, better one should probably appear in the <a title="27c3 recordings (empty atm)" href="https://events.ccc.de/congress/2010/wiki/Conference_Recordings">Recordings section</a>) certainly was, and since there are few other interesting topics on schedule (like DJB's talk about high-speed network security) and I have some extra time, I decided not to miss the fun.</p><p>Problem is, "watching stuff online" is even worse than just "watching stuff" - either you pay attention or you just miss it, so I set up recording as a sort of "fs-based cache", at the very least to watch the recorded streams right as they get written, being able to pause or rewind, should I need to do so.</p><p>Natural tool to do the job is mplayer, with it's "-streamdump" flag.<br />It works well up until some network (remote or local) error or just mplayer glitch, which seem to happen quite often.<br />That's when mplayer crashes with funny "Core dumped ;)" line and if you're watching the recorded stream atm, you'll certainly miss it at the time, noticing the fuckup when whatever you're watching ends aburptly and the real-time talk is already finished.<br />Somehow, I managed to forget about the issue and got bit by it soon enough.</p><p>So, mplayer needs to be wrapped in a while loop, but then you also need to give dump files unique names to keep mplayer from overwriting them, and actually do several such loops for several streams you're recording (different halls, different talks, same time), and then probably because of strain on the streaming servers mplayer tend to reconnect several times in a row, producing lots of small dumps, which aren't really good for anything, and you'd also like to get some feedback on what's happening, and... so on.</p><p>Well, I just needed a better tool, and it looks like there aren't much simple non-gui dumpers for video+audio streams and not many libs to connect to http video streams from python, existing one being vlc bindings, which isn't probably any better than mplayer, provided all I need is just to dump a stream to a file, without any reconnection or rotation or multi-stream handling mechanism.<br />To cut the story short I ended up writing a bit more complicated eventloop script to control several mplayer instances, aggregating (and marking each accordingly) their output, restarting failed ones, discarding failed dumps and making up sensible names for the produced files.<br />It was a quick ad-hoc hack, so I thought to implement it straight through signal handling and poll loop for the output, but thinking about all the async calls and state-tracking it'd involve I quickly reconsidered and just used twisted to shove all this mess under the rug, ending up with quick and simple 100-liner.<br /><a title="script itself" href="http://fraggod.net/oss/projects/streamrip.py">Script code</a>, <a title="Twisted framework" href="http://twistedmatrix.com/">twisted</a> is required.</p><p>And now, back to the ongoing talks of day 3.</p>-=||||||||||=-2011/2/cgroups-initialization-libcgroup-and-my-ad-hoc-replacement-for-it-=|||=-2011-02-26T20:28:13-=|||=-cgroups initialization, libcgroup and my ad-hoc replacement for it-=|||=-[u'Python', u'SysAdmin', u'Rate-limiting', u'Systemd']-=|||=-<p>Linux control groups (cgroups) rock.<br />If you've never used them at all, you bloody well should.</p><p>"git gc --aggressive" of a linux kernel tree killing you disk and cpu? Background compilation makes desktop apps sluggish? Apps step on each others' toes? Disk activity totally kills performance?<br />I've lived with all of the above on the desktop in the (not so distant) past and cgroups just make all this shit go away - even forkbombs and super-multithreaded i/o can just be isolated in their own cgroup (from which there's no way to escape, not with any amount of forks) and scheduled/throttled (cpu hard-throttling - w/o cpusets - <a title="cpu hard-limits patchset" href="http://thread.gmane.org/gmane.linux.kernel/934338">seem to be coming soon as well</a>) as necessary.</p><p>Some problems with process classification and management of these limits seem to exist though.</p><p><a title="my systemd setup" href="http://blog.fraggod.net/2010/11/From-Baselayout-to-Systemd-setup-on-Exherbo">Systemd</a> does a great job of classification of everything outside of user session (i.e. all the daemons) - any rc/cgroup can be specified right in the unit files or set by default via system.conf.<br />And it also makes all this stuff neat and tidy because cgroup support there is not even optional - it's basic mechanism on which systemd is built, used to isolate all the processes belonging to one daemon or the other in place of hopefully-gone-for-good crappy and unreliable pidfiles. No renegade processes, leftovers, pids mixups... etc, ever.</p><p>Bad news however is that all the cool stuff it can do ends right there.<br />Classification is nice, but there's little point in it from resource-limiting perspective without setting the actual limits, and systemd doesn't do that (<a title='"Limiting resources"' href="http://thread.gmane.org/gmane.comp.sysutils.systemd.devel/1322">recent thread on systemd-devel</a>).<br />Besides, no classification for user-started processes means that desktop users are totally on their own, since all resource consumption there branches off the user's fingertips. And desktop is where responsiveness actually matters for me (as in "me the regular user"), so clearly something is needed to create cgroups, set limits there and classify processes to be put into these cgroups.</p><p><a title="libcgroup project site" href="http://libcg.sourceforge.net/">libcgroup project</a> looks like the remedy at first, but since I started using it about a year ago, it's been nothing but the source of pain.<br />First task that stands before it is to actually create cgroups' tree, mount all the resource controller pseudo-filesystems and set the appropriate limits there.<br />libcgroup project has cgconfigparser for that, which is probably the most brain-dead tool one can come up with. Configuration is stone-age pain in the ass, making you clench your teeth, fuck all the DRY principles and write N*100 line crap for even the simpliest tasks with as much punctuation as possible to cram in w/o making eyes water.<br />Then, that cool toy parses the config, giving no indication where you messed it up but the dreaded message like "failed to parse file". Maybe it's not harder to get right by hand than XML, but at least XML-processing tools give some useful feedback.<br />Syntax aside, tool still sucks hard when it comes to apply all the stuff there - it either does every mount/mkdir/write w/o error or just gives you the same "something failed, go figure" indication. Something being already mounted counts as failure as well, so it doesn't play along with anything, including systemd.<br />Worse yet, when it inevitably fails, it starts a "rollback" sequence, unmounting and removing all the shit it was supposed to mount/create. After killing all the configuration you could've had, it will fail anyway. strace will show you why, of course, but if that's the feedback mechanism the developers had in mind...<br />Surely, classification tools there can't be any worse than that? Wrong, they certainly are.<br />Maybe C-API is where the project shines, but I have no reason to believe that, and luckily I don't seem to have any need to find out.</p><p>Luckily, cgroups can be controlled via regular filesystem calls, and thoroughly documented (in <a title="extensive documentation on usage, all the knobs and features" href="http://git.kernel.org/?p=linux/kernel/git/next/linux-next.git;a=tree;f=Documentation/cgroups;hb=HEAD">Documentation/cgroups</a>).</p><p>Anyways, my humble needs (for the purposes of this post) are:</p><ul>
<li>isolate compilation processes, usually performed by "cave" client of paludis package mangler (exherbo) and occasionally shell-invoked make in a kernel tree, from all the other processes;</li>
<li>insulate specific "desktop" processes like firefox and occasional java-based crap from the rest of the system as well;</li>
<li>create all these hierarchies in a freezer and have a convenient stop-switch for these groups.</li>
</ul>
<p>So, how would I initially approach it with libcgroup? Ok, here's the cgconfig.conf:</p><pre><br />### Mounts<br /><br />mount {<br /> cpu = /sys/fs/cgroup/cpu;<br /> blkio = /sys/fs/cgroup/blkio;<br /> freezer = /sys/fs/cgroup/freezer;<br />}<br /><br /><br />### Hierarchical RCs<br /><br />group tagged/cave {<br /> perm {<br /> task {<br /> uid = root;<br /> gid = paludisbuild;<br /> }<br /> admin {<br /> uid = root;<br /> gid = root;<br /> }<br /> }<br /><br /> cpu {<br /> cpu.shares = 100;<br /> }<br /> freezer {<br /> }<br />}<br /><br />group tagged/desktop/roam {<br /> perm {<br /> task {<br /> uid = root;<br /> gid = users;<br /> }<br /> admin {<br /> uid = root;<br /> gid = root;<br /> }<br /> }<br /><br /> cpu {<br /> cpu.shares = 300;<br /> }<br /> freezer {<br /> }<br />}<br /><br />group tagged/desktop/java {<br /> perm {<br /> task {<br /> uid = root;<br /> gid = users;<br /> }<br /> admin {<br /> uid = root;<br /> gid = root;<br /> }<br /> }<br /><br /> cpu {<br /> cpu.shares = 100;<br /> }<br /> freezer {<br /> }<br />}<br /><br /><br />### Non-hierarchical RCs (blkio)<br /><br />group tagged.cave {<br /> perm {<br /> task {<br /> uid = root;<br /> gid = users;<br /> }<br /> admin {<br /> uid = root;<br /> gid = root;<br /> }<br /> }<br /><br /> blkio {<br /> blkio.weight = 100;<br /> }<br />}<br /><br />group tagged.desktop.roam {<br /> perm {<br /> task {<br /> uid = root;<br /> gid = users;<br /> }<br /> admin {<br /> uid = root;<br /> gid = root;<br /> }<br /> }<br /><br /> blkio {<br /> blkio.weight = 300;<br /> }<br />}<br /><br />group tagged.desktop.java {<br /> perm {<br /> task {<br /> uid = root;<br /> gid = users;<br /> }<br /> admin {<br /> uid = root;<br /> gid = root;<br /> }<br /> }<br /><br /> blkio {<br /> blkio.weight = 100;<br /> }<br />}<br /><br /></pre><p>Yep, it's huge, ugly and stupid.<br />Oh, and you have to do some chmods afterwards (more wrapping!) to make the "group = ..." lines actually matter.</p><p>So, what do I want it to look like? This:</p><pre><br />path: /sys/fs/cgroup<br /><br />defaults:<br /> _tasks: root:wheel:664<br /> _admin: root:wheel:644<br /> freezer:<br /><br />groups:<br /><br /> base:<br /> _default: true<br /> cpu.shares: 1000<br /> blkio.weight: 1000<br /><br /> tagged:<br /> cave:<br /> _tasks: root:paludisbuild<br /> _admin: root:paludisbuild<br /> cpu.shares: 100<br /> blkio.weight: 100<br /> desktop:<br /> roam:<br /> _tasks: root:users<br /> cpu.shares: 300<br /> blkio.weight: 300<br /> java:<br /> _tasks: root:users<br /> cpu.shares: 100<br /> blkio.weight: 100<br /><br /></pre><p>It's parseable and readable <a title="YAML data serialization format" href="http://en.wikipedia.org/wiki/Yaml">YAML</a>, not some parenthesis-semicolon nightmare of a C junkie (you may think that because of these spaces don't matter there btw... well, think again!).</p><p>After writing <a title="it was presented just a few paragraphs above" href="http://fraggod.net/static/code/cgroup-tools/cgconf.yaml">that config-I-like-to-see</a> I just spent a few hours to write a <a title="cgconf script" href="http://fraggod.net/static/code/cgroup-tools/cgconf.py">script to apply all the rules there</a> while providing all the debugging facilities I can think of and wiped my system clean of libcgroup, it's that simple.<br />Didn't had to touch the parser again or debug it either (especially with - god forbid - strace), everything just worked as expected, so I thought I'd dump it here jic.</p><p>Configuration file above (<a title="YAML data serialization format" href="http://en.wikipedia.org/wiki/Yaml">YAML</a>) consists of three basic definition blocks:</p><p>"path" to where cgroups should be initialized.<br />Names for the created and mounted rc's are taken right from "groups" and "defaults" sections.<br />Yes, that doesn't allow mounting "blkio" resource controller to "cpu" directory, guess I'll go back to using libcgroup when I'd want to do that... right after seeing the psychiatrist to have my head examined... if they'd let me go back to society afterwards, that is.</p><p>"groups" with actual tree of group parameter definitions.<br />Two special nodes here - "_tasks" and "_admin" - may contain (otherwise the stuff from "defaults" is used) ownership/modes for all cgroup knob-files ("_admin") and "tasks" file ("_tasks"), these can be specified as "user[:group[:mode]]" (with brackets indicating optional definition, of course) with non-specified optional parts taken from the "defaults" section.<br />Limits (or any other settings for any kernel-provided knobs there, for that matter) can either be defined on per-rc-dict basis, like this:</p><pre><br />roam:<br /> _tasks: root:users<br /> cpu:<br /> shares: 300<br /> blkio:<br /> weight: 300<br /> throttle.write_bps_device: 253:9 1000000<br /><br /></pre><p>Or just with one line per rc knob, like this:</p><pre><br />roam:<br /> _tasks: root:users<br /> cpu.shares: 300<br /> blkio.weight: 300<br /> blkio.throttle.write_bps_device: 253:9 1000000<br /><br /></pre><p>Empty dicts (like "freezer" in "defaults") will just create cgroup in a named rc, but won't touch any knobs there.<br />And the "_default" parameter indicates that every pid/tid, listed in a root "tasks" file of resource controllers, specified in this cgroup, should belong to it. That is, act like default cgroup for any tasks, not classified into any other cgroup.</p><p>"defaults" section mirrors the structure of any leaf cgroup.<br />RCs/parameters here will be used for created cgroups, unless overidden in "groups" section.</p><p><a title="cgconf script" href="http://fraggod.net/static/code/cgroup-tools/cgconf.py">Script to process this stuff (cgconf)</a> can be run with --debug to dump a shitload of info about every step it takes (and why it does that), plus with --dry-run flag to just dump all the actions w/o actually doing anything.<br />cgconf can be launched as many times as needed to get the job done - it won't unmount anything (what for? out of fear of data loss on a pseudo-fs?), will just create/mount missing stuff, adjust defined permissions and set defined limits without touching anything else, thus it will work alongside with everything that can also be using these hierarchies - systemd, libcgroup, ulatencyd, whatever... just set what you need to adjust in .yaml and it wll be there after run, no side effects.<br /><a title="sample" href="http://fraggod.net/static/code/cgroup-tools/cgconf.yaml">cgconf.yaml</a> (.yaml, generally speaking) file can be put alongside cgconf or passed via the -c parameter.<br />Anyway, -h or --help is there, in case of any further questions.</p><p>That handles the limits and initial (default cgroup for all tasks) classification part, but then chosen tasks also need to be assigned to a dedicated cgroups.</p><p>libcgroup has pam_cgroup module and cgred daemon, neither of which can sensibly (re)classify anything within a user session, plus cgexec and cgclassify wrappers to basically do "echo $$ >/.../some_cg/tasks && exec $1" or just "echo" respectively.<br />These are dumb simple, nothing done there to make them any easier than echo, so even using libcgroup I had to wrap these.</p><p>Since I knew exactly which (few) apps should be confined to which groups, I just wrote a simple wrapper scripts for each, putting these in a separate dir, in the head of PATH. Example:</p><pre><br />#!/usr/local/bin/cgrc -s desktop/roam<br />/usr/bin/firefox<br /><br /></pre><p><a title="cgrc script" href="http://fraggod.net/static/code/cgroup-tools/cgrc.py">cgrc script</a> here is a dead-simple wrapper to parse cgroup parameter, putting itself into corresponding cgroup within every rc where it exists, making special conversion in case not-yet-hierarchical (there's a patchset for that though: http://lkml.org/lkml/2010/8/30/30) blkio, exec'ing the specified binary with all the passed arguments afterwards.<br />All the parameters after cgroup (or "-g ", for the sake of clarity) go to the specified binary. "-s" option indicates that script is used in shebang, so it'll read command from the file specified in argv after that and pass all the further arguments to it.<br />Otherwise cgrc script can be used as "cgrc -g /usr/bin/firefox " or "cgrc. /usr/bin/firefox ", so it's actually painless and effortless to use this right from the interactive shell. Amen for the crappy libcgroup tools.</p><p>Another special use-case for cgroups I've found useful on many occasions is a "freezer" thing - no matter how many processes compilation (or whatever other cgroup-confined stuff) forks, they can be instantly and painlessly stopped and resumed afterwards.<br /><a title="cgfreeze script" href="http://fraggod.net/static/code/cgroup-tools/cgfreeze.py">cgfreeze dozen-liner script</a> addresses this need in my case - "cgfreeze cave" will stop "cave" cgroup, "cgfreeze -u cave" resume, and "cgfreeze -c cave" will just show it's current status, see -h there for details. No pgrep, kill -STOP or ^Z involved.</p><p>Guess I'll direct the next poor soul struggling with libcgroup here, instead of wasting time explaining how to work around that crap and facing the inevitable question "what else is there?" *sigh*.</p><p>All the mentioned scripts can be found <a title="cgconf, cgconf.yaml, cgrc, cgfreeze" href="http://fraggod.net/static/code/cgroup-tools/">here</a>.</p>-=|||=-<p>Linux control groups (cgroups) rock.<br />If you've never used them at all, you bloody well should.</p><p>"git gc --aggressive" of a linux kernel tree killing you disk and cpu? Background compilation makes desktop apps sluggish? Apps step on each others' toes? Disk activity totally kills performance?<br />I've lived with all of the above on the desktop in the (not so distant) past and cgroups just make all this shit go away - even forkbombs and super-multithreaded i/o can just be isolated in their own cgroup (from which there's no way to escape, not with any amount of forks) and scheduled/throttled (cpu hard-throttling - w/o cpusets - <a title="cpu hard-limits patchset" href="http://thread.gmane.org/gmane.linux.kernel/934338">seem to be coming soon as well</a>) as necessary.</p><p>Some problems with process classification and management of these limits seem to exist though.</p><p><a title="my systemd setup" href="http://blog.fraggod.net/2010/11/From-Baselayout-to-Systemd-setup-on-Exherbo">Systemd</a> does a great job of classification of everything outside of user session (i.e. all the daemons) - any rc/cgroup can be specified right in the unit files or set by default via system.conf.<br />And it also makes all this stuff neat and tidy because cgroup support there is not even optional - it's basic mechanism on which systemd is built, used to isolate all the processes belonging to one daemon or the other in place of hopefully-gone-for-good crappy and unreliable pidfiles. No renegade processes, leftovers, pids mixups... etc, ever.</p><p>Bad news however is that all the cool stuff it can do ends right there.<br />Classification is nice, but there's little point in it from resource-limiting perspective without setting the actual limits, and systemd doesn't do that (<a title='"Limiting resources"' href="http://thread.gmane.org/gmane.comp.sysutils.systemd.devel/1322">recent thread on systemd-devel</a>).<br />Besides, no classification for user-started processes means that desktop users are totally on their own, since all resource consumption there branches off the user's fingertips. And desktop is where responsiveness actually matters for me (as in "me the regular user"), so clearly something is needed to create cgroups, set limits there and classify processes to be put into these cgroups.</p><p><a title="libcgroup project site" href="http://libcg.sourceforge.net/">libcgroup project</a> looks like the remedy at first, but since I started using it about a year ago, it's been nothing but the source of pain.<br />First task that stands before it is to actually create cgroups' tree, mount all the resource controller pseudo-filesystems and set the appropriate limits there.<br />libcgroup project has cgconfigparser for that, which is probably the most brain-dead tool one can come up with. Configuration is stone-age pain in the ass, making you clench your teeth, fuck all the DRY principles and write N*100 line crap for even the simpliest tasks with as much punctuation as possible to cram in w/o making eyes water.<br />Then, that cool toy parses the config, giving no indication where you messed it up but the dreaded message like "failed to parse file". Maybe it's not harder to get right by hand than XML, but at least XML-processing tools give some useful feedback.<br />Syntax aside, tool still sucks hard when it comes to apply all the stuff there - it either does every mount/mkdir/write w/o error or just gives you the same "something failed, go figure" indication. Something being already mounted counts as failure as well, so it doesn't play along with anything, including systemd.<br />Worse yet, when it inevitably fails, it starts a "rollback" sequence, unmounting and removing all the shit it was supposed to mount/create. After killing all the configuration you could've had, it will fail anyway. strace will show you why, of course, but if that's the feedback mechanism the developers had in mind...<br />Surely, classification tools there can't be any worse than that? Wrong, they certainly are.<br />Maybe C-API is where the project shines, but I have no reason to believe that, and luckily I don't seem to have any need to find out.</p><p>Luckily, cgroups can be controlled via regular filesystem calls, and thoroughly documented (in <a title="extensive documentation on usage, all the knobs and features" href="http://git.kernel.org/?p=linux/kernel/git/next/linux-next.git;a=tree;f=Documentation/cgroups;hb=HEAD">Documentation/cgroups</a>).</p><p>Anyways, my humble needs (for the purposes of this post) are:</p><ul>
<li>isolate compilation processes, usually performed by "cave" client of paludis package mangler (exherbo) and occasionally shell-invoked make in a kernel tree, from all the other processes;</li>
<li>insulate specific "desktop" processes like firefox and occasional java-based crap from the rest of the system as well;</li>
<li>create all these hierarchies in a freezer and have a convenient stop-switch for these groups.</li>
</ul>
<p>So, how would I initially approach it with libcgroup? Ok, here's the cgconfig.conf:</p><pre><br />### Mounts<br /><br />mount {<br /> cpu = /sys/fs/cgroup/cpu;<br /> blkio = /sys/fs/cgroup/blkio;<br /> freezer = /sys/fs/cgroup/freezer;<br />}<br /><br /><br />### Hierarchical RCs<br /><br />group tagged/cave {<br /> perm {<br /> task {<br /> uid = root;<br /> gid = paludisbuild;<br /> }<br /> admin {<br /> uid = root;<br /> gid = root;<br /> }<br /> }<br /><br /> cpu {<br /> cpu.shares = 100;<br /> }<br /> freezer {<br /> }<br />}<br /><br />group tagged/desktop/roam {<br /> perm {<br /> task {<br /> uid = root;<br /> gid = users;<br /> }<br /> admin {<br /> uid = root;<br /> gid = root;<br /> }<br /> }<br /><br /> cpu {<br /> cpu.shares = 300;<br /> }<br /> freezer {<br /> }<br />}<br /><br />group tagged/desktop/java {<br /> perm {<br /> task {<br /> uid = root;<br /> gid = users;<br /> }<br /> admin {<br /> uid = root;<br /> gid = root;<br /> }<br /> }<br /><br /> cpu {<br /> cpu.shares = 100;<br /> }<br /> freezer {<br /> }<br />}<br /><br /><br />### Non-hierarchical RCs (blkio)<br /><br />group tagged.cave {<br /> perm {<br /> task {<br /> uid = root;<br /> gid = users;<br /> }<br /> admin {<br /> uid = root;<br /> gid = root;<br /> }<br /> }<br /><br /> blkio {<br /> blkio.weight = 100;<br /> }<br />}<br /><br />group tagged.desktop.roam {<br /> perm {<br /> task {<br /> uid = root;<br /> gid = users;<br /> }<br /> admin {<br /> uid = root;<br /> gid = root;<br /> }<br /> }<br /><br /> blkio {<br /> blkio.weight = 300;<br /> }<br />}<br /><br />group tagged.desktop.java {<br /> perm {<br /> task {<br /> uid = root;<br /> gid = users;<br /> }<br /> admin {<br /> uid = root;<br /> gid = root;<br /> }<br /> }<br /><br /> blkio {<br /> blkio.weight = 100;<br /> }<br />}<br /><br /></pre><p>Yep, it's huge, ugly and stupid.<br />Oh, and you have to do some chmods afterwards (more wrapping!) to make the "group = ..." lines actually matter.</p><p>So, what do I want it to look like? This:</p><pre><br />path: /sys/fs/cgroup<br /><br />defaults:<br /> _tasks: root:wheel:664<br /> _admin: root:wheel:644<br /> freezer:<br /><br />groups:<br /><br /> base:<br /> _default: true<br /> cpu.shares: 1000<br /> blkio.weight: 1000<br /><br /> tagged:<br /> cave:<br /> _tasks: root:paludisbuild<br /> _admin: root:paludisbuild<br /> cpu.shares: 100<br /> blkio.weight: 100<br /> desktop:<br /> roam:<br /> _tasks: root:users<br /> cpu.shares: 300<br /> blkio.weight: 300<br /> java:<br /> _tasks: root:users<br /> cpu.shares: 100<br /> blkio.weight: 100<br /><br /></pre><p>It's parseable and readable <a title="YAML data serialization format" href="http://en.wikipedia.org/wiki/Yaml">YAML</a>, not some parenthesis-semicolon nightmare of a C junkie (you may think that because of these spaces don't matter there btw... well, think again!).</p><p>After writing <a title="it was presented just a few paragraphs above" href="http://fraggod.net/static/code/cgroup-tools/cgconf.yaml">that config-I-like-to-see</a> I just spent a few hours to write a <a title="cgconf script" href="http://fraggod.net/static/code/cgroup-tools/cgconf.py">script to apply all the rules there</a> while providing all the debugging facilities I can think of and wiped my system clean of libcgroup, it's that simple.<br />Didn't had to touch the parser again or debug it either (especially with - god forbid - strace), everything just worked as expected, so I thought I'd dump it here jic.</p><p>Configuration file above (<a title="YAML data serialization format" href="http://en.wikipedia.org/wiki/Yaml">YAML</a>) consists of three basic definition blocks:</p><p>"path" to where cgroups should be initialized.<br />Names for the created and mounted rc's are taken right from "groups" and "defaults" sections.<br />Yes, that doesn't allow mounting "blkio" resource controller to "cpu" directory, guess I'll go back to using libcgroup when I'd want to do that... right after seeing the psychiatrist to have my head examined... if they'd let me go back to society afterwards, that is.</p><p>"groups" with actual tree of group parameter definitions.<br />Two special nodes here - "_tasks" and "_admin" - may contain (otherwise the stuff from "defaults" is used) ownership/modes for all cgroup knob-files ("_admin") and "tasks" file ("_tasks"), these can be specified as "user[:group[:mode]]" (with brackets indicating optional definition, of course) with non-specified optional parts taken from the "defaults" section.<br />Limits (or any other settings for any kernel-provided knobs there, for that matter) can either be defined on per-rc-dict basis, like this:</p><pre><br />roam:<br /> _tasks: root:users<br /> cpu:<br /> shares: 300<br /> blkio:<br /> weight: 300<br /> throttle.write_bps_device: 253:9 1000000<br /><br /></pre><p>Or just with one line per rc knob, like this:</p><pre><br />roam:<br /> _tasks: root:users<br /> cpu.shares: 300<br /> blkio.weight: 300<br /> blkio.throttle.write_bps_device: 253:9 1000000<br /><br /></pre><p>Empty dicts (like "freezer" in "defaults") will just create cgroup in a named rc, but won't touch any knobs there.<br />And the "_default" parameter indicates that every pid/tid, listed in a root "tasks" file of resource controllers, specified in this cgroup, should belong to it. That is, act like default cgroup for any tasks, not classified into any other cgroup.</p><p>"defaults" section mirrors the structure of any leaf cgroup.<br />RCs/parameters here will be used for created cgroups, unless overidden in "groups" section.</p><p><a title="cgconf script" href="http://fraggod.net/static/code/cgroup-tools/cgconf.py">Script to process this stuff (cgconf)</a> can be run with --debug to dump a shitload of info about every step it takes (and why it does that), plus with --dry-run flag to just dump all the actions w/o actually doing anything.<br />cgconf can be launched as many times as needed to get the job done - it won't unmount anything (what for? out of fear of data loss on a pseudo-fs?), will just create/mount missing stuff, adjust defined permissions and set defined limits without touching anything else, thus it will work alongside with everything that can also be using these hierarchies - systemd, libcgroup, ulatencyd, whatever... just set what you need to adjust in .yaml and it wll be there after run, no side effects.<br /><a title="sample" href="http://fraggod.net/static/code/cgroup-tools/cgconf.yaml">cgconf.yaml</a> (.yaml, generally speaking) file can be put alongside cgconf or passed via the -c parameter.<br />Anyway, -h or --help is there, in case of any further questions.</p><p>That handles the limits and initial (default cgroup for all tasks) classification part, but then chosen tasks also need to be assigned to a dedicated cgroups.</p><p>libcgroup has pam_cgroup module and cgred daemon, neither of which can sensibly (re)classify anything within a user session, plus cgexec and cgclassify wrappers to basically do "echo $$ >/.../some_cg/tasks && exec $1" or just "echo" respectively.<br />These are dumb simple, nothing done there to make them any easier than echo, so even using libcgroup I had to wrap these.</p><p>Since I knew exactly which (few) apps should be confined to which groups, I just wrote a simple wrapper scripts for each, putting these in a separate dir, in the head of PATH. Example:</p><pre><br />#!/usr/local/bin/cgrc -s desktop/roam<br />/usr/bin/firefox<br /><br /></pre><p><a title="cgrc script" href="http://fraggod.net/static/code/cgroup-tools/cgrc.py">cgrc script</a> here is a dead-simple wrapper to parse cgroup parameter, putting itself into corresponding cgroup within every rc where it exists, making special conversion in case not-yet-hierarchical (there's a patchset for that though: http://lkml.org/lkml/2010/8/30/30) blkio, exec'ing the specified binary with all the passed arguments afterwards.<br />All the parameters after cgroup (or "-g ", for the sake of clarity) go to the specified binary. "-s" option indicates that script is used in shebang, so it'll read command from the file specified in argv after that and pass all the further arguments to it.<br />Otherwise cgrc script can be used as "cgrc -g /usr/bin/firefox " or "cgrc. /usr/bin/firefox ", so it's actually painless and effortless to use this right from the interactive shell. Amen for the crappy libcgroup tools.</p><p>Another special use-case for cgroups I've found useful on many occasions is a "freezer" thing - no matter how many processes compilation (or whatever other cgroup-confined stuff) forks, they can be instantly and painlessly stopped and resumed afterwards.<br /><a title="cgfreeze script" href="http://fraggod.net/static/code/cgroup-tools/cgfreeze.py">cgfreeze dozen-liner script</a> addresses this need in my case - "cgfreeze cave" will stop "cave" cgroup, "cgfreeze -u cave" resume, and "cgfreeze -c cave" will just show it's current status, see -h there for details. No pgrep, kill -STOP or ^Z involved.</p><p>Guess I'll direct the next poor soul struggling with libcgroup here, instead of wasting time explaining how to work around that crap and facing the inevitable question "what else is there?" *sigh*.</p><p>All the mentioned scripts can be found <a title="cgconf, cgconf.yaml, cgrc, cgfreeze" href="http://fraggod.net/static/code/cgroup-tools/">here</a>.</p>-=||||||||||=-2011/2/Dashboard-for-enabled-services-in-systemd-=|||=-2011-02-27T07:02:55-=|||=-Dashboard for enabled services in systemd-=|||=-[u'Python', u'SysAdmin', u'Unix', u'Systemd']-=|||=-<p>Systemd does a good job at monitoring and restarting services. It also keeps track of "failed" services, which you can easily see in systemctl output.</p><p>Problem for me is that services that should be running at the machine don't always "fail".<br />I can stop them and forget to start again, .service file can be broken (like, reload may actually terminate the service), they can be accidentally or deliberately killed or just exit with 0 code due to some internal event, just because they think that's okay to stop now.<br />Most often such "accidents" seem to happen on boot - some services just perform sanity checks, see that some required path or socket is missing and exit, sometimes with code 0.<br />As a good sysadmin, you take a peek at systemctl, see no failures there and think "ok, successful reboot, everything is started", and well, it's not, and systemd doesn't reveal that fact.</p><p>What's needed here is kinda "dashboard" of what is enabled and thus should be running with clear indication if something is not. Best implementation of such thing I've seen in openrc init system, which comes with baselayout-2 on Gentoo Linux ("unstable" or "~" branch atm, but guess it'll be stabilized one day):</p><pre><br />root@damnation:~# rc-status -a<br />Runlevel: shutdown<br /> killprocs  [ stopped ]<br /> savecache  [ stopped ]<br /> mount-ro  [ stopped ]<br />Runlevel: single<br />Runlevel: nonetwork<br /> local   [ started ]<br />Runlevel: cryptinit<br /> rsyslog   [ started ]<br /> ip6tables  [ started ]<br />...<br /> twistd   [ started ]<br /> local   [ started ]<br />Runlevel: sysinit<br /> dmesg   [ started ]<br /> udev   [ started ]<br /> devfs   [ started ]<br />Runlevel: boot<br /> hwclock   [ started ]<br /> lvm   [ started ]<br />...<br /> wdd   [ started ]<br /> keymaps   [ started ]<br />Runlevel: default<br /> rsyslog   [ started ]<br /> ip6tables  [ started ]<br />...<br /> twistd   [ started ]<br /> local   [ started ]<br />Dynamic Runlevel: hotplugged<br />Dynamic Runlevel: needed<br /> sysfs   [ started ]<br /> rpc.pipefs  [ started ]<br />...<br /> rpcbind   [ started ]<br /> rpc.idmapd  [ started ]<br />Dynamic Runlevel: manual<br /><br /></pre><p>Just "grep -v started" and you see everything that's "stopped", "crashed", etc.<br />I <a title="thread on systemd-devel ml" href="http://thread.gmane.org/gmane.comp.sysutils.systemd.devel/1334">tried to raise issue on systemd-devel</a>, but looks like I'm the only one who cares about it, so I went ahead to write my own tool for the job.</p><p>Implementation uses extensive dbus interface provided by systemd to get a set of all the .service units loaded by systemd, then gets "enabled" stuff from symlinks on a filesystem. Latter are easily located in places /{etc,lib}/systemd/system/*/*.service and systemd doesn't seem to keep track of these, using them only at boot-time.<br />Having some experience using rc-status tool from openrc I also fixed the main annoyance it has - there's no point to show "started" services, ever! I always cared about "not enabled" or "not started" only, and shitload of "started" crap it dumps is just annoying, and has to always be grepped-out.</p><p>So, meet the systemd-dashboard tool:</p><pre><br />root@damnation:~# systemd-dashboard -h<br />usage: systemd-dashboard [-h] [-s] [-u] [-n]<br /><br />Tool to compare the set of enabled systemd services against currently running<br />ones. If started without parameters, it'll just show all the enabled services<br />that should be running (Type != oneshot) yet for some reason they aren't.<br /><br />optional arguments:<br /> -h, --help  show this help message and exit<br /> -s, --status Show status report on found services.<br /> -u, --unknown Show enabled but unknown (not loaded) services.<br /> -n, --not-enabled Show list of services that are running but are not<br />   enabled directly.<br /><br /></pre><p>Simple invocation will show what's not running while it should be:</p><pre><br />root@damnation:~# systemd-dashboard<br />smartd.service<br />systemd-readahead-replay.service<br />apache.service<br /><br /></pre><p>Adding "-s" flag will show what happened there in more detail (by the grace of "systemctl status" command):</p><pre><br />root@damnation:~# systemd-dashboard -s<br /><br />smartd.service - smartd<br />  Loaded: loaded (/lib64/systemd/system/smartd.service)<br />  Active: failed since Sun, 27 Feb 2011 11:44:05 +0500; 2s ago<br />  Process: 16322 ExecStart=/usr/sbin/smartd --no-fork --capabilities (code=killed, signal=KILL)<br />  CGroup: name=systemd:/system/smartd.service<br /><br />systemd-readahead-replay.service - Replay Read-Ahead Data<br />  Loaded: loaded (/lib64/systemd/system/systemd-readahead-replay.service)<br />  Active: inactive (dead)<br />  CGroup: name=systemd:/system/systemd-readahead-replay.service<br /><br />apache.service - apache2<br />  Loaded: loaded (/lib64/systemd/system/apache.service)<br />  Active: inactive (dead) since Sun, 27 Feb 2011 11:42:34 +0500; 51s ago<br />  Process: 16281 ExecStop=/usr/bin/apachectl -k stop (code=exited, status=0/SUCCESS)<br /> Main PID: 5664 (code=exited, status=0/SUCCESS)<br />  CGroup: name=systemd:/system/apache.service<br /><br /></pre><p>Would you've noticed that readahead fails on a remote machine because the kernel is missing fanotify and the service apparently thinks "it's okay not to start" in this case? What about smartd you've killed a while ago and forgot to restart?</p><p>And you can check if you forgot to enable something with "-n" flag, which will show all the running stuff that was not explicitly enabled.</p><p>Code is under a hundred lines of python with the only dep of <a title="dbus-python project" href="http://dbus.freedesktop.org/releases/dbus-python/">dbus-python package</a>. You can grab the initial (probably not updated much, although it's probably finished as it is) version <a title="systemd-dashboard.py" href="http://fraggod.net/static/code/systemd-dashboard.py">from here</a> or a maintained version from <a title="fgtk project repo" href="http://fraggod.net/code/fossil/fgtk/login?g=/code/fossil/fgtk/dir">fgtk repo</a> (yes, there's an anonymous login form to pass).</p><p>If someone will also find the thing useful, I'd appreciate if you'll raise awareness to the issue within systemd project - I'd rather like to see such functionality in the main package, not hacked-up on ad-hoc basis around it.</p><p><strong>Update (+20d):</strong> issue was noticed and will probably be addressed in systemd. Yay!</p>-=|||=-<p>Systemd does a good job at monitoring and restarting services. It also keeps track of "failed" services, which you can easily see in systemctl output.</p><p>Problem for me is that services that should be running at the machine don't always "fail".<br />I can stop them and forget to start again, .service file can be broken (like, reload may actually terminate the service), they can be accidentally or deliberately killed or just exit with 0 code due to some internal event, just because they think that's okay to stop now.<br />Most often such "accidents" seem to happen on boot - some services just perform sanity checks, see that some required path or socket is missing and exit, sometimes with code 0.<br />As a good sysadmin, you take a peek at systemctl, see no failures there and think "ok, successful reboot, everything is started", and well, it's not, and systemd doesn't reveal that fact.</p><p>What's needed here is kinda "dashboard" of what is enabled and thus should be running with clear indication if something is not. Best implementation of such thing I've seen in openrc init system, which comes with baselayout-2 on Gentoo Linux ("unstable" or "~" branch atm, but guess it'll be stabilized one day):</p><pre><br />root@damnation:~# rc-status -a<br />Runlevel: shutdown<br /> killprocs  [ stopped ]<br /> savecache  [ stopped ]<br /> mount-ro  [ stopped ]<br />Runlevel: single<br />Runlevel: nonetwork<br /> local   [ started ]<br />Runlevel: cryptinit<br /> rsyslog   [ started ]<br /> ip6tables  [ started ]<br />...<br /> twistd   [ started ]<br /> local   [ started ]<br />Runlevel: sysinit<br /> dmesg   [ started ]<br /> udev   [ started ]<br /> devfs   [ started ]<br />Runlevel: boot<br /> hwclock   [ started ]<br /> lvm   [ started ]<br />...<br /> wdd   [ started ]<br /> keymaps   [ started ]<br />Runlevel: default<br /> rsyslog   [ started ]<br /> ip6tables  [ started ]<br />...<br /> twistd   [ started ]<br /> local   [ started ]<br />Dynamic Runlevel: hotplugged<br />Dynamic Runlevel: needed<br /> sysfs   [ started ]<br /> rpc.pipefs  [ started ]<br />...<br /> rpcbind   [ started ]<br /> rpc.idmapd  [ started ]<br />Dynamic Runlevel: manual<br /><br /></pre><p>Just "grep -v started" and you see everything that's "stopped", "crashed", etc.<br />I <a title="thread on systemd-devel ml" href="http://thread.gmane.org/gmane.comp.sysutils.systemd.devel/1334">tried to raise issue on systemd-devel</a>, but looks like I'm the only one who cares about it, so I went ahead to write my own tool for the job.</p><p>Implementation uses extensive dbus interface provided by systemd to get a set of all the .service units loaded by systemd, then gets "enabled" stuff from symlinks on a filesystem. Latter are easily located in places /{etc,lib}/systemd/system/*/*.service and systemd doesn't seem to keep track of these, using them only at boot-time.<br />Having some experience using rc-status tool from openrc I also fixed the main annoyance it has - there's no point to show "started" services, ever! I always cared about "not enabled" or "not started" only, and shitload of "started" crap it dumps is just annoying, and has to always be grepped-out.</p><p>So, meet the systemd-dashboard tool:</p><pre><br />root@damnation:~# systemd-dashboard -h<br />usage: systemd-dashboard [-h] [-s] [-u] [-n]<br /><br />Tool to compare the set of enabled systemd services against currently running<br />ones. If started without parameters, it'll just show all the enabled services<br />that should be running (Type != oneshot) yet for some reason they aren't.<br /><br />optional arguments:<br /> -h, --help  show this help message and exit<br /> -s, --status Show status report on found services.<br /> -u, --unknown Show enabled but unknown (not loaded) services.<br /> -n, --not-enabled Show list of services that are running but are not<br />   enabled directly.<br /><br /></pre><p>Simple invocation will show what's not running while it should be:</p><pre><br />root@damnation:~# systemd-dashboard<br />smartd.service<br />systemd-readahead-replay.service<br />apache.service<br /><br /></pre><p>Adding "-s" flag will show what happened there in more detail (by the grace of "systemctl status" command):</p><pre><br />root@damnation:~# systemd-dashboard -s<br /><br />smartd.service - smartd<br />  Loaded: loaded (/lib64/systemd/system/smartd.service)<br />  Active: failed since Sun, 27 Feb 2011 11:44:05 +0500; 2s ago<br />  Process: 16322 ExecStart=/usr/sbin/smartd --no-fork --capabilities (code=killed, signal=KILL)<br />  CGroup: name=systemd:/system/smartd.service<br /><br />systemd-readahead-replay.service - Replay Read-Ahead Data<br />  Loaded: loaded (/lib64/systemd/system/systemd-readahead-replay.service)<br />  Active: inactive (dead)<br />  CGroup: name=systemd:/system/systemd-readahead-replay.service<br /><br />apache.service - apache2<br />  Loaded: loaded (/lib64/systemd/system/apache.service)<br />  Active: inactive (dead) since Sun, 27 Feb 2011 11:42:34 +0500; 51s ago<br />  Process: 16281 ExecStop=/usr/bin/apachectl -k stop (code=exited, status=0/SUCCESS)<br /> Main PID: 5664 (code=exited, status=0/SUCCESS)<br />  CGroup: name=systemd:/system/apache.service<br /><br /></pre><p>Would you've noticed that readahead fails on a remote machine because the kernel is missing fanotify and the service apparently thinks "it's okay not to start" in this case? What about smartd you've killed a while ago and forgot to restart?</p><p>And you can check if you forgot to enable something with "-n" flag, which will show all the running stuff that was not explicitly enabled.</p><p>Code is under a hundred lines of python with the only dep of <a title="dbus-python project" href="http://dbus.freedesktop.org/releases/dbus-python/">dbus-python package</a>. You can grab the initial (probably not updated much, although it's probably finished as it is) version <a title="systemd-dashboard.py" href="http://fraggod.net/static/code/systemd-dashboard.py">from here</a> or a maintained version from <a title="fgtk project repo" href="http://fraggod.net/code/fossil/fgtk/login?g=/code/fossil/fgtk/dir">fgtk repo</a> (yes, there's an anonymous login form to pass).</p><p>If someone will also find the thing useful, I'd appreciate if you'll raise awareness to the issue within systemd project - I'd rather like to see such functionality in the main package, not hacked-up on ad-hoc basis around it.</p><p><strong>Update (+20d):</strong> issue was noticed and will probably be addressed in systemd. Yay!</p>-=||||||||||=-2011/3/Auto-updating-desktop-background-with-scaling-via-LQR-and-some-other-tricks-=|||=-2011-03-05T12:44:27-=|||=-Auto-updating desktop background with scaling via LQR and some other tricks-=|||=-[u'Python', u'Bash', u'Desktop']-=|||=-<p>Got around to publish my (pretty-sophisticated at this point) background updater script.</p><p>Check it out, if you keep a local image collection, as a sample of how to batch-process images in most perversive ways using gimp and python or if you've never heard of "liquid rescale" algorithm:</p><ul>
<li><a title="sf.net project" href="http://desktop-aura.sourceforge.net/">Sourceforge project for the whole thing</a></li>
<li><a title="GIMP plugin registry" href="http://registry.gimp.org/node/25103">GIMP plugin in the registry</a></li>
</ul>-=|||=-<p>Got around to publish my (pretty-sophisticated at this point) background updater script.</p><p>Check it out, if you keep a local image collection, as a sample of how to batch-process images in most perversive ways using gimp and python or if you've never heard of "liquid rescale" algorithm:</p><ul>
<li><a title="sf.net project" href="http://desktop-aura.sourceforge.net/">Sourceforge project for the whole thing</a></li>
<li><a title="GIMP plugin registry" href="http://registry.gimp.org/node/25103">GIMP plugin in the registry</a></li>
</ul>-=||||||||||=-2011/3/Parallel-port-LED-notification-for-extra-high-system-load-=|||=-2011-03-14T11:26:32-=|||=-Parallel port LED notification for extra-high system load-=|||=-[u'Python', u'Notification', u'SysAdmin']-=|||=-<p>I've heard about how easy it is to control stuff with a parallel port, but recently I've been asked to write a simple script to send repeated signals to some hardware via lpt and I really needed some way to test whether signals are coming through or not.<br />Googling around a bit, I've found that it's trivial to plug leds right into the port and did just that to test the script.</p><p>Since it's trivial to control these leds and they provide quite a simple way for a visual notification for an otherwise headless servers, I've put together another script to monitor system resources usage and indicate extra-high load with various blinking rates.</p><p>Probably the coolest thing is that parallel port on mini-ITX motherboards comes in a standard "male" pin group like usb or audio with "ground" pins exactly opposite of "data" pins, so it's easy to take a few leds (power, ide, and they usually come in different colors!) from an old computer case and plug these directly into the board.<img alt="LED indicators from a mini-ITX board" title="LED indicators from a mini-ITX board" src="/static/embed/led_notification.jpg" align="right" /></p><p>Parallel port interaction is implemented in fairly simple <a title="pyparallel module" href="http://pyserial.sourceforge.net/pyparallel.html">pyparallel module</a>.<br />Making leds blink actually involves an active switching of data bits on the port in an infinite loop, so I've forked one subprocess to do that while another one checks/processes the data and feeds led blinking intervals' updates to the first one via pipe.<br />System load data is easy to acquire from "/proc/loadavg" for cpu and as "%util" percentage from "sar -d" reports.<br />And the easiest way to glue several subprocesses and a timer together into an eventloop is <a title="twisted python" href="http://twistedmatrix.com/">twisted</a>, so the script is basically 150 lines sar output processing, checks and blinking rate settings.</p><p>Obligatory <a title="script source" href="http://fraggod.net/static/code/ledd.py">link to the source</a>. Deps are <a title="python" href="http://python.org/">python-2.7</a>, <a title="twisted module" href="http://twistedmatrix.com/">twisted</a> and <a title="pyparallel module" href="http://pyserial.sourceforge.net/pyparallel.html">pyparallel</a>.</p><p>Guess mail notifications could've been just as useful, but quickly-blinking leds are more spectacular and kinda way to utilize legacy hardware capabilities that these motherboards still have.</p>-=|||=-<p>I've heard about how easy it is to control stuff with a parallel port, but recently I've been asked to write a simple script to send repeated signals to some hardware via lpt and I really needed some way to test whether signals are coming through or not.<br />Googling around a bit, I've found that it's trivial to plug leds right into the port and did just that to test the script.</p><p>Since it's trivial to control these leds and they provide quite a simple way for a visual notification for an otherwise headless servers, I've put together another script to monitor system resources usage and indicate extra-high load with various blinking rates.</p><p>Probably the coolest thing is that parallel port on mini-ITX motherboards comes in a standard "male" pin group like usb or audio with "ground" pins exactly opposite of "data" pins, so it's easy to take a few leds (power, ide, and they usually come in different colors!) from an old computer case and plug these directly into the board.<img alt="LED indicators from a mini-ITX board" title="LED indicators from a mini-ITX board" src="/static/embed/led_notification.jpg" align="right" /></p><p>Parallel port interaction is implemented in fairly simple <a title="pyparallel module" href="http://pyserial.sourceforge.net/pyparallel.html">pyparallel module</a>.<br />Making leds blink actually involves an active switching of data bits on the port in an infinite loop, so I've forked one subprocess to do that while another one checks/processes the data and feeds led blinking intervals' updates to the first one via pipe.<br />System load data is easy to acquire from "/proc/loadavg" for cpu and as "%util" percentage from "sar -d" reports.<br />And the easiest way to glue several subprocesses and a timer together into an eventloop is <a title="twisted python" href="http://twistedmatrix.com/">twisted</a>, so the script is basically 150 lines sar output processing, checks and blinking rate settings.</p><p>Obligatory <a title="script source" href="http://fraggod.net/static/code/ledd.py">link to the source</a>. Deps are <a title="python" href="http://python.org/">python-2.7</a>, <a title="twisted module" href="http://twistedmatrix.com/">twisted</a> and <a title="pyparallel module" href="http://pyserial.sourceforge.net/pyparallel.html">pyparallel</a>.</p><p>Guess mail notifications could've been just as useful, but quickly-blinking leds are more spectacular and kinda way to utilize legacy hardware capabilities that these motherboards still have.</p>-=||||||||||=-2011/3/Selective-IPv6-AAAA-DNS-resolution-=|||=-2011-03-19T09:54:28-=|||=-Selective IPv6 (AAAA) DNS resolution-=|||=-[u'Python', u'SysAdmin', u'IPv6']-=|||=-<p>Had IPv6 tunnel from <a title="Hurricane Electric tunnelbroker" href="http://tunnelbroker.net/">HE</a> for a few years now, but since I've changed ISP about a year ago, I've been unable to use it because ISP dropped sit tunnel packets for some weird reason.<br />A quick check yesterday revealed that this limitation seem to have been lifted, so I've re-enabled the tunnel at once.</p><p>All the IPv6-enabled stuff started using AAAA-provided IPs at once, and that resulted in some problems.<br />Particulary annoying thing is that <a title="ZNC" href="http://en.znc.in/">ZNC IRC bouncer</a> managed to loose connection to <a title="FreeNode IRC network" href="http://freenode.net/">freenode</a> about five times in two days, interrupting conversations and missing some channel history.</p><p>Of course, problem can be easily solved by making znc connect to IPv4 addresses, as it was doing before, but since there's no option like "connect to IPv4" and "irc.freenode.net" doesn't seem to have some alias like "ipv4.irc.freenode.net", that'd mean either specifying single IP in znc.conf (instead on DNS-provided list of servers) or filtering AAAA results, while leaving A records intact.<br />Latter solution seem to be better in many ways, so I decided to look for something that can override AAAA RR's for a single domain (irc.freenode.net in my case) or a configurable list of them.</p><p>I use dead-simple dnscache resolver from <a title="DJBDNS project" href="http://cr.yp.to/djbdns/">djbdns bundle</a>, which doesn't seem to be capable of such filtering by itself.<br /><a title="ISC BIND project" href="http://www.isc.org/software/bind">ISC BIND</a> seem to have "filter-aaaa" global option to provide A-only results to a list of clients/networks, but that's also not what I need, since it will make IPv6-only mirrors (upon which I seem to stumble more and more lately) inaccessible.<br />Rest of the recursive DNS resolvers doesn't seem to have even that capability, so some hack was needed here.</p><p>Useful feature that most resolvers have though is the ability to query specific DNS servers for a specific domains. Even dnscache is capable of doing that, so putting BIND with AAAA resolution disabled behind dnscache and forwarding freenode.net domain to it should do the trick.<br />But installing and running BIND just to resolve one (or maybe a few more, in the future) domain looks like an overkill to me, so I thought of <a title="twisted python project" href="http://twistedmatrix.com/">twisted</a> and it's names component, implementing DNS protocols.</p><p>And all it took with twisted to implement such no-AAAA DNS proxy, as it turns out, was these five lines of code:</p><pre name="code" class="python">
class IPv4OnlyResolver(client.Resolver):
	def lookupIPV6Address(self, name, timeout = None):
		return self._lookup('nx.fraggod.net', dns.IN, dns.AAAA, timeout)

protocol = dns.DNSDatagramProtocol(
	server.DNSServerFactory(clients=[IPv4OnlyResolver()]) )

</pre><p>Meh, should've skipped the search for existing implementation altogether.</p><p>That script plus "echo IP > /etc/djbdns/cache/servers/freenode.net" solved the problem, although dnscache doesn't seem to be capable of forwarding queries to non-standard port, so proxy has to be bound to specific localhost interface, not just some wildcard:port socket.</p><p>Code, with trivial CLI, logging, dnscache forwarders-file support and redirected AAAA-answer caching, is <a title="Source for the tool" href="http://fraggod.net/static/code/dns-aaaa-filter.py">here</a>.</p>-=|||=-<p>Had IPv6 tunnel from <a title="Hurricane Electric tunnelbroker" href="http://tunnelbroker.net/">HE</a> for a few years now, but since I've changed ISP about a year ago, I've been unable to use it because ISP dropped sit tunnel packets for some weird reason.<br />A quick check yesterday revealed that this limitation seem to have been lifted, so I've re-enabled the tunnel at once.</p><p>All the IPv6-enabled stuff started using AAAA-provided IPs at once, and that resulted in some problems.<br />Particulary annoying thing is that <a title="ZNC" href="http://en.znc.in/">ZNC IRC bouncer</a> managed to loose connection to <a title="FreeNode IRC network" href="http://freenode.net/">freenode</a> about five times in two days, interrupting conversations and missing some channel history.</p><p>Of course, problem can be easily solved by making znc connect to IPv4 addresses, as it was doing before, but since there's no option like "connect to IPv4" and "irc.freenode.net" doesn't seem to have some alias like "ipv4.irc.freenode.net", that'd mean either specifying single IP in znc.conf (instead on DNS-provided list of servers) or filtering AAAA results, while leaving A records intact.<br />Latter solution seem to be better in many ways, so I decided to look for something that can override AAAA RR's for a single domain (irc.freenode.net in my case) or a configurable list of them.</p><p>I use dead-simple dnscache resolver from <a title="DJBDNS project" href="http://cr.yp.to/djbdns/">djbdns bundle</a>, which doesn't seem to be capable of such filtering by itself.<br /><a title="ISC BIND project" href="http://www.isc.org/software/bind">ISC BIND</a> seem to have "filter-aaaa" global option to provide A-only results to a list of clients/networks, but that's also not what I need, since it will make IPv6-only mirrors (upon which I seem to stumble more and more lately) inaccessible.<br />Rest of the recursive DNS resolvers doesn't seem to have even that capability, so some hack was needed here.</p><p>Useful feature that most resolvers have though is the ability to query specific DNS servers for a specific domains. Even dnscache is capable of doing that, so putting BIND with AAAA resolution disabled behind dnscache and forwarding freenode.net domain to it should do the trick.<br />But installing and running BIND just to resolve one (or maybe a few more, in the future) domain looks like an overkill to me, so I thought of <a title="twisted python project" href="http://twistedmatrix.com/">twisted</a> and it's names component, implementing DNS protocols.</p><p>And all it took with twisted to implement such no-AAAA DNS proxy, as it turns out, was these five lines of code:</p><pre name="code" class="python"><br />class IPv4OnlyResolver(client.Resolver):<br />	def lookupIPV6Address(self, name, timeout = None):<br />		return self._lookup('nx.fraggod.net', dns.IN, dns.AAAA, timeout)<br /><br />protocol = dns.DNSDatagramProtocol(<br />	server.DNSServerFactory(clients=[IPv4OnlyResolver()]) )<br /><br /></pre><p>Meh, should've skipped the search for existing implementation altogether.</p><p>That script plus "echo IP > /etc/djbdns/cache/servers/freenode.net" solved the problem, although dnscache doesn't seem to be capable of forwarding queries to non-standard port, so proxy has to be bound to specific localhost interface, not just some wildcard:port socket.</p><p>Code, with trivial CLI, logging, dnscache forwarders-file support and redirected AAAA-answer caching, is <a title="Source for the tool" href="http://fraggod.net/static/code/dns-aaaa-filter.py">here</a>.</p>-=||||||||||=-2011/4/Key-Value-storage-with-historyversioning-on-top-of-scm-=|||=-2011-04-18T02:59:11-=|||=-Key-Value storage with history/versioning on top of scm-=|||=-[u'SCM', u'SysAdmin', u'Python']-=|||=-<p>Working with a number of non-synced servers remotely (via <a title="fabric home" href="http://fabfile.org/">fabric</a>) lately, I've found the need to push updates to a set of (fairly similar) files.</p><p>It's a bit different story for each server, of course, like crontabs for a web backend with a lot of periodic maintenance, data-shuffle and cache-related tasks, firewall configurations, common html templates... well, you get the idea.<br />I'm not the only one who makes the changes there, and without any change/version control for these sets of files, state for each file/server combo is essentially unique and accidental change can only be reverted from a weekly backup.<br />Not really a sensible infrastructure as far as I can tell (or just got used to), but since I'm a total noob here, working for only a couple of weeks, global changes are out of question, plus I've got my hands full with the other tasks as it is.</p><p>So, I needed to change files, keeping the old state for each one in case rollback is necessary, and actually check remote state before updating files, since someone might've introduced either the same or conflicting change while I was preparing mine.<br />Problem of conflicting changes can be solved by keeping some reference (local) state and just applying patches on top of it. If file in question is important enough, having such state is double-handy, since you can pull the remote state in case of changes there, look through the diff (if any) and then decide whether the patch is still valid or not.<br />Problem of rollbacks is solved long ago by various versioning tools.</p><p>Combined, two issues kinda beg for some sort of storage with a history of changes for each value there, and since it's basically a text, diffs and patches between any points of this history would also be nice to have.<br />It's the domain of the SCM's, but my use-case is a bit more complicated then the usual usage of these by the fact that I need to create new revisions non-interactively - ideally via something like a key-value api (set, get, get_older_version) with the usual interactive interface to the history at hand in case of any conflicts or complications.</p><p>Being most comfortable with <a title="git scm" href="http://git-scm.com/">git</a>, I looked for non-interactive db solutions on top of it, and the simpliest one I've found was <a title="gitshelve description and source links (blog post)" href="http://newartisans.com/2008/05/using-git-as-a-versioned-data-store-in-python/">gitshelve</a>. <a title="GitDB project" href="http://packages.python.org/gitdb/">GitDB</a> seem to be more powerful, but unnecessary complex for my use-case.<br />Then I just implemented patch (update key by a diff stream) and diff methods (generate diff stream from key and file) on top of gitshelve plus writeback operation, and thus got a fairly complete implementation of what I needed.</p><p>Looking at such storage from a DBA perspective, it's looking pretty good - integrity and atomicity are assured by git locking, all sorts of replication and merging possible in a quite efficient and robust manner via git-merge and friends, cli interface and transparency of operation is just superb. Regular storage performance is probably far off db level though, but it's not an issue in my use-case.</p><p>Here's <a title="gitshelve" href="http://newartisans.com/2008/05/using-git-as-a-versioned-data-store-in-python/">gitshelve</a> and <a title="StateDB implementation source" href="https://fraggod.net/code/static/state.py">state.py</a>, as used in my fabric stuff. fabric imports can be just dropped there without much problem (I use fabric api to vary keys depending on host).</p><p>Pity I'm far more used to git than pure-py solutions like <a title="mercurial (hg) scm" href="http://mercurial.selenic.com/">mercurial</a> or <a title="bazaar scm" href="http://bazaar.canonical.com/">bazaar</a>, since it'd have probably been much cleaner and simplier to implement such storage on top of them - they probably expose python interface directly.<br />Guess I'll put rewriting the thing on top of hg on my long todo list.</p>-=|||=-<p>Working with a number of non-synced servers remotely (via <a title="fabric home" href="http://fabfile.org/">fabric</a>) lately, I've found the need to push updates to a set of (fairly similar) files.</p><p>It's a bit different story for each server, of course, like crontabs for a web backend with a lot of periodic maintenance, data-shuffle and cache-related tasks, firewall configurations, common html templates... well, you get the idea.<br />I'm not the only one who makes the changes there, and without any change/version control for these sets of files, state for each file/server combo is essentially unique and accidental change can only be reverted from a weekly backup.<br />Not really a sensible infrastructure as far as I can tell (or just got used to), but since I'm a total noob here, working for only a couple of weeks, global changes are out of question, plus I've got my hands full with the other tasks as it is.</p><p>So, I needed to change files, keeping the old state for each one in case rollback is necessary, and actually check remote state before updating files, since someone might've introduced either the same or conflicting change while I was preparing mine.<br />Problem of conflicting changes can be solved by keeping some reference (local) state and just applying patches on top of it. If file in question is important enough, having such state is double-handy, since you can pull the remote state in case of changes there, look through the diff (if any) and then decide whether the patch is still valid or not.<br />Problem of rollbacks is solved long ago by various versioning tools.</p><p>Combined, two issues kinda beg for some sort of storage with a history of changes for each value there, and since it's basically a text, diffs and patches between any points of this history would also be nice to have.<br />It's the domain of the SCM's, but my use-case is a bit more complicated then the usual usage of these by the fact that I need to create new revisions non-interactively - ideally via something like a key-value api (set, get, get_older_version) with the usual interactive interface to the history at hand in case of any conflicts or complications.</p><p>Being most comfortable with <a title="git scm" href="http://git-scm.com/">git</a>, I looked for non-interactive db solutions on top of it, and the simpliest one I've found was <a title="gitshelve description and source links (blog post)" href="http://newartisans.com/2008/05/using-git-as-a-versioned-data-store-in-python/">gitshelve</a>. <a title="GitDB project" href="http://packages.python.org/gitdb/">GitDB</a> seem to be more powerful, but unnecessary complex for my use-case.<br />Then I just implemented patch (update key by a diff stream) and diff methods (generate diff stream from key and file) on top of gitshelve plus writeback operation, and thus got a fairly complete implementation of what I needed.</p><p>Looking at such storage from a DBA perspective, it's looking pretty good - integrity and atomicity are assured by git locking, all sorts of replication and merging possible in a quite efficient and robust manner via git-merge and friends, cli interface and transparency of operation is just superb. Regular storage performance is probably far off db level though, but it's not an issue in my use-case.</p><p>Here's <a title="gitshelve" href="http://newartisans.com/2008/05/using-git-as-a-versioned-data-store-in-python/">gitshelve</a> and <a title="StateDB implementation source" href="https://fraggod.net/code/static/state.py">state.py</a>, as used in my fabric stuff. fabric imports can be just dropped there without much problem (I use fabric api to vary keys depending on host).</p><p>Pity I'm far more used to git than pure-py solutions like <a title="mercurial (hg) scm" href="http://mercurial.selenic.com/">mercurial</a> or <a title="bazaar scm" href="http://bazaar.canonical.com/">bazaar</a>, since it'd have probably been much cleaner and simplier to implement such storage on top of them - they probably expose python interface directly.<br />Guess I'll put rewriting the thing on top of hg on my long todo list.</p>-=||||||||||=-2011/4/xdiskusage-like-visualization-for-any-remote-machine-=|||=-2011-04-19T14:57:50-=|||=-xdiskusage-like visualization for any remote machine-=|||=-[u'SysAdmin', u'CoffeeScript', u'Web']-=|||=-<p><a title="xdiskusage project" href="http://xdiskusage.sourceforge.net/">xdiskusage(1)</a> is a simple and useful tool to visualize disk space usage (a must-have thing in any admin's toolkit!).<br />Probably the best thing about it is that it's built on top of "du" command, so if there's a problem with free space on a remote X-less server, just "ssh user@host 'du -k' | xdiskusage" and in a few moments you'll get the idea where the space has gone to.</p><p>Lately though I've had problems building fltk, and noticed that xdiskusage is the only app that uses it on my system, so I just got rid of both, in hopes that I'll be able to find some lite gtk replacement (don't have qt either).<br />Maybe I do suck at googling (or just giving up too early), but <a title="filelight tool" href="http://www.methylblue.com/filelight/">filelight</a> (kde util), <a title="baobab tool" href="http://www.marzocca.net/linux/baobab/">baobab</a> (gnome util) and <a title="philesight tool" href="http://zevv.nl/play/code/philesight/">philesight</a> (ruby) are pretty much the only alternatives I've found. First one drags in half of the kde, second one - half of gnome, and I don't really need ruby in my system either.<br />And for what? xdiskusage seem to be totally sufficient and much easier to interpret (apparently it's a lot easier to compare lengths than angles for me) than stupid round graphs that filelight and it's ruby clone produce, plus it looks like a no-brainer to write.<br />There are some CLI alternatives as well, but this task is definitely outside of CLI domain.</p><p>Thus I wrote <a title="web-based du tool" href="http://fraggod.net/static/code/du/">this tool</a>. Real source is actually <a title="coffee-script compiler" href="http://jashkenas.github.com/coffee-script/">coffeescript</a>, <a title="CoffeeScript source" href="http://fraggod.net/static/code/du/du.coffee">here</a>, JS is compiled from it.<a href="/static/embed/web_du.jpg"><img alt="it's just like xdiskusage" title="it's just like xdiskusage" style="width: 600px; height: 352px; display: block;" src="/static/embed/web_du.jpg" align="middle" /></a><br />Initially I wanted to do this in python, but then took a break to read some reddit and blogs, which just happened to push me in the direction of a web. Good thing they did, too, as it turned out to be simple and straightforward to work with graphics there these days.<br />I didn't use (much-hyped) html5 canvas though, since svg seem to be much more fitting in html world, plus it's much easier to make it interactive (titles, events, changes, etc).</p><p>Aside from the intended stuff, tool also shows performance shortcomings in firefox and opera browsers - they both are horribly slow on pasting large text into textarea (or iframe with "design mode") and just slow on rendering svg. Google chrome is fairly good at both tasks.<br />Not that I'll migrate all my firefox addons/settings and habits to chrome anytime soon, but it's certainly something to think about.</p><p>Also, JS calculations can probably be made hundred-times faster by caching size of the traversed subtrees (right now they're recalculated gozillion times over, and that's basically all the work).<br />I was just too lazy to do it initially and textarea pasting is still a lot slower than JS, so it doesn't seem to be a big deal, but guess I'll do that eventually anyway.</p>-=|||=-<p><a title="xdiskusage project" href="http://xdiskusage.sourceforge.net/">xdiskusage(1)</a> is a simple and useful tool to visualize disk space usage (a must-have thing in any admin's toolkit!).<br />Probably the best thing about it is that it's built on top of "du" command, so if there's a problem with free space on a remote X-less server, just "ssh user@host 'du -k' | xdiskusage" and in a few moments you'll get the idea where the space has gone to.</p><p>Lately though I've had problems building fltk, and noticed that xdiskusage is the only app that uses it on my system, so I just got rid of both, in hopes that I'll be able to find some lite gtk replacement (don't have qt either).<br />Maybe I do suck at googling (or just giving up too early), but <a title="filelight tool" href="http://www.methylblue.com/filelight/">filelight</a> (kde util), <a title="baobab tool" href="http://www.marzocca.net/linux/baobab/">baobab</a> (gnome util) and <a title="philesight tool" href="http://zevv.nl/play/code/philesight/">philesight</a> (ruby) are pretty much the only alternatives I've found. First one drags in half of the kde, second one - half of gnome, and I don't really need ruby in my system either.<br />And for what? xdiskusage seem to be totally sufficient and much easier to interpret (apparently it's a lot easier to compare lengths than angles for me) than stupid round graphs that filelight and it's ruby clone produce, plus it looks like a no-brainer to write.<br />There are some CLI alternatives as well, but this task is definitely outside of CLI domain.</p><p>Thus I wrote <a title="web-based du tool" href="http://fraggod.net/static/code/du/">this tool</a>. Real source is actually <a title="coffee-script compiler" href="http://jashkenas.github.com/coffee-script/">coffeescript</a>, <a title="CoffeeScript source" href="http://fraggod.net/static/code/du/du.coffee">here</a>, JS is compiled from it.<a href="/static/embed/web_du.jpg"><img alt="it's just like xdiskusage" title="it's just like xdiskusage" style="width: 600px; height: 352px; display: block;" src="/static/embed/web_du.jpg" align="middle" /></a><br />Initially I wanted to do this in python, but then took a break to read some reddit and blogs, which just happened to push me in the direction of a web. Good thing they did, too, as it turned out to be simple and straightforward to work with graphics there these days.<br />I didn't use (much-hyped) html5 canvas though, since svg seem to be much more fitting in html world, plus it's much easier to make it interactive (titles, events, changes, etc).</p><p>Aside from the intended stuff, tool also shows performance shortcomings in firefox and opera browsers - they both are horribly slow on pasting large text into textarea (or iframe with "design mode") and just slow on rendering svg. Google chrome is fairly good at both tasks.<br />Not that I'll migrate all my firefox addons/settings and habits to chrome anytime soon, but it's certainly something to think about.</p><p>Also, JS calculations can probably be made hundred-times faster by caching size of the traversed subtrees (right now they're recalculated gozillion times over, and that's basically all the work).<br />I was just too lazy to do it initially and textarea pasting is still a lot slower than JS, so it doesn't seem to be a big deal, but guess I'll do that eventually anyway.</p>-=||||||||||=-2011/5/Fossil-to-Git-export-and-mirroring-=|||=-2011-05-02T14:34:23-=|||=-Fossil to Git export and mirroring-=|||=-[u'Fossil', u'SCM', u'Python', u'SysAdmin']-=|||=-<p>The biggest issue I have with <a title="fossil scm" href="http://fossil-scm.org/">fossil scm</a> is that it's not <a title="git scm" href="http://git-scm.com/">git</a> - there are just too many advanced tools which I got used to with git over time, which probably will never be implemented in fossil just because of it's "lean single binary" philosophy.<br />And things get even worse when you need to bridge git-fossil repos - common denominator here is git, so it's either constant "export-merge-import" cycle or some hacks, since fossil doesn't support incremental export to a git repo out of the box (but it does have <a title="fossil/git full export/import" href="http://www.fossil-scm.org/index.html/doc/trunk/www/inout.wiki">support for full import/export</a>), and git doesn't seem to have a plugin to track fossil remotes (yet?).</p><p>I thought of migrating away from fossil, but there's just no substitute (although <a title="list of distributed issue-tracking software" href="http://dist-bugs.kitenet.net/software/">quite a lot of attempts to implement that</a>) for distributed issue tracking and documentation right in the same repository and plain easy to access format with a sensible web frontend for those who don't want to install/learn scm and clone the repo just to file a ticket.<br />None of git-based tools I've been able to find seem to meet this (seemingly) simple criterias, so dual-stack it is then.</p><p>Solution I came up with is real-time mirroring of all the changes in fossil repositories to a git.<br />It's <a title="script itself" href="http://fraggod.net/static/code/fossil_echo.py">quite a simple script</a>, which is</p><ul>
<li>watching fossil-path with inotify(7) for IN_MODIFY events (needs <a title="pyinotify project" href="http://pyinotify.sourceforge.net/">pyinotify</a> for that)</li>
<li>checking for new revisions in fossil (source) repo against tip of a git</li>
<li>comparing these by timestamps, which are kept in perfect sync (by fossil-export as well)</li>
<li>exporting revisions from fossil as a full artifacts (blobs), importing these into git via git-fast-import</li>
</ul>
<p>It's also capable to do oneshot updates (in which case it doesn't need anything but python-2.7, git and fossil), bootstrapping git mirrors as new fossil repos are created and catching-up with their sync on startup.</p><p>While the script uses quite a low-level (but standard and documented <a title="fossil artifact format documentation" href="http://fossil-scm.org/index.html/doc/trunk/www/fileformat.wiki">here</a> and <a title="git-fast-import manpage" href="http://www.kernel.org/pub/software/scm/git/docs/git-fast-import.html">there</a>) scm internals, it was actually very easy to write (~200 lines, mostly simple processing-generation code), because both scms in question are built upon principles of simple and robust design, which I deeply admire.</p><p><a title="my sample mirrors" href="http://fraggod.net/code/git">Resulting mirrors of fossil repos</a> retain all the metadata like commit messages, timestamps and authors.<br />Limitation is that it only tracks one branch, specified at startup ("trunk", by default), and doesn't care about the tags at the moment, but I'll probably fix the latter when I'll do some tagging next time (hence will have a realworld test case).<br />It's also trivial to make the script do two-way synchronization, since fossil supports "fossil import --incremental" update right from git-fast-export, so it's just a simple pipe, which can be run w/o any special tools on demand.</p><p><a title="fossil_echo.py script" href="http://fraggod.net/static/code/fossil_echo.py">Script itself</a>.</p><p>fossil_echo --help:</p><pre>usage: fossil_echo [-h] [-1] [-s] [-c] [-b BRANCH] [--dry-run] [-x EXCLUDE]<br />     [--debug]<br />     fossil_root git_root<br /><br />Tool to keep fossil and git repositories in sync. Monitors fossil_root for<br />changes in *.fossil files (which are treated as source fossil repositories)<br />and pushes them to corresponding (according to basename) git repositories.<br />Also has --oneshot mode to do a one-time sync between specified repos.<br /><br />positional arguments:<br /> fossil_root   Path to fossil repos.<br /> git_root    Path to git repos.<br /><br />optional arguments:<br /> -h, --help   show this help message and exit<br /> -1, --oneshot   Treat fossil_root and git_root as repository paths and<br />      try to sync them at once.<br /> -s, --initial-sync Do an initial sync for every *.fossil repository found<br />      in fossil_root at start.<br /> -c, --create   Dynamically create missing git repositories (bare)<br />      inside git-root.<br /> -b BRANCH, --branch BRANCH<br />      Branch to sync (must exist on both sides, default:<br />      trunk).<br /> --dry-run    Dump git updates (fast-import format) to stdout,<br />      instead of feeding them to git. Cancels --create.<br /> -x EXCLUDE, --exclude EXCLUDE<br />      Repository names to exclude from syncing (w/o .fossil<br />      or .git suffix, can be specified multiple times).<br /> --debug    Verbose operation mode.<br /><br /></pre>-=|||=-<p>The biggest issue I have with <a title="fossil scm" href="http://fossil-scm.org/">fossil scm</a> is that it's not <a title="git scm" href="http://git-scm.com/">git</a> - there are just too many advanced tools which I got used to with git over time, which probably will never be implemented in fossil just because of it's "lean single binary" philosophy.<br />And things get even worse when you need to bridge git-fossil repos - common denominator here is git, so it's either constant "export-merge-import" cycle or some hacks, since fossil doesn't support incremental export to a git repo out of the box (but it does have <a title="fossil/git full export/import" href="http://www.fossil-scm.org/index.html/doc/trunk/www/inout.wiki">support for full import/export</a>), and git doesn't seem to have a plugin to track fossil remotes (yet?).</p><p>I thought of migrating away from fossil, but there's just no substitute (although <a title="list of distributed issue-tracking software" href="http://dist-bugs.kitenet.net/software/">quite a lot of attempts to implement that</a>) for distributed issue tracking and documentation right in the same repository and plain easy to access format with a sensible web frontend for those who don't want to install/learn scm and clone the repo just to file a ticket.<br />None of git-based tools I've been able to find seem to meet this (seemingly) simple criterias, so dual-stack it is then.</p><p>Solution I came up with is real-time mirroring of all the changes in fossil repositories to a git.<br />It's <a title="script itself" href="http://fraggod.net/static/code/fossil_echo.py">quite a simple script</a>, which is</p><ul>
<li>watching fossil-path with inotify(7) for IN_MODIFY events (needs <a title="pyinotify project" href="http://pyinotify.sourceforge.net/">pyinotify</a> for that)</li>
<li>checking for new revisions in fossil (source) repo against tip of a git</li>
<li>comparing these by timestamps, which are kept in perfect sync (by fossil-export as well)</li>
<li>exporting revisions from fossil as a full artifacts (blobs), importing these into git via git-fast-import</li>
</ul>
<p>It's also capable to do oneshot updates (in which case it doesn't need anything but python-2.7, git and fossil), bootstrapping git mirrors as new fossil repos are created and catching-up with their sync on startup.</p><p>While the script uses quite a low-level (but standard and documented <a title="fossil artifact format documentation" href="http://fossil-scm.org/index.html/doc/trunk/www/fileformat.wiki">here</a> and <a title="git-fast-import manpage" href="http://www.kernel.org/pub/software/scm/git/docs/git-fast-import.html">there</a>) scm internals, it was actually very easy to write (~200 lines, mostly simple processing-generation code), because both scms in question are built upon principles of simple and robust design, which I deeply admire.</p><p><a title="my sample mirrors" href="http://fraggod.net/code/git">Resulting mirrors of fossil repos</a> retain all the metadata like commit messages, timestamps and authors.<br />Limitation is that it only tracks one branch, specified at startup ("trunk", by default), and doesn't care about the tags at the moment, but I'll probably fix the latter when I'll do some tagging next time (hence will have a realworld test case).<br />It's also trivial to make the script do two-way synchronization, since fossil supports "fossil import --incremental" update right from git-fast-export, so it's just a simple pipe, which can be run w/o any special tools on demand.</p><p><a title="fossil_echo.py script" href="http://fraggod.net/static/code/fossil_echo.py">Script itself</a>.</p><p>fossil_echo --help:</p><pre>usage: fossil_echo [-h] [-1] [-s] [-c] [-b BRANCH] [--dry-run] [-x EXCLUDE]<br />     [--debug]<br />     fossil_root git_root<br /><br />Tool to keep fossil and git repositories in sync. Monitors fossil_root for<br />changes in *.fossil files (which are treated as source fossil repositories)<br />and pushes them to corresponding (according to basename) git repositories.<br />Also has --oneshot mode to do a one-time sync between specified repos.<br /><br />positional arguments:<br /> fossil_root   Path to fossil repos.<br /> git_root    Path to git repos.<br /><br />optional arguments:<br /> -h, --help   show this help message and exit<br /> -1, --oneshot   Treat fossil_root and git_root as repository paths and<br />      try to sync them at once.<br /> -s, --initial-sync Do an initial sync for every *.fossil repository found<br />      in fossil_root at start.<br /> -c, --create   Dynamically create missing git repositories (bare)<br />      inside git-root.<br /> -b BRANCH, --branch BRANCH<br />      Branch to sync (must exist on both sides, default:<br />      trunk).<br /> --dry-run    Dump git updates (fast-import format) to stdout,<br />      instead of feeding them to git. Cancels --create.<br /> -x EXCLUDE, --exclude EXCLUDE<br />      Repository names to exclude from syncing (w/o .fossil<br />      or .git suffix, can be specified multiple times).<br /> --debug    Verbose operation mode.<br /><br /></pre>-=||||||||||=-2011/5/Backup-of-5-million-tiny-files-and-paths-=|||=-2011-05-08T11:57:09-=|||=-Backup of 5 million tiny files and paths-=|||=-[u'SysAdmin', u'Unix', u'Python']-=|||=-<p>I think in ideal world this shouldn't be happening, it really is a job for a proper database engine.<br />Some filesystems (reiserfs, <a title="Pomegranate FS" href="https://github.com/macan/Pomegranate/wiki">pomegranate</a>) are fairly good at dealing with such use-cases though, but not the usual tools for working with fs-based data, which generally suck all the time and resources on such a mess.</p><p>In my particular case, there's a (mostly) legacy system, which keeps such tons-of-files db with ~5M files, taking about 5G of space, which have to be backed-up somehow. Every file can be changed, added or unlinked, total consistency between parts (like snapshotting the same point in time for every file) is not necessary. Contents are (typically) php serializations (yuck!).</p><p>Tar and rsync are prime example of tools that aren't quite fit for the task - both eat huge amounts of RAM (gigs) and time to do this, especially when you have to make these backups incremental, and ideally this path should be backed-up every single day.<br />Both seem to build some large and not very efficient list of existing files in memory and then do a backup against that. Both aren't really good at capturing the state - increments either take a huge amounts of space/inodes (with rsync --link-dest) or loose info on removed entries (tar).</p><p>Nice off-the-shelf alternatives are <a title="dar archiver" href="http://dar.linux.free.fr/">dar</a>, which is not a fs-to-stream packer, but rather squashfs-like image builder with the ability to make proper incremental backups, and of course <a title="SquashFS project" href="http://squashfs.sourceforge.net/">mksquashfs</a> itself, which supports append these days.<br />These sound nice, but somehow I failed to check for "append" support in squashfs (although I remember hearing about it before), plus there's still doesn't seem to be a way to remove paths.<br />dar seem to be good enough solution, and I'll probably get back to it, but as I was investigating "the right way" to do such backups, first thing that naturally came to mind (probably because even <a title="Ted Tso argument for using db for small files" href="https://bugs.launchpad.net/ubuntu/+source/linux/+bug/317781/comments/54">fs developers suggest that</a>), is to cram all this mess into a single db, and I wanted to test it via <a title="bdb_backup script" href="http://fraggod.net/static/code/bdb_backup.py">straightforward fs - berkdb (btree) implementation</a>.</p><p>Results turned out to be really good - 40min to back all this stuff up from scratch and under 20min to do an incremental update (mostly comparing the timestamps plus adding/removing new/unlinked keys).<br />Implementation on top of berkdb also turned out to be fairly straightorward (just 150 lines total!) with just a little bit of optimization magic to put higter-level paths before the ones nested inside (by adding \0 and \1 bytes before basename for file/dir).</p><p>I still need to test it against dar and squashfs when I'll have more time (as if that will ever happen) on my hands, but even such makeshift python implementation (which includes full "extract" and "list" functionality though) proven to be sufficient and ended up in a daily crontab.<br />So much for the infamous "don't keep the files in a database!" argument, btw, wonder if original developers of this "db" used this hype to justify this mess...</p><p>Obligatory <a title="bdb_backup script" href="http://fraggod.net/static/code/bdb_backup.py">proof-of-concept code link</a>.</p><p><strong>Update: </strong>tried mksquashfs, but quickly pulled a plug as it started to eat more than 3G of RAM - sadly unfit for the task as well. dar also ate ~1G and been at it for a few hours, guess no tool cares about such fs use-cases at all.</p>-=|||=-<p>I think in ideal world this shouldn't be happening, it really is a job for a proper database engine.<br />Some filesystems (reiserfs, <a title="Pomegranate FS" href="https://github.com/macan/Pomegranate/wiki">pomegranate</a>) are fairly good at dealing with such use-cases though, but not the usual tools for working with fs-based data, which generally suck all the time and resources on such a mess.</p><p>In my particular case, there's a (mostly) legacy system, which keeps such tons-of-files db with ~5M files, taking about 5G of space, which have to be backed-up somehow. Every file can be changed, added or unlinked, total consistency between parts (like snapshotting the same point in time for every file) is not necessary. Contents are (typically) php serializations (yuck!).</p><p>Tar and rsync are prime example of tools that aren't quite fit for the task - both eat huge amounts of RAM (gigs) and time to do this, especially when you have to make these backups incremental, and ideally this path should be backed-up every single day.<br />Both seem to build some large and not very efficient list of existing files in memory and then do a backup against that. Both aren't really good at capturing the state - increments either take a huge amounts of space/inodes (with rsync --link-dest) or loose info on removed entries (tar).</p><p>Nice off-the-shelf alternatives are <a title="dar archiver" href="http://dar.linux.free.fr/">dar</a>, which is not a fs-to-stream packer, but rather squashfs-like image builder with the ability to make proper incremental backups, and of course <a title="SquashFS project" href="http://squashfs.sourceforge.net/">mksquashfs</a> itself, which supports append these days.<br />These sound nice, but somehow I failed to check for "append" support in squashfs (although I remember hearing about it before), plus there's still doesn't seem to be a way to remove paths.<br />dar seem to be good enough solution, and I'll probably get back to it, but as I was investigating "the right way" to do such backups, first thing that naturally came to mind (probably because even <a title="Ted Tso argument for using db for small files" href="https://bugs.launchpad.net/ubuntu/+source/linux/+bug/317781/comments/54">fs developers suggest that</a>), is to cram all this mess into a single db, and I wanted to test it via <a title="bdb_backup script" href="http://fraggod.net/static/code/bdb_backup.py">straightforward fs - berkdb (btree) implementation</a>.</p><p>Results turned out to be really good - 40min to back all this stuff up from scratch and under 20min to do an incremental update (mostly comparing the timestamps plus adding/removing new/unlinked keys).<br />Implementation on top of berkdb also turned out to be fairly straightorward (just 150 lines total!) with just a little bit of optimization magic to put higter-level paths before the ones nested inside (by adding \0 and \1 bytes before basename for file/dir).</p><p>I still need to test it against dar and squashfs when I'll have more time (as if that will ever happen) on my hands, but even such makeshift python implementation (which includes full "extract" and "list" functionality though) proven to be sufficient and ended up in a daily crontab.<br />So much for the infamous "don't keep the files in a database!" argument, btw, wonder if original developers of this "db" used this hype to justify this mess...</p><p>Obligatory <a title="bdb_backup script" href="http://fraggod.net/static/code/bdb_backup.py">proof-of-concept code link</a>.</p><p><strong>Update: </strong>tried mksquashfs, but quickly pulled a plug as it started to eat more than 3G of RAM - sadly unfit for the task as well. dar also ate ~1G and been at it for a few hours, guess no tool cares about such fs use-cases at all.</p>-=||||||||||=-2011/6/Using-csync2-for-security-sensitive-paths-=|||=-2011-06-12T16:25:02-=|||=-Using csync2 for security-sensitive paths-=|||=-[u'SysAdmin', u'Unix', u'Python', u'SSH']-=|||=-<p>Usually I was using fabric to clone similar stuff to many machines, but since I've been deploying <a title="csync2 project" href="http://oss.linbit.com/csync2/">csync2</a> everywhere to sync some web templates and I'm not the only one introducing changes, it ocurred to me that it'd be great to use it for scripts as well.<br />Problem I see there is security - most scripts I need to sync are cronjobs executed as root, so updating some script one one (compromised) machine with "rm -Rf /*" and running csync2 to push this change to other machines will cause a lot of trouble.</p><p>So I came up with simple way to provide one-time keys to csync2 hosts, which will be valid only when I want them to.</p><p>Idea is to create FIFO socket in place of a key on remote hosts, then just pipe a key into each socket while script is running on my dev machine. Simpliest form of such "pipe" I could come up with is an "ssh host 'cat &gt;remote.key.fifo'", no fancy sockets, queues or protocols.<br />That way, even if one host is compromised changes can't be propagnated to other hosts without access to fifo sockets there and knowing the right key. Plus running sync for that "privileged" group accidentally will just result in a hang 'till the script will push data to fifo socket - nothing will break down or crash horribly, just wait.<br />Key can be spoofed of course, and sync can be timed to the moment the keys are available, so the method is far from perfect, but it's insanely fast and convenient.</p><p><a title="script source" href="http://fraggod.net/static/code/csync2_unlocker.py">Implementation</a> is fairly simple <a title="twisted framework" href="http://twistedmatrix.com/">twisted</a> eventloop, spawning ssh processes (guess <a title="twisted conch module" href="http://twistedmatrix.com/trac/wiki/TwistedConch">twisted.conch</a> or stuff like <a title="paramiko python ssh2 implementation" href="http://www.lag.net/paramiko/">paramiko</a> can be used for ssh implementation there, but neither performance nor flexibility is an issue with ssh binary).<br />Script also (by default) figures out the hosts to connect to from the provided group name(s) and the local copy of csync2 configuration file, so I don't have to specify keep separate list of these or specify them each time.<br />As always, twisted makes it insanely simple to write such IO-parallel loop.</p><p>csync2 can be configured like this:</p><pre>group sbin_sync {<br />	host host1 host2;<br />	key /var/lib/csync2/privileged.key;<br /><br />	include /usr/local/sbin/*.sh<br />}<br /></pre><p>And then I just run it with something like "./csync2_unlocker.py sbin_sync" when I need to replicate updates between hosts.</p><p><a title="script source" href="http://fraggod.net/static/code/csync2_unlocker.py">Source</a>.</p>-=|||=-<p>Usually I was using fabric to clone similar stuff to many machines, but since I've been deploying <a title="csync2 project" href="http://oss.linbit.com/csync2/">csync2</a> everywhere to sync some web templates and I'm not the only one introducing changes, it ocurred to me that it'd be great to use it for scripts as well.<br />Problem I see there is security - most scripts I need to sync are cronjobs executed as root, so updating some script one one (compromised) machine with "rm -Rf /*" and running csync2 to push this change to other machines will cause a lot of trouble.</p><p>So I came up with simple way to provide one-time keys to csync2 hosts, which will be valid only when I want them to.</p><p>Idea is to create FIFO socket in place of a key on remote hosts, then just pipe a key into each socket while script is running on my dev machine. Simpliest form of such "pipe" I could come up with is an "ssh host 'cat &gt;remote.key.fifo'", no fancy sockets, queues or protocols.<br />That way, even if one host is compromised changes can't be propagnated to other hosts without access to fifo sockets there and knowing the right key. Plus running sync for that "privileged" group accidentally will just result in a hang 'till the script will push data to fifo socket - nothing will break down or crash horribly, just wait.<br />Key can be spoofed of course, and sync can be timed to the moment the keys are available, so the method is far from perfect, but it's insanely fast and convenient.</p><p><a title="script source" href="http://fraggod.net/static/code/csync2_unlocker.py">Implementation</a> is fairly simple <a title="twisted framework" href="http://twistedmatrix.com/">twisted</a> eventloop, spawning ssh processes (guess <a title="twisted conch module" href="http://twistedmatrix.com/trac/wiki/TwistedConch">twisted.conch</a> or stuff like <a title="paramiko python ssh2 implementation" href="http://www.lag.net/paramiko/">paramiko</a> can be used for ssh implementation there, but neither performance nor flexibility is an issue with ssh binary).<br />Script also (by default) figures out the hosts to connect to from the provided group name(s) and the local copy of csync2 configuration file, so I don't have to specify keep separate list of these or specify them each time.<br />As always, twisted makes it insanely simple to write such IO-parallel loop.</p><p>csync2 can be configured like this:</p><pre>group sbin_sync {<br />	host host1 host2;<br />	key /var/lib/csync2/privileged.key;<br /><br />	include /usr/local/sbin/*.sh<br />}<br /></pre><p>And then I just run it with something like "./csync2_unlocker.py sbin_sync" when I need to replicate updates between hosts.</p><p><a title="script source" href="http://fraggod.net/static/code/csync2_unlocker.py">Source</a>.</p>-=||||||||||=-2011/8/Notification-daemon-in-python-=|||=-2011-08-14T09:12:40-=|||=-Notification-daemon in python-=|||=-[u'Python', u'Desktop', u'Unix', u'Notification', u'Rate-limiting']-=|||=-<p>I've delayed update of the whole libnotify / notification-daemon / notify-python stack for a while now, because notification-daemon got too GNOME-oriented around 0.7, making it a lot more simplier, but sadly dropping lots of good stuff I've used there.<br />Default nice-looking theme is gone in favor of black blobs (although colors are probably subject to gtkrc); it's one-note-at-a-time only, which makes reading them intolerable; configurability was dropped as well, guess blobs follow some gnome-panel settings now.<br />Older notification-daemon versions won't build with newer libnotify. Same problem with notify-python, which seem to be unnecessary now, since it's functionality is accessible via introspection and <a title="PyGObject project" href="http://live.gnome.org/PyGObject">PyGObject</a> (part known as <a title="PyGI project page" href="http://live.gnome.org/PyGI">PyGI</a> before merge - gi.repositories.Notify).</p><p>Looking for more-or-less drop-in replacements I've found <a title="notipy notification daemon" href="https://github.com/the-isz/notipy">notipy</a> project, which looked like what I needed, and the best part is that it's python - no need to filter notification requests in a proxy anymore, eliminating some associated complexity.<br />Project has a bit different goals however, them being simplicity, less deps and concept separation, so I incorporated (more-or-less) notipy as a simple NotificationDisplay class into notification-proxy, making it into "<a title="notification-thing source" href="http://fraggod.net/static/code/notification-thing.py">notification-thing</a>" (first name that came to mind, not that it matters).</p><p>All the rendering now is in python using PyGObject (gi) / gtk-3.0 toolkit, which seem to be a good idea, given that I still have no reason to keep Qt in my system, and gtk-2.0 being obsolete.<br />Exploring newer Gtk stuff like <a title="GtkCssProvider docs" href="http://developer.gnome.org/gtk3/3.1/GtkCssProvider.html">css styling</a> and honest auto-generated interfaces was fun, although the whole mess seem to be much harder than expected. Simple things like adding a border, margins or some non-solid background to existing widgets seem to be very complex and totally counter-intuitive, unlike say, doing the same (even in totally cross-browser fashion) with html. I also failed to find a way to just draw what I want on arbitrary widgets, looks like it was removed (in favor of GtkDrawable) on purpose.<br />My (uneducated) guess is that gtk authors geared toward "one way to do one thing" philosophy, but unlike Python motto, they've to ditch the "one *obvious* way" part. But then, maybe it's just me being too lazy to read docs properly.</p><p><a title="notification-thing desktop notification daemon" href="http://fraggod.net/static/code/notification-thing.py">Source link</a>. All the previous features like <a title="post about filtering feature (and some others)" href="http://blog.fraggod.net/2010/12/Further-improvements-on-notification-daemon">filtering</a> and <a title="initial post about rate-limiting implementation as notification-proxy" href="http://blog.fraggod.net/2010/2/libnotify-notification-daemon-shortcomings-and-my-solution">rate-limiting</a> are there.</p><p>Looking over <a title="Desktop Notifications Specification" href="http://developer.gnome.org/notification-spec/">Desktop Notifications Spec</a> in process, I've noticed that there are more good ideas that I'm not using, so guess I might need to revisit local notification setup in the near future.</p>-=|||=-<p>I've delayed update of the whole libnotify / notification-daemon / notify-python stack for a while now, because notification-daemon got too GNOME-oriented around 0.7, making it a lot more simplier, but sadly dropping lots of good stuff I've used there.<br />Default nice-looking theme is gone in favor of black blobs (although colors are probably subject to gtkrc); it's one-note-at-a-time only, which makes reading them intolerable; configurability was dropped as well, guess blobs follow some gnome-panel settings now.<br />Older notification-daemon versions won't build with newer libnotify. Same problem with notify-python, which seem to be unnecessary now, since it's functionality is accessible via introspection and <a title="PyGObject project" href="http://live.gnome.org/PyGObject">PyGObject</a> (part known as <a title="PyGI project page" href="http://live.gnome.org/PyGI">PyGI</a> before merge - gi.repositories.Notify).</p><p>Looking for more-or-less drop-in replacements I've found <a title="notipy notification daemon" href="https://github.com/the-isz/notipy">notipy</a> project, which looked like what I needed, and the best part is that it's python - no need to filter notification requests in a proxy anymore, eliminating some associated complexity.<br />Project has a bit different goals however, them being simplicity, less deps and concept separation, so I incorporated (more-or-less) notipy as a simple NotificationDisplay class into notification-proxy, making it into "<a title="notification-thing source" href="http://fraggod.net/static/code/notification-thing.py">notification-thing</a>" (first name that came to mind, not that it matters).</p><p>All the rendering now is in python using PyGObject (gi) / gtk-3.0 toolkit, which seem to be a good idea, given that I still have no reason to keep Qt in my system, and gtk-2.0 being obsolete.<br />Exploring newer Gtk stuff like <a title="GtkCssProvider docs" href="http://developer.gnome.org/gtk3/3.1/GtkCssProvider.html">css styling</a> and honest auto-generated interfaces was fun, although the whole mess seem to be much harder than expected. Simple things like adding a border, margins or some non-solid background to existing widgets seem to be very complex and totally counter-intuitive, unlike say, doing the same (even in totally cross-browser fashion) with html. I also failed to find a way to just draw what I want on arbitrary widgets, looks like it was removed (in favor of GtkDrawable) on purpose.<br />My (uneducated) guess is that gtk authors geared toward "one way to do one thing" philosophy, but unlike Python motto, they've to ditch the "one *obvious* way" part. But then, maybe it's just me being too lazy to read docs properly.</p><p><a title="notification-thing desktop notification daemon" href="http://fraggod.net/static/code/notification-thing.py">Source link</a>. All the previous features like <a title="post about filtering feature (and some others)" href="http://blog.fraggod.net/2010/12/Further-improvements-on-notification-daemon">filtering</a> and <a title="initial post about rate-limiting implementation as notification-proxy" href="http://blog.fraggod.net/2010/2/libnotify-notification-daemon-shortcomings-and-my-solution">rate-limiting</a> are there.</p><p>Looking over <a title="Desktop Notifications Specification" href="http://developer.gnome.org/notification-spec/">Desktop Notifications Spec</a> in process, I've noticed that there are more good ideas that I'm not using, so guess I might need to revisit local notification setup in the near future.</p>-=||||||||||=-2011/9/Detailed-process-memory-accounting-including-shared-and-swapped-one-=|||=-2011-09-16T18:12:06-=|||=-Detailed process memory accounting, including shared and swapped one-=|||=-[u'Unix', u'SysAdmin']-=|||=-<p>Two questions:</p><ul>
<li>How to tell which pids (or groups of forks) eat most swap right now?</li>
<li>How much RAM one apache/php/whatever really consumes?</li>
</ul>
<p>Somehow people keep pointing me at "top" and "ps" tools to do this sort of things, but there's an obvious problem:</p><pre name="code" class="c">#include &lt;stdlib.h&gt;
#include &lt;unistd.h&gt;

#define G 1024*1024*1024

int main (void) {
	(void *) malloc(2 * G);
	sleep(10);
	return 0;
}
</pre><p>This code will immediately float to 1st position in top, sorted by "swap" (F p &lt;return&gt;), showing 2G even with no swap in the system.</p><p>Second question/issue is also common but somehow not universally recognized, which is kinda obvious when scared admins (or whoever happen to ssh into web backend machine) see N pids of something, summing up to more than total amount of RAM in the system, like 50 httpd processes 50M each.<br />It gets even worse when tools like "atop" helpfully aggregate the numbers ("atop -p"), showing that there are 6 sphinx processes, eating 15G on a machine with 4-6G physical RAM + 4-8G swap, causing local panic and mayhem.<br />The answer is, of course, that sphinx, apache and pretty much anything using worker processes share a lot of memory pages between their processes, and not just because of shared objects like libc.</p><p>Guess it's just general ignorance of how memory works in linux (or other unix-os'es) of those who never had to write a fork() or deal with malloc's in C, which kinda make lots of these concepts look fairly trivial.</p><p>So, mostly out of curiosity than the real need, decided to find a way to answer these questions.<br />proc(5) reveals this data more-or-less via "maps" / "smaps" files, but that needs some post-processing to give per-pid numbers.<br />Closest tools I was able to find were pmap from procps package and <a title="ps_mem.py script" href="http://www.pixelbeat.org/scripts/ps_mem.py">ps_mem.py script</a> from coreutils maintainer. Former seem to give only mapped memory region sizes, latter cleverly shows shared memory divided by a number of similar processes, omitting per-process numbers and swap.<br />Oh, and of course there are glorious valgrind and gdb, but both seem to be active debugging tools, not much suitable for normal day-to-day operation conditions and a bit too complex for the task.</p><p>So I though I'd write my own tool for the job to put the matter at rest once and for all, and so I can later point people at it and just say "see?" (although I bet it'll never be that simple).</p><p>Idea is to group similar processes (by cmd) and show details for each one, like this:</p><pre>agetty:<br /> -stats:<br />  private: 252.0 KiB<br />  shared: 712.0 KiB<br />  swap: 0<br /><br /> 7606:<br />  -stats:<br />   private: 84.0 KiB<br />   shared: 712.0 KiB<br />   swap: 0<br />  -cmdline: /sbin/agetty tty3 38400<br />  /lib/ld-2.12.2.so:<br />   -shared-with: rpcbind, _plutorun, redshift, dbus-launch, acpid, ...<br />   private: 8.0 KiB<br />   shared: 104.0 KiB<br />   swap: 0<br />  /lib/libc-2.12.2.so:<br />   -shared-with: rpcbind, _plutorun, redshift, dbus-launch, acpid, ...<br />   private: 12.0 KiB<br />   shared: 548.0 KiB<br />   swap: 0<br />  ...<br />  /sbin/agetty:<br />   -shared-with: agetty<br />   private: 4.0 KiB<br />   shared: 24.0 KiB<br />   swap: 0<br />  /usr/lib/locale/locale-archive:<br />   -shared-with: firefox, redshift, tee, sleep, ypbind, pulseaudio [updated], ...<br />   private: 0<br />   shared: 8.0 KiB<br />   swap: 0<br />  [anon]:<br />   private: 20.0 KiB<br />   shared: 0<br />   swap: 0<br />  [heap]:<br />   private: 8.0 KiB<br />   shared: 0<br />   swap: 0<br />  [stack]:<br />   private: 24.0 KiB<br />   shared: 0<br />   swap: 0<br />  [vdso]:<br />   private: 0<br />   shared: 0<br />   swap: 0<br /><br /> 7608:<br />  -stats:<br />   private: 84.0 KiB<br />   shared: 712.0 KiB<br />   swap: 0<br />  -cmdline: /sbin/agetty tty4 38400<br />  ...<br /><br /> 7609:<br />  -stats:<br />   private: 84.0 KiB<br />   shared: 712.0 KiB<br />   swap: 0<br />  -cmdline: /sbin/agetty tty5 38400<br />  ...<br /><br /></pre><p>So it's obvious that there are 3 agetty processes, which ps will report as 796 KiB RSS:</p><pre>root 7606 0.0 0.0 3924 796 tty3 Ss+ 23:05 0:00 /sbin/agetty tty3 38400<br />root 7608 0.0 0.0 3924 796 tty4 Ss+ 23:05 0:00 /sbin/agetty tty4 38400<br />root 7609 0.0 0.0 3924 796 tty5 Ss+ 23:05 0:00 /sbin/agetty tty5 38400<br /></pre><p>Each of which, in fact, consumes only 84 KiB of RAM, with 24 KiB more shared between all agettys as /sbin/agetty binary, rest of stuff like ld and libc is shared system-wide (shared-with list contains pretty much every process in the system), so it won't be freed by killing agetty and starting 10 more of them will consume ~1 MiB, not ~10 MiB, as "ps" output might suggest.<br />"top" will show ~3M of "swap" (same with "SZ" in ps) for each agetty, which is also obviously untrue.</p><p>More machine-friendly (flat) output might remind of sysctl:</p><pre>agetty.-stats.private: 252.0 KiB<br />agetty.-stats.shared: 712.0 KiB<br />agetty.-stats.swap: 0<br />agetty.7606.-stats.private: 84.0 KiB<br />agetty.7606.-stats.shared: 712.0 KiB<br />agetty.7606.-stats.swap: 0<br />agetty.7606.-cmdline: /sbin/agetty tty3 38400<br />agetty.7606.'/lib/ld-2.12.2.so'.-shared-with: ...<br />agetty.7606.'/lib/ld-2.12.2.so'.private: 8.0 KiB<br />agetty.7606.'/lib/ld-2.12.2.so'.shared: 104.0 KiB<br />agetty.7606.'/lib/ld-2.12.2.so'.swap: 0<br />agetty.7606.'/lib/libc-2.12.2.so'.-shared-with: ...<br />...<br /><br /></pre><p><a title="Script source (python)" href="http://fraggod.net/static/code/ps_mem_details.py">Script</a>. No dependencies needed, apart from python 2.7 or 3.X (works with both w/o conversion).</p><p>Some optional parameters are supported:</p><pre>usage: ps_mem_details.py [-h] [-p] [-s] [-n MIN_VAL] [-f] [--debug] [name]<br /><br />Detailed process memory usage accounting tool.<br /><br />positional arguments:<br /> name         String to look for in process cmd/binary.<br /><br />optional arguments:<br /> -h, --help      show this help message and exit<br /> -p, --private     Show only private memory leaks.<br /> -s, --swap      Show only swapped-out stuff.<br /> -n MIN_VAL, --min-val MIN_VAL<br />            Minimal (non-inclusive) value for tracked parameter<br />            (KiB, see --swap, --private, default: 0).<br /> -f, --flat      Flat output.<br /> --debug        Verbose operation mode.<br /><br /></pre><p>For example, to find what hogs more than 500K swap in the system:</p><pre># ps_mem_details.py --flat --swap -n 500<br />memcached.-stats.private: 28.4 MiB<br />memcached.-stats.shared: 588.0 KiB<br />memcached.-stats.swap: 1.5 MiB<br />memcached.927.-cmdline: /usr/bin/memcached -p 11211 -l 127.0.0.1<br />memcached.927.[anon].private: 28.0 MiB<br />memcached.927.[anon].shared: 0<br />memcached.927.[anon].swap: 1.5 MiB<br />squid.-stats.private: 130.9 MiB<br />squid.-stats.shared: 1.2 MiB<br />squid.-stats.swap: 668.0 KiB<br />squid.1334.-cmdline: /usr/sbin/squid -NYC<br />squid.1334.[heap].private: 128.0 MiB<br />squid.1334.[heap].shared: 0<br />squid.1334.[heap].swap: 660.0 KiB<br />udevd.-stats.private: 368.0 KiB<br />udevd.-stats.shared: 796.0 KiB<br />udevd.-stats.swap: 748.0 KiB<br /></pre><p>...or what eats more than 20K in agetty pids (should be useful to see which .so or binary "leaks" in a process):</p><pre># ps_mem_details.py --private --flat -n 20 agetty<br />agetty.-stats.private: 252.0 KiB<br />agetty.-stats.shared: 712.0 KiB<br />agetty.-stats.swap: 0<br />agetty.7606.-stats.private: 84.0 KiB<br />agetty.7606.-stats.shared: 712.0 KiB<br />agetty.7606.-stats.swap: 0<br />agetty.7606.-cmdline: /sbin/agetty tty3 38400<br />agetty.7606.[stack].private: 24.0 KiB<br />agetty.7606.[stack].shared: 0<br />agetty.7606.[stack].swap: 0<br />agetty.7608.-stats.private: 84.0 KiB<br />agetty.7608.-stats.shared: 712.0 KiB<br />agetty.7608.-stats.swap: 0<br />agetty.7608.-cmdline: /sbin/agetty tty4 38400<br />agetty.7608.[stack].private: 24.0 KiB<br />agetty.7608.[stack].shared: 0<br />agetty.7608.[stack].swap: 0<br />agetty.7609.-stats.private: 84.0 KiB<br />agetty.7609.-stats.shared: 712.0 KiB<br />agetty.7609.-stats.swap: 0<br />agetty.7609.-cmdline: /sbin/agetty tty5 38400<br />agetty.7609.[stack].private: 24.0 KiB<br />agetty.7609.[stack].shared: 0<br />agetty.7609.[stack].swap: 0<br /></pre><p></p>-=|||=-<p>Two questions:</p><ul>
<li>How to tell which pids (or groups of forks) eat most swap right now?</li>
<li>How much RAM one apache/php/whatever really consumes?</li>
</ul>
<p>Somehow people keep pointing me at "top" and "ps" tools to do this sort of things, but there's an obvious problem:</p><pre name="code" class="c">#include &lt;stdlib.h&gt;<br />#include &lt;unistd.h&gt;
<br />#define G 1024*1024*1024<br /><br />int main (void) {<br />	(void *) malloc(2 * G);<br />	sleep(10);<br />	return 0;<br />}<br /></pre><p>This code will immediately float to 1st position in top, sorted by "swap" (F p &lt;return&gt;), showing 2G even with no swap in the system.</p><p>Second question/issue is also common but somehow not universally recognized, which is kinda obvious when scared admins (or whoever happen to ssh into web backend machine) see N pids of something, summing up to more than total amount of RAM in the system, like 50 httpd processes 50M each.<br />It gets even worse when tools like "atop" helpfully aggregate the numbers ("atop -p"), showing that there are 6 sphinx processes, eating 15G on a machine with 4-6G physical RAM + 4-8G swap, causing local panic and mayhem.<br />The answer is, of course, that sphinx, apache and pretty much anything using worker processes share a lot of memory pages between their processes, and not just because of shared objects like libc.</p><p>Guess it's just general ignorance of how memory works in linux (or other unix-os'es) of those who never had to write a fork() or deal with malloc's in C, which kinda make lots of these concepts look fairly trivial.</p><p>So, mostly out of curiosity than the real need, decided to find a way to answer these questions.<br />proc(5) reveals this data more-or-less via "maps" / "smaps" files, but that needs some post-processing to give per-pid numbers.<br />Closest tools I was able to find were pmap from procps package and <a title="ps_mem.py script" href="http://www.pixelbeat.org/scripts/ps_mem.py">ps_mem.py script</a> from coreutils maintainer. Former seem to give only mapped memory region sizes, latter cleverly shows shared memory divided by a number of similar processes, omitting per-process numbers and swap.<br />Oh, and of course there are glorious valgrind and gdb, but both seem to be active debugging tools, not much suitable for normal day-to-day operation conditions and a bit too complex for the task.</p><p>So I though I'd write my own tool for the job to put the matter at rest once and for all, and so I can later point people at it and just say "see?" (although I bet it'll never be that simple).</p><p>Idea is to group similar processes (by cmd) and show details for each one, like this:</p><pre>agetty:<br /> -stats:<br />  private: 252.0 KiB<br />  shared: 712.0 KiB<br />  swap: 0<br /><br /> 7606:<br />  -stats:<br />   private: 84.0 KiB<br />   shared: 712.0 KiB<br />   swap: 0<br />  -cmdline: /sbin/agetty tty3 38400<br />  /lib/ld-2.12.2.so:<br />   -shared-with: rpcbind, _plutorun, redshift, dbus-launch, acpid, ...<br />   private: 8.0 KiB<br />   shared: 104.0 KiB<br />   swap: 0<br />  /lib/libc-2.12.2.so:<br />   -shared-with: rpcbind, _plutorun, redshift, dbus-launch, acpid, ...<br />   private: 12.0 KiB<br />   shared: 548.0 KiB<br />   swap: 0<br />  ...<br />  /sbin/agetty:<br />   -shared-with: agetty<br />   private: 4.0 KiB<br />   shared: 24.0 KiB<br />   swap: 0<br />  /usr/lib/locale/locale-archive:<br />   -shared-with: firefox, redshift, tee, sleep, ypbind, pulseaudio [updated], ...<br />   private: 0<br />   shared: 8.0 KiB<br />   swap: 0<br />  [anon]:<br />   private: 20.0 KiB<br />   shared: 0<br />   swap: 0<br />  [heap]:<br />   private: 8.0 KiB<br />   shared: 0<br />   swap: 0<br />  [stack]:<br />   private: 24.0 KiB<br />   shared: 0<br />   swap: 0<br />  [vdso]:<br />   private: 0<br />   shared: 0<br />   swap: 0<br /><br /> 7608:<br />  -stats:<br />   private: 84.0 KiB<br />   shared: 712.0 KiB<br />   swap: 0<br />  -cmdline: /sbin/agetty tty4 38400<br />  ...<br /><br /> 7609:<br />  -stats:<br />   private: 84.0 KiB<br />   shared: 712.0 KiB<br />   swap: 0<br />  -cmdline: /sbin/agetty tty5 38400<br />  ...<br /><br /></pre><p>So it's obvious that there are 3 agetty processes, which ps will report as 796 KiB RSS:</p><pre>root 7606 0.0 0.0 3924 796 tty3 Ss+ 23:05 0:00 /sbin/agetty tty3 38400<br />root 7608 0.0 0.0 3924 796 tty4 Ss+ 23:05 0:00 /sbin/agetty tty4 38400<br />root 7609 0.0 0.0 3924 796 tty5 Ss+ 23:05 0:00 /sbin/agetty tty5 38400<br /></pre><p>Each of which, in fact, consumes only 84 KiB of RAM, with 24 KiB more shared between all agettys as /sbin/agetty binary, rest of stuff like ld and libc is shared system-wide (shared-with list contains pretty much every process in the system), so it won't be freed by killing agetty and starting 10 more of them will consume ~1 MiB, not ~10 MiB, as "ps" output might suggest.<br />"top" will show ~3M of "swap" (same with "SZ" in ps) for each agetty, which is also obviously untrue.</p><p>More machine-friendly (flat) output might remind of sysctl:</p><pre>agetty.-stats.private: 252.0 KiB<br />agetty.-stats.shared: 712.0 KiB<br />agetty.-stats.swap: 0<br />agetty.7606.-stats.private: 84.0 KiB<br />agetty.7606.-stats.shared: 712.0 KiB<br />agetty.7606.-stats.swap: 0<br />agetty.7606.-cmdline: /sbin/agetty tty3 38400<br />agetty.7606.'/lib/ld-2.12.2.so'.-shared-with: ...<br />agetty.7606.'/lib/ld-2.12.2.so'.private: 8.0 KiB<br />agetty.7606.'/lib/ld-2.12.2.so'.shared: 104.0 KiB<br />agetty.7606.'/lib/ld-2.12.2.so'.swap: 0<br />agetty.7606.'/lib/libc-2.12.2.so'.-shared-with: ...<br />...<br /><br /></pre><p><a title="Script source (python)" href="http://fraggod.net/static/code/ps_mem_details.py">Script</a>. No dependencies needed, apart from python 2.7 or 3.X (works with both w/o conversion).</p><p>Some optional parameters are supported:</p><pre>usage: ps_mem_details.py [-h] [-p] [-s] [-n MIN_VAL] [-f] [--debug] [name]<br /><br />Detailed process memory usage accounting tool.<br /><br />positional arguments:<br /> name         String to look for in process cmd/binary.<br /><br />optional arguments:<br /> -h, --help      show this help message and exit<br /> -p, --private     Show only private memory leaks.<br /> -s, --swap      Show only swapped-out stuff.<br /> -n MIN_VAL, --min-val MIN_VAL<br />            Minimal (non-inclusive) value for tracked parameter<br />            (KiB, see --swap, --private, default: 0).<br /> -f, --flat      Flat output.<br /> --debug        Verbose operation mode.<br /><br /></pre><p>For example, to find what hogs more than 500K swap in the system:</p><pre># ps_mem_details.py --flat --swap -n 500<br />memcached.-stats.private: 28.4 MiB<br />memcached.-stats.shared: 588.0 KiB<br />memcached.-stats.swap: 1.5 MiB<br />memcached.927.-cmdline: /usr/bin/memcached -p 11211 -l 127.0.0.1<br />memcached.927.[anon].private: 28.0 MiB<br />memcached.927.[anon].shared: 0<br />memcached.927.[anon].swap: 1.5 MiB<br />squid.-stats.private: 130.9 MiB<br />squid.-stats.shared: 1.2 MiB<br />squid.-stats.swap: 668.0 KiB<br />squid.1334.-cmdline: /usr/sbin/squid -NYC<br />squid.1334.[heap].private: 128.0 MiB<br />squid.1334.[heap].shared: 0<br />squid.1334.[heap].swap: 660.0 KiB<br />udevd.-stats.private: 368.0 KiB<br />udevd.-stats.shared: 796.0 KiB<br />udevd.-stats.swap: 748.0 KiB<br /></pre><p>...or what eats more than 20K in agetty pids (should be useful to see which .so or binary "leaks" in a process):</p><pre># ps_mem_details.py --private --flat -n 20 agetty<br />agetty.-stats.private: 252.0 KiB<br />agetty.-stats.shared: 712.0 KiB<br />agetty.-stats.swap: 0<br />agetty.7606.-stats.private: 84.0 KiB<br />agetty.7606.-stats.shared: 712.0 KiB<br />agetty.7606.-stats.swap: 0<br />agetty.7606.-cmdline: /sbin/agetty tty3 38400<br />agetty.7606.[stack].private: 24.0 KiB<br />agetty.7606.[stack].shared: 0<br />agetty.7606.[stack].swap: 0<br />agetty.7608.-stats.private: 84.0 KiB<br />agetty.7608.-stats.shared: 712.0 KiB<br />agetty.7608.-stats.swap: 0<br />agetty.7608.-cmdline: /sbin/agetty tty4 38400<br />agetty.7608.[stack].private: 24.0 KiB<br />agetty.7608.[stack].shared: 0<br />agetty.7608.[stack].swap: 0<br />agetty.7609.-stats.private: 84.0 KiB<br />agetty.7609.-stats.shared: 712.0 KiB<br />agetty.7609.-stats.swap: 0<br />agetty.7609.-cmdline: /sbin/agetty tty5 38400<br />agetty.7609.[stack].private: 24.0 KiB<br />agetty.7609.[stack].shared: 0<br />agetty.7609.[stack].swap: 0<br /></pre><p></p>-=||||||||||=-2011/10/dm-crypt-password-caching-between-dracut-and-systemd-systemd-password-agent-=|||=-2011-10-23T08:01:39-=|||=-dm-crypt password caching between dracut and systemd, systemd password agent-=|||=-[u'Python', u'Systemd', u'SysAdmin', u'Unix', u'Caching', u'Encryption']-=|||=-<p>Up until now I've used lvm on top of single full-disk dm-crypt partition.<br />It seems easiest to work with - no need to decrypt individual lv's, no confusion between what's encrypted (everything but /boot!) and what's not, etc.<br />Main problem with it though is that it's harder to have non-encrypted parts, everything is encrypted with the same keys (unless there're several dm-crypt layers) and it's bad for SSD - dm-crypt still (as of 3.0) doesn't pass any TRIM requests through, leading to nasty <a title="Write Amplification (wiki)" href="http://en.wikipedia.org/wiki/Write_amplification">write amplification effect</a>, even more so with full disk given to dm-crypt+lvm.</p><p>While there's hope that SSD issues <a title="didn't bother to verify the info myself" href="http://superuser.com/questions/302710/trim-support-via-dm-crypt-device-mapper#318847">will be kinda-solved</a> (with an optional security trade-off) in 3.1, it's still much easier to keep different distros or some decrypted-when-needed partitions with dm-crypt after lvm, so I've decided to go with the latter for new 120G SSD.<br />Also, such scheme allows to re-create encrypted lvs, issuing TRIM for the old ones, thus recycling the blocks even w/o support for this in dm-crypt.</p><p>In same as with <a title="CIRD thing" href="http://blog.fraggod.net/2010/4/LUKS-dm-crypt-rootfs-without-password-via-smartcard">previous initramfs</a>, I've had simple "openct" module (udev there makes it even easier) in <a title="Dracut project" href="http://sourceforge.net/apps/trac/dracut/wiki">dracut</a> to find inserted smartcard and use it to obtain encryption key, which is used once to decrypt the only partition on which everything resides.<br />Since the only goal of dracut is to find root and get-the-hell-outta-the-way, it won't even try to decrypt all the /var and /home stuff without serious ideological changes.<br />The problem is actually solved in generic distros by <a title="Plymouth project" href="http://www.freedesktop.org/wiki/Software/Plymouth">plymouth</a>, which gets the password(s), caches it, and provides it to dracut and systemd (or whatever comes as the real "init"). I don't need splash, and actually hate it for hiding all the info that scrolls in it's place, so plymouth is a no-go for me.</p><p>Having a hack to obtain and cache key for dracut by non-conventional means anyway, I just needed to pass it further to systemd, and since they share common /run tmpfs these days, it basically means not to rm it in dracut after use.<br />Luckily, system-wide password handling mechanism in systemd is <a title="systemd password agents interface docs" href="http://www.freedesktop.org/wiki/Software/systemd/PasswordAgents">well-documented</a> and easily extensible beyond plymouth and default console prompt.</p><p>So whole key management in my system goes like this now:</p><ul>
<li>dracut.cmdline: create udev rule to generate key.</li>
<li>dracut.udev.openct: find smartcard, run rule to generate and cache key in /run/initramfs.</li>
<li>dracut.udev.crypt: check for cached key or prompt for it (caching result), decrypt root, run systemd.</li>
<li>systemd: start post-dracut-crypt.path unit to monitor /run/systemd/ask-password for password prompts, along with default .path units for fallback prompts via wall/console.</li>
<li>systemd.udev: discover encrypted devices, create key requests.</li>
<li>systemd.post-dracut-crypt.path: start post-dracut-crypt.service to read cached passwords from /run/initramfs and use these to satisfy requests.</li>
<li>systemd.post-dracut-crypt-cleanup.service (after local-fs.target is activated): stop post-dracut-crypt.service, flush caches, generate new one-time keys for decrypted partitions.</li>
</ul>
<p>End result is passwordless boot with this new layout, which seem to be only possible to spoof by getting root during that process somehow, with altering unencrypted /boot to run some extra code and revert it back being the most obvious possibility.<br />It's kinda weird that there doesn't seem to be any caching in place already, surely not everyone with dm-crypt are using plymouth?</p><p>Most complicated piece here is probably the password agent (in python), which can actually could've been simplier if I haven't followed the <a title="systemd password agent implementation guidelines" href="http://www.freedesktop.org/wiki/Software/systemd/PasswordAgents">proper guidelines</a> and thought a bit around them.<br />For example, whole inotify handling thing (I've used it via <a title="python ctypes ffi" href="http://docs.python.org/library/ctypes.html">ctypes</a>) can be dropped with .path unit with DirectoryNotEmpty= activation condition - it's there already, <a title="PolicyKit project" href="http://www.freedesktop.org/wiki/PolicyKit">PolicyKit</a> authorization just isn't working at such an early stage, there doesn't seem to be much need to check request validity since sending replies to sockets is racy anyway, etc<br />Still, a good excercise.</p><p><a title="systemd password agent implementation in python" href="http://fraggod.net/static/code/systemd_password_agent/password_agent.py">Python password agent for systemd</a>. <a title="Unit files to start agent on early boot and flush caches afterwards" href="http://fraggod.net/static/code/systemd_password_agent/">Unit files</a> to start and stop it on demand.    </p>-=|||=-<p>Up until now I've used lvm on top of single full-disk dm-crypt partition.<br />It seems easiest to work with - no need to decrypt individual lv's, no confusion between what's encrypted (everything but /boot!) and what's not, etc.<br />Main problem with it though is that it's harder to have non-encrypted parts, everything is encrypted with the same keys (unless there're several dm-crypt layers) and it's bad for SSD - dm-crypt still (as of 3.0) doesn't pass any TRIM requests through, leading to nasty <a title="Write Amplification (wiki)" href="http://en.wikipedia.org/wiki/Write_amplification">write amplification effect</a>, even more so with full disk given to dm-crypt+lvm.</p><p>While there's hope that SSD issues <a title="didn't bother to verify the info myself" href="http://superuser.com/questions/302710/trim-support-via-dm-crypt-device-mapper#318847">will be kinda-solved</a> (with an optional security trade-off) in 3.1, it's still much easier to keep different distros or some decrypted-when-needed partitions with dm-crypt after lvm, so I've decided to go with the latter for new 120G SSD.<br />Also, such scheme allows to re-create encrypted lvs, issuing TRIM for the old ones, thus recycling the blocks even w/o support for this in dm-crypt.</p><p>In same as with <a title="CIRD thing" href="http://blog.fraggod.net/2010/4/LUKS-dm-crypt-rootfs-without-password-via-smartcard">previous initramfs</a>, I've had simple "openct" module (udev there makes it even easier) in <a title="Dracut project" href="http://sourceforge.net/apps/trac/dracut/wiki">dracut</a> to find inserted smartcard and use it to obtain encryption key, which is used once to decrypt the only partition on which everything resides.<br />Since the only goal of dracut is to find root and get-the-hell-outta-the-way, it won't even try to decrypt all the /var and /home stuff without serious ideological changes.<br />The problem is actually solved in generic distros by <a title="Plymouth project" href="http://www.freedesktop.org/wiki/Software/Plymouth">plymouth</a>, which gets the password(s), caches it, and provides it to dracut and systemd (or whatever comes as the real "init"). I don't need splash, and actually hate it for hiding all the info that scrolls in it's place, so plymouth is a no-go for me.</p><p>Having a hack to obtain and cache key for dracut by non-conventional means anyway, I just needed to pass it further to systemd, and since they share common /run tmpfs these days, it basically means not to rm it in dracut after use.<br />Luckily, system-wide password handling mechanism in systemd is <a title="systemd password agents interface docs" href="http://www.freedesktop.org/wiki/Software/systemd/PasswordAgents">well-documented</a> and easily extensible beyond plymouth and default console prompt.</p><p>So whole key management in my system goes like this now:</p><ul>
<li>dracut.cmdline: create udev rule to generate key.</li>
<li>dracut.udev.openct: find smartcard, run rule to generate and cache key in /run/initramfs.</li>
<li>dracut.udev.crypt: check for cached key or prompt for it (caching result), decrypt root, run systemd.</li>
<li>systemd: start post-dracut-crypt.path unit to monitor /run/systemd/ask-password for password prompts, along with default .path units for fallback prompts via wall/console.</li>
<li>systemd.udev: discover encrypted devices, create key requests.</li>
<li>systemd.post-dracut-crypt.path: start post-dracut-crypt.service to read cached passwords from /run/initramfs and use these to satisfy requests.</li>
<li>systemd.post-dracut-crypt-cleanup.service (after local-fs.target is activated): stop post-dracut-crypt.service, flush caches, generate new one-time keys for decrypted partitions.</li>
</ul>
<p>End result is passwordless boot with this new layout, which seem to be only possible to spoof by getting root during that process somehow, with altering unencrypted /boot to run some extra code and revert it back being the most obvious possibility.<br />It's kinda weird that there doesn't seem to be any caching in place already, surely not everyone with dm-crypt are using plymouth?</p><p>Most complicated piece here is probably the password agent (in python), which can actually could've been simplier if I haven't followed the <a title="systemd password agent implementation guidelines" href="http://www.freedesktop.org/wiki/Software/systemd/PasswordAgents">proper guidelines</a> and thought a bit around them.<br />For example, whole inotify handling thing (I've used it via <a title="python ctypes ffi" href="http://docs.python.org/library/ctypes.html">ctypes</a>) can be dropped with .path unit with DirectoryNotEmpty= activation condition - it's there already, <a title="PolicyKit project" href="http://www.freedesktop.org/wiki/PolicyKit">PolicyKit</a> authorization just isn't working at such an early stage, there doesn't seem to be much need to check request validity since sending replies to sockets is racy anyway, etc<br />Still, a good excercise.</p><p><a title="systemd password agent implementation in python" href="http://fraggod.net/static/code/systemd_password_agent/password_agent.py">Python password agent for systemd</a>. <a title="Unit files to start agent on early boot and flush caches afterwards" href="http://fraggod.net/static/code/systemd_password_agent/">Unit files</a> to start and stop it on demand.    </p>-=||||||||||=-2011/11/Running-stuff-like-firefox-flash-and-skype-with-apparmor-=|||=-2011-11-12T09:08:48-=|||=-Running stuff like firefox, flash and skype with apparmor-=|||=-[u'Desktop', u'Unix']-=|||=-<p>Should've done it a long time ago, actually.<br />I was totally sure it'd be much harder task, but then recently I've had some spare time and decided to do something about this binary crap, and looking for possible solutions stumbled upon <a title="AppArmor LSM" href="http://apparmor.net/">apparmor</a>.</p><p>A while ago I've used <a title="SELinux LSM" href="http://selinuxproject.org/">SELinux</a> (which was the reason why I thought it'd have to be hard) and kinda considered LSM-based security as kind of heavy-handed no-nonsense shit you chose NOT to deal with if you have such choice, but apparmor totally proved this to be a silly misconception, which I'm insanely happy about.<br />With apparmor, it's just one file with a set of permissions, which can be loaded/enforced/removed at runtime, no xattrs (and associated maintenance burden) or huge and complicated policies like SELinux has. For good whole-system security SELinux still seem to be a better approach, but not for confining a few crappy apps on a otherwise general system.<br />On top of that, it's also trivially easy to install on a general system - only kernel LSM and one userspace package needed.</p><p>Case in point - skype apparmor profile, which doesn't allow it to access anything but ~/.Skype, /opt/skype and a few other system-wide things:</p><pre>#include &lt;tunables/global&gt;<br /><br />/usr/bin/skype {<br /> #include &lt;abstractions/base&gt;<br /> #include &lt;abstractions/user-tmp&gt;<br /> #include &lt;abstractions/pulse&gt;<br /> #include &lt;abstractions/nameservice&gt;<br /> #include &lt;abstractions/ssl_certs&gt;<br /> #include &lt;abstractions/fonts&gt;<br /> #include &lt;abstractions/X&gt;<br /> #include &lt;abstractions/freedesktop.org&gt;<br /> #include &lt;abstractions/kde&gt;<br /> #include &lt;abstractions/site/base&gt;<br /> #include &lt;abstractions/site/de&gt;<br /><br /> /usr/bin/skype mr,<br /> /opt/skype/skype pix,<br /> /opt/skype/** mr,<br /> /usr/share/fonts/X11/** m,<br /><br /> @{PROC}/*/net/arp r,<br /> @{PROC}/sys/kernel/ostype r,<br /> @{PROC}/sys/kernel/osrelease r,<br /><br /> /dev/ r,<br /> /dev/tty rw,<br /> /dev/pts/* rw,<br /> /dev/video* mrw,<br /><br /> @{HOME}/.Skype/ rw,<br /> @{HOME}/.Skype/** krw,<br /><br /> deny @{HOME}/.mozilla/ r, # no idea what it tries to get there<br /> deny @{PROC}/[0-9]*/fd/ r,<br /> deny @{PROC}/[0-9]*/task/ r,<br /> deny @{PROC}/[0-9]*/task/** r,<br />}<br /><br /></pre><p>"deny" lines here are just to supress audit warnings about this paths, everything is denied by default, unless explicitly allowed.</p><p>Compared to "default" linux DAC-only "as user" confinement, where it has access to all your documents, activities, smartcard, gpg keys and processes, ssh keys and sessions, etc - it's a huge improvement.</p><p>Even more useful confinement is firefox and it's plugin-container process (which can - and does, in my configuration - have separate profile), where known-to-be-extremely-exploitable adobe flash player runs.<br />Before apparmor, I mostly relied on FlashBlock extension to keep Flash in check somehow, but at some point I noted that plugin-container with libflashplayer.so seem to be running regardless of FlashBlock and whether flash is displayed on pages or not. I don't know if it's just a warm-start, check-run or something, but still looks like a possible hole.</p><p>Aforementioned (among others) profiles <a title="my set of AppArmor profiles" href="http://fraggod.net/static/code/apparmor/">can be found here</a>.<br />I'm actually quite surprised that I failed to find functional profiles for common apps like firefox and pulseaudio on the internets, aside from some blog posts like this one.<br />In theory, Ubuntu and SUSE should have these, since apparmor is developed and deployed there by default (afaik), so maybe google just haven't picked these files up in the package manifests, and all I needed was to go over them by hand. Not sure if it was much faster or more productive than writing them myself though.</p>-=|||=-<p>Should've done it a long time ago, actually.<br />I was totally sure it'd be much harder task, but then recently I've had some spare time and decided to do something about this binary crap, and looking for possible solutions stumbled upon <a title="AppArmor LSM" href="http://apparmor.net/">apparmor</a>.</p><p>A while ago I've used <a title="SELinux LSM" href="http://selinuxproject.org/">SELinux</a> (which was the reason why I thought it'd have to be hard) and kinda considered LSM-based security as kind of heavy-handed no-nonsense shit you chose NOT to deal with if you have such choice, but apparmor totally proved this to be a silly misconception, which I'm insanely happy about.<br />With apparmor, it's just one file with a set of permissions, which can be loaded/enforced/removed at runtime, no xattrs (and associated maintenance burden) or huge and complicated policies like SELinux has. For good whole-system security SELinux still seem to be a better approach, but not for confining a few crappy apps on a otherwise general system.<br />On top of that, it's also trivially easy to install on a general system - only kernel LSM and one userspace package needed.</p><p>Case in point - skype apparmor profile, which doesn't allow it to access anything but ~/.Skype, /opt/skype and a few other system-wide things:</p><pre>#include &lt;tunables/global&gt;<br /><br />/usr/bin/skype {<br /> #include &lt;abstractions/base&gt;<br /> #include &lt;abstractions/user-tmp&gt;<br /> #include &lt;abstractions/pulse&gt;<br /> #include &lt;abstractions/nameservice&gt;<br /> #include &lt;abstractions/ssl_certs&gt;<br /> #include &lt;abstractions/fonts&gt;<br /> #include &lt;abstractions/X&gt;<br /> #include &lt;abstractions/freedesktop.org&gt;<br /> #include &lt;abstractions/kde&gt;<br /> #include &lt;abstractions/site/base&gt;<br /> #include &lt;abstractions/site/de&gt;<br /><br /> /usr/bin/skype mr,<br /> /opt/skype/skype pix,<br /> /opt/skype/** mr,<br /> /usr/share/fonts/X11/** m,<br /><br /> @{PROC}/*/net/arp r,<br /> @{PROC}/sys/kernel/ostype r,<br /> @{PROC}/sys/kernel/osrelease r,<br /><br /> /dev/ r,<br /> /dev/tty rw,<br /> /dev/pts/* rw,<br /> /dev/video* mrw,<br /><br /> @{HOME}/.Skype/ rw,<br /> @{HOME}/.Skype/** krw,<br /><br /> deny @{HOME}/.mozilla/ r, # no idea what it tries to get there<br /> deny @{PROC}/[0-9]*/fd/ r,<br /> deny @{PROC}/[0-9]*/task/ r,<br /> deny @{PROC}/[0-9]*/task/** r,<br />}<br /><br /></pre><p>"deny" lines here are just to supress audit warnings about this paths, everything is denied by default, unless explicitly allowed.</p><p>Compared to "default" linux DAC-only "as user" confinement, where it has access to all your documents, activities, smartcard, gpg keys and processes, ssh keys and sessions, etc - it's a huge improvement.</p><p>Even more useful confinement is firefox and it's plugin-container process (which can - and does, in my configuration - have separate profile), where known-to-be-extremely-exploitable adobe flash player runs.<br />Before apparmor, I mostly relied on FlashBlock extension to keep Flash in check somehow, but at some point I noted that plugin-container with libflashplayer.so seem to be running regardless of FlashBlock and whether flash is displayed on pages or not. I don't know if it's just a warm-start, check-run or something, but still looks like a possible hole.</p><p>Aforementioned (among others) profiles <a title="my set of AppArmor profiles" href="http://fraggod.net/static/code/apparmor/">can be found here</a>.<br />I'm actually quite surprised that I failed to find functional profiles for common apps like firefox and pulseaudio on the internets, aside from some blog posts like this one.<br />In theory, Ubuntu and SUSE should have these, since apparmor is developed and deployed there by default (afaik), so maybe google just haven't picked these files up in the package manifests, and all I needed was to go over them by hand. Not sure if it was much faster or more productive than writing them myself though.</p>-=||||||||||=-2012/2/On-github-as-well-now-=|||=-2012-02-03T20:57:04-=|||=-On github as well now-=|||=-[u'SCM', u'Web', u'Social']-=|||=-<p>Following another hiatus from a day job, I finally have enough spare time to read some of the internets and do something about them.</p><p>For quite a while I had lots of quite small scripts and projects, which I kinda documented here (and on the site pages before that).<br />I always kept them in some kind of scm - be it system-wide repo for configuration files, ~/.cFG repo for DE and misc user configuration and ~/bin scripts, or ~/hatch repo I keep for misc stuff, but as their number grows, as well as the size and complexity, I think maybe some of this stuff deserves some kind of repo, maybe attention, and best-case scenario, will even be useful to someone but me.</p><p>So I thought to gradually push all this stuff out to github and/or bitbucket (still need to learn or at least look at hg for that!).<br />github being the most obvious and easiest choice, just created a few repos there and started the migration. More to come.</p><p>Still don't really trust a silo like github to keep anything reliably (besides it lags like hell here, especially compared to local servers I'm kinda used to), so need to devise some mirroring scheme asap.<br />Initial idea is to take some flexible tool (hg seem to be ideal, being python and scm proper) and build a hooks into local repos to push stuff out to mirrors from there, ideally both bitbucket and github, also exploiting their metadata APIs to fetch stuff like tickets/issues and commit history of these into separate repo branch as well.</p><p>Effort should be somewhat justified by the fact that such repos will be geo-distributed backups, shareable links and I can learn more SCM internals by the way.</p><p>For now - <a title="mk-fg.github, yay" href="https://github.com/mk-fg/">me on github</a>.</p>-=|||=-<p>Following another hiatus from a day job, I finally have enough spare time to read some of the internets and do something about them.</p><p>For quite a while I had lots of quite small scripts and projects, which I kinda documented here (and on the site pages before that).<br />I always kept them in some kind of scm - be it system-wide repo for configuration files, ~/.cFG repo for DE and misc user configuration and ~/bin scripts, or ~/hatch repo I keep for misc stuff, but as their number grows, as well as the size and complexity, I think maybe some of this stuff deserves some kind of repo, maybe attention, and best-case scenario, will even be useful to someone but me.</p><p>So I thought to gradually push all this stuff out to github and/or bitbucket (still need to learn or at least look at hg for that!).<br />github being the most obvious and easiest choice, just created a few repos there and started the migration. More to come.</p><p>Still don't really trust a silo like github to keep anything reliably (besides it lags like hell here, especially compared to local servers I'm kinda used to), so need to devise some mirroring scheme asap.<br />Initial idea is to take some flexible tool (hg seem to be ideal, being python and scm proper) and build a hooks into local repos to push stuff out to mirrors from there, ideally both bitbucket and github, also exploiting their metadata APIs to fetch stuff like tickets/issues and commit history of these into separate repo branch as well.</p><p>Effort should be somewhat justified by the fact that such repos will be geo-distributed backups, shareable links and I can learn more SCM internals by the way.</p><p>For now - <a title="mk-fg.github, yay" href="https://github.com/mk-fg/">me on github</a>.</p>-=||||||||||=-2012/2/Phasing-out-fossil-completely-=|||=-2012-02-07T07:15:31-=|||=-Phasing out fossil completely-=|||=-[u'SCM', u'Fossil']-=|||=-<p>Having used git excessively for the last few days decided to ditch <a title="fossil scm project" href="http://www.fossil-scm.org/">fossil scm</a> at last.<br />All the stuff will be <a title="self-hosted git stuff" href="http://fraggod.net/code/git">in git</a> and mirorred on the <a title="my github page" href="https://github.com/mk-fg/">github</a> (maybe later <a title="my bitbucket page" href="https://bitbucket.org/mk_fg">on bittbucket</a> as well).</p><p>Will probably re-import meta stuff (issues, wikis) from there into the main tree, but still can't find nice-enough tool for that.<br />Closest thing seem to be <a title="Artemis project" href="http://www.mrzv.org/software/artemis/">Artemis</a>, but it's for mercurial, so I'll probably need to port it to git first, shouldn't be too hard.</p><p>Also, I'm torn at this point between the thoughts along the lines "selection of modern DVCS spoil us" against "damn, why they there is no clear popular + works-for-everything thing", but it's probably normal, as I have (or had) similar thoughts about lot of technologies.</p>-=|||=-<p>Having used git excessively for the last few days decided to ditch <a title="fossil scm project" href="http://www.fossil-scm.org/">fossil scm</a> at last.<br />All the stuff will be <a title="self-hosted git stuff" href="http://fraggod.net/code/git">in git</a> and mirorred on the <a title="my github page" href="https://github.com/mk-fg/">github</a> (maybe later <a title="my bitbucket page" href="https://bitbucket.org/mk_fg">on bittbucket</a> as well).</p><p>Will probably re-import meta stuff (issues, wikis) from there into the main tree, but still can't find nice-enough tool for that.<br />Closest thing seem to be <a title="Artemis project" href="http://www.mrzv.org/software/artemis/">Artemis</a>, but it's for mercurial, so I'll probably need to port it to git first, shouldn't be too hard.</p><p>Also, I'm torn at this point between the thoughts along the lines "selection of modern DVCS spoil us" against "damn, why they there is no clear popular + works-for-everything thing", but it's probably normal, as I have (or had) similar thoughts about lot of technologies.</p>-=||||||||||=-2012/2/Late-adventures-with-time-series-data-collection-and-representation-=|||=-2012-02-28T09:16:10-=|||=-Late adventures with time-series data collection and representation-=|||=-[u'Monitoring', u'SysAdmin', u'Notification', u'Python', u'Unix']-=|||=-<p>When something is wrong and you look at the system, most often you'll see that... well, it works. There's some cpu, disk, ram usage, some number of requests per second on different services, some stuff piling up, something in short supply here and there...</p><p>And there's just no way of telling what's wrong without answers to the questions like "so, what's the usual load average here?", "is the disk always loaded with requests 80% of time?", "is it much more requests than usual?", etc, otherwise you might be off to some wild chase just to find out that load has always been that high, or solve the mystery of some unoptimized code that's been there for ages, without doing anything about the problem in question.</p><p>Historical data is the answer, and having used <a title="rrdtool project" href="http://oss.oetiker.ch/rrdtool/">rrdtool</a> with stuff like (customized) <a title="cacti project" href="http://www.cacti.net/">cacti</a> and <a title="net-snmp project" href="http://www.net-snmp.org/">snmpd</a> (with <a title="my snmpd-pyagentx project" href="http://snmpd-pyagentx.sf.net/">some my hacks</a> on top) in the past, I was overjoyed when I stumbled upon a <a title="graphite project" href="http://graphite.readthedocs.org/en/latest/">graphite project</a> at some point.</p><p>From then on, I strived to collect as much metrics as possible, to be able to look at history of anything I want (and lots of values can be a reasonable symptom for the actual problem), without any kind of limitations.<br />carbon-cache does magic by batching writes and carbon-aggregator does a great job at relieving you of having to push aggregate metrics along with a granular ones or sum all these on graphs.</p><p>Initially, I started using it with just <a title="collectd project" href="http://collectd.org/">collectd</a> (and still using it), but there's a need for something to convert metric names to a graphite hierarcy.</p><p>After looking over quite a few solutions to collecd-carbon bridge, decided to use <a title="original bucky project" href="https://github.com/cloudant/bucky">bucky</a>, with <a title="my bucky fork" href="https://github.com/mk-fg/bucky">a few fixes of my own</a> and <a title="bucky config to translate collectd metadata to graphite metric names" href="http://fraggod.net/static/code/bucky_conf.py">quite large translation config</a>.</p><p>Bucky can work anywhere, just receiving data from collectd network plugin, understands collectd types and properly translates counter increments to N/s rates. It also includes <a title="Etsy statsd daemon (also, there's a gozillion of forks of it!)" href="https://github.com/etsy/statsd">statsd daemon</a>, which is brilliant at handling data from non-collector daemons and scripts and more powerful <a title="original metricsd project" href="https://github.com/kpumuk/metricsd">metricsd</a> implementation.<br />Downside is that it's only maintained in forks, has bugs in less-used code (like metricsd), quite resource-hungry (but can be easily scaled-out) and there's kinda-official <a title="collectd-carbon plugin" href="http://collectd.org/wiki/index.php/Plugin:Carbon">collectd-carbon plugin</a> now (although I found it buggy as well, not to mention much less featureful, but hopefully that'll be addressed in future collectd versions).</p><p>Some of the problems I've noticed with such collectd setup:</p><ul>
<li>Disk I/O metrics are godawful or just doesn't work - collected metrics of read/write either for processes of device are either zeroes, have weird values detached from reality (judging by actual problems and tools like <a title="atop tool" href="http://www.atoptool.nl/">atop</a> and <a title="sysstat project" href="http://sebastien.godard.pagesperso-orange.fr/">sysstat</a> provide) or just useless.</li>
<li>Lots of metrics for network and memory (vmem, slab) and from various plugins have naming, inconsistent with linux /proc or documentation names.</li>
<li>Some useful metrics that are in, say, sysstat doesn't seem to work with collectd, like sensor data, nfsv4, some paging and socket counters.</li>
<li>Some metrics need non-trivial post-processing to be useful - disk utilization % time is one good example.</li>
<li>Python plugins leak memory on every returned value. Some plugins (ping, for example) make collectd segfault several times a day.</li>
<li>One of the most useful info is the metrics from per-service cgroup hierarchies, created by <a title="systemd init daemon" href="http://www.freedesktop.org/wiki/Software/systemd">systemd</a> - there you can compare resource usage of various user-space components, totally pinpointing exactly what caused the spikes on all the other graphs at some time.</li>
<li>Second most useful info by far is produced from logs and while collectd has a damn powerful <a title="collectd tail plugin" href="http://collectd.org/wiki/index.php/Plugin:Tail">tail plugin</a>, I still found it to be too limited or just too complicated to use, while simple log-tailing code does the better job and is actually simplier due to more powerful language than collectd configuration. Same problem with <a title="collectd table plugin" href="http://collectd.org/wiki/index.php/Plugin:Table">table plugin</a> and /proc.</li>
<li>There's still a need for lagre post-processing chunk of code and pushing the values to carbon.</li>
</ul>
<p>Of course, I wanted to add systemd cgroup metrics, some log values and missing (and just properly-named) /proc tables data, and initially I wrote a collectd plugin for that.<br />It worked, leaked memory, occasionally crashed (with collectd itself), used some custom data types, had to have some metric-name post-processing code chunk in bucky...<br />Um, what the fuck for, when sending metric value directly takes just "echo some.metric.name $val $(printf %(%s)T -1) &gt;/dev/tcp/carbon_host/2003"?</p><p>So off with collectd for all the custom metrics.</p><p>Wrote <a title="harvestd carbon data collector" href="http://fraggod.net/static/code/harvestd.py">a simple "while True: collect_and_send() &amp;</a><a title="harvestd carbon data collector" href="http://fraggod.net/static/code/harvestd.py">&amp;</a><a title="harvestd carbon data collector" href="http://fraggod.net/static/code/harvestd.py"> sleep(till_deadline);" loop</a> in python, along with the cgroup data collectors (there are even proper "block io" and "syscall io" per-service values!), log tailer and <a title="sysstat (sadf) data carbon collector" href="http://fraggod.net/static/code/sa_carbon.py">sysstat data processor</a> (mainly for disk and network metrics which have batshit-crazy values in collectd plugins).</p><p>Another interesting data-collection alternative I've explored recently is <a title="ganglia project" href="http://ganglia.info">ganglia</a>.<br />Redundant gmond collectors and aggregators, communicating efficiently over multicast are nice. It has support for python plugins, and is very easy to use - pulling data from gmond node network can be done with one telnet or nc command, and it's fairly comprehensible xml, not some binary protocol. Another nice feature is that it can re-publish values only on some significant changes (where you define what "significant" is), thus probably eliminating traffic for 90% of "still 0" updates.</p><p>But as I found out while trying to use it as a collectd replacement (forwarding data to graphite through amqp via <a title="gmond-amqp-graphite project" href="https://github.com/mk-fg/gmond-amqp-graphite">custom scripts</a>), there's a fatal flaw - gmond plugins can't handle dynamic number of values, so writing a plugin that collects metrics from systemd services' cgroups without knowing how many of these will be started in advance is just impossible.<br />Also it has no concept for timestamps of values - it only has "current" ones, making plugins like "sysstat data parser" impossible to implement as well.<br />collectd, in contrast, has no constraint on how many values plugin returns and has timestamps, but with limitations on how far backwards they are.</p><p>Pity, gmond looked like a nice, solid and resilent thing otherwise.</p><p>I still like the idea to pipe graphite metrics through AMQP (like <a title="rocksteady project" href="http://code.google.com/p/rocksteady/">rocksteady</a> does), routing them there not only to graphite, but also to some proper threshold-monitoring daemon like <a title="shinken project" href="http://www.shinken-monitoring.org">shinken</a> (basically <a title="original nagios" href="http://www.nagios.org/">nagios</a>, but distributed and more powerful), with alerts, escalations, trending and flapping detection, etc, but most of the <a title="just a recent example" href="http://www.protocolostomy.com/2012/02/24/sending-alerts-with-graphite-graphs-from-nagios/">existing solutions</a> all seem to use graphite and whisper directly, which seem kinda wasteful.</p><p>Looking forward, I'm actually deciding between replacing collectd completely for a few most basic metrics it now collects, pulling them from sysstat or just /proc directly or maybe integrating my collectors back into collectd as plugins, extending collectd-carbon as needed and using collectd threshold monitoring and matches/filters to generate and export events to nagios/shinken... somehow first option seem to be more effort-effective, even in the long run, but then maybe I should just work more with collectd upstream, not hack around it.</p>-=|||=-<p>When something is wrong and you look at the system, most often you'll see that... well, it works. There's some cpu, disk, ram usage, some number of requests per second on different services, some stuff piling up, something in short supply here and there...</p><p>And there's just no way of telling what's wrong without answers to the questions like "so, what's the usual load average here?", "is the disk always loaded with requests 80% of time?", "is it much more requests than usual?", etc, otherwise you might be off to some wild chase just to find out that load has always been that high, or solve the mystery of some unoptimized code that's been there for ages, without doing anything about the problem in question.</p><p>Historical data is the answer, and having used <a title="rrdtool project" href="http://oss.oetiker.ch/rrdtool/">rrdtool</a> with stuff like (customized) <a title="cacti project" href="http://www.cacti.net/">cacti</a> and <a title="net-snmp project" href="http://www.net-snmp.org/">snmpd</a> (with <a title="my snmpd-pyagentx project" href="http://snmpd-pyagentx.sf.net/">some my hacks</a> on top) in the past, I was overjoyed when I stumbled upon a <a title="graphite project" href="http://graphite.readthedocs.org/en/latest/">graphite project</a> at some point.</p><p>From then on, I strived to collect as much metrics as possible, to be able to look at history of anything I want (and lots of values can be a reasonable symptom for the actual problem), without any kind of limitations.<br />carbon-cache does magic by batching writes and carbon-aggregator does a great job at relieving you of having to push aggregate metrics along with a granular ones or sum all these on graphs.</p><p>Initially, I started using it with just <a title="collectd project" href="http://collectd.org/">collectd</a> (and still using it), but there's a need for something to convert metric names to a graphite hierarcy.</p><p>After looking over quite a few solutions to collecd-carbon bridge, decided to use <a title="original bucky project" href="https://github.com/cloudant/bucky">bucky</a>, with <a title="my bucky fork" href="https://github.com/mk-fg/bucky">a few fixes of my own</a> and <a title="bucky config to translate collectd metadata to graphite metric names" href="http://fraggod.net/static/code/bucky_conf.py">quite large translation config</a>.</p><p>Bucky can work anywhere, just receiving data from collectd network plugin, understands collectd types and properly translates counter increments to N/s rates. It also includes <a title="Etsy statsd daemon (also, there's a gozillion of forks of it!)" href="https://github.com/etsy/statsd">statsd daemon</a>, which is brilliant at handling data from non-collector daemons and scripts and more powerful <a title="original metricsd project" href="https://github.com/kpumuk/metricsd">metricsd</a> implementation.<br />Downside is that it's only maintained in forks, has bugs in less-used code (like metricsd), quite resource-hungry (but can be easily scaled-out) and there's kinda-official <a title="collectd-carbon plugin" href="http://collectd.org/wiki/index.php/Plugin:Carbon">collectd-carbon plugin</a> now (although I found it buggy as well, not to mention much less featureful, but hopefully that'll be addressed in future collectd versions).</p><p>Some of the problems I've noticed with such collectd setup:</p><ul>
<li>Disk I/O metrics are godawful or just doesn't work - collected metrics of read/write either for processes of device are either zeroes, have weird values detached from reality (judging by actual problems and tools like <a title="atop tool" href="http://www.atoptool.nl/">atop</a> and <a title="sysstat project" href="http://sebastien.godard.pagesperso-orange.fr/">sysstat</a> provide) or just useless.</li>
<li>Lots of metrics for network and memory (vmem, slab) and from various plugins have naming, inconsistent with linux /proc or documentation names.</li>
<li>Some useful metrics that are in, say, sysstat doesn't seem to work with collectd, like sensor data, nfsv4, some paging and socket counters.</li>
<li>Some metrics need non-trivial post-processing to be useful - disk utilization % time is one good example.</li>
<li>Python plugins leak memory on every returned value. Some plugins (ping, for example) make collectd segfault several times a day.</li>
<li>One of the most useful info is the metrics from per-service cgroup hierarchies, created by <a title="systemd init daemon" href="http://www.freedesktop.org/wiki/Software/systemd">systemd</a> - there you can compare resource usage of various user-space components, totally pinpointing exactly what caused the spikes on all the other graphs at some time.</li>
<li>Second most useful info by far is produced from logs and while collectd has a damn powerful <a title="collectd tail plugin" href="http://collectd.org/wiki/index.php/Plugin:Tail">tail plugin</a>, I still found it to be too limited or just too complicated to use, while simple log-tailing code does the better job and is actually simplier due to more powerful language than collectd configuration. Same problem with <a title="collectd table plugin" href="http://collectd.org/wiki/index.php/Plugin:Table">table plugin</a> and /proc.</li>
<li>There's still a need for lagre post-processing chunk of code and pushing the values to carbon.</li>
</ul>
<p>Of course, I wanted to add systemd cgroup metrics, some log values and missing (and just properly-named) /proc tables data, and initially I wrote a collectd plugin for that.<br />It worked, leaked memory, occasionally crashed (with collectd itself), used some custom data types, had to have some metric-name post-processing code chunk in bucky...<br />Um, what the fuck for, when sending metric value directly takes just "echo some.metric.name $val $(printf %(%s)T -1) &gt;/dev/tcp/carbon_host/2003"?</p><p>So off with collectd for all the custom metrics.</p><p>Wrote <a title="harvestd carbon data collector" href="http://fraggod.net/static/code/harvestd.py">a simple "while True: collect_and_send() &amp;</a><a title="harvestd carbon data collector" href="http://fraggod.net/static/code/harvestd.py">&amp;</a><a title="harvestd carbon data collector" href="http://fraggod.net/static/code/harvestd.py"> sleep(till_deadline);" loop</a> in python, along with the cgroup data collectors (there are even proper "block io" and "syscall io" per-service values!), log tailer and <a title="sysstat (sadf) data carbon collector" href="http://fraggod.net/static/code/sa_carbon.py">sysstat data processor</a> (mainly for disk and network metrics which have batshit-crazy values in collectd plugins).</p><p>Another interesting data-collection alternative I've explored recently is <a title="ganglia project" href="http://ganglia.info">ganglia</a>.<br />Redundant gmond collectors and aggregators, communicating efficiently over multicast are nice. It has support for python plugins, and is very easy to use - pulling data from gmond node network can be done with one telnet or nc command, and it's fairly comprehensible xml, not some binary protocol. Another nice feature is that it can re-publish values only on some significant changes (where you define what "significant" is), thus probably eliminating traffic for 90% of "still 0" updates.</p><p>But as I found out while trying to use it as a collectd replacement (forwarding data to graphite through amqp via <a title="gmond-amqp-graphite project" href="https://github.com/mk-fg/gmond-amqp-graphite">custom scripts</a>), there's a fatal flaw - gmond plugins can't handle dynamic number of values, so writing a plugin that collects metrics from systemd services' cgroups without knowing how many of these will be started in advance is just impossible.<br />Also it has no concept for timestamps of values - it only has "current" ones, making plugins like "sysstat data parser" impossible to implement as well.<br />collectd, in contrast, has no constraint on how many values plugin returns and has timestamps, but with limitations on how far backwards they are.</p><p>Pity, gmond looked like a nice, solid and resilent thing otherwise.</p><p>I still like the idea to pipe graphite metrics through AMQP (like <a title="rocksteady project" href="http://code.google.com/p/rocksteady/">rocksteady</a> does), routing them there not only to graphite, but also to some proper threshold-monitoring daemon like <a title="shinken project" href="http://www.shinken-monitoring.org">shinken</a> (basically <a title="original nagios" href="http://www.nagios.org/">nagios</a>, but distributed and more powerful), with alerts, escalations, trending and flapping detection, etc, but most of the <a title="just a recent example" href="http://www.protocolostomy.com/2012/02/24/sending-alerts-with-graphite-graphs-from-nagios/">existing solutions</a> all seem to use graphite and whisper directly, which seem kinda wasteful.</p><p>Looking forward, I'm actually deciding between replacing collectd completely for a few most basic metrics it now collects, pulling them from sysstat or just /proc directly or maybe integrating my collectors back into collectd as plugins, extending collectd-carbon as needed and using collectd threshold monitoring and matches/filters to generate and export events to nagios/shinken... somehow first option seem to be more effort-effective, even in the long run, but then maybe I should just work more with collectd upstream, not hack around it.</p>-=||||||||||=-2012/6/Proper-ish-way-to-start-long-running-systemd-service-on-udev-event-device-hotplug-=|||=-2012-06-16T12:26:07-=|||=-Proper(-ish) way to start long-running systemd service on udev event (device hotplug)-=|||=-[u'SysAdmin', u'Systemd', u'Udev']-=|||=-<p>I use a smartcard token which requires long-running (while device is plugged) handler process to communicate with the chip.<br />Basically, udev has to start a daemon process when the device get plugged.</p><p>Until recently, udev didn't mind doing that via just RUN+="/path/to/binary ...", but in recent merged systemd-udevd versions this behavior was deprecated:</p><pre>RUN<br />...<br />Starting daemons or other long running processes is not appropriate for udev; the forked<br />processes, detached or not, will be unconditionally killed after the event handling has<br />finished.<br /></pre><p>I think it's for the best - less accumulating cruft and unmanageable pids forked from udevd, but unfortunately it also breaks existing udev rule-files, the ones which use RUN+="..." to do just that.</p><p>One of the most obvious breakage for me was the smartcard failing, so decided to fix that. Documentation on the whole migration process is somewhat lacking (hence this post), even though docs on all the individual pieces are there (which are actually awesome).</p><p>Main doc here is systemd.device(5) for the reference on the udev attributes which systemd recognizes, and of course udev(7) for a generic syntax reference.<br />Also, there's <a title="systemd for Developers II" href="http://0pointer.de/blog/projects/socket-activation2.html">this entry</a> on Lennart's blog.</p><p>In my case, when device (usb smartcard token) get plugged, ifdhandler process should be started via openct-control (<a title="OpenCT project" href="https://www.opensc-project.org/openct">OpenCT sc middleware</a>), which then creates unix socket through which openct libraries (used in turn by OpenSC PKCS#11 or PCSClite) can access the hardware.</p><p>So, basically I've had something like this (there are more rules for different hw, of course, but for the sake of clarity...):</p><pre>SUBSYSTEM!="usb", GOTO="openct_rules_end"<br />ACTION!="add", GOTO="openct_rules_end"<br /><br />PROGRAM="/bin/sleep 0.1"<br />SUBSYSTEM=="usb", ENV{DEVTYPE}=="usb_device",\<br /> ENV{ID_VENDOR_ID}=="0529", ENV{ID_MODEL_ID}=="0600",\<br /> GROUP="usb",\<br /> RUN+="/usr/sbin/openct-control attach usb:$env{PRODUCT} usb $env{DEVNAME}"<br /><br />LABEL="openct_rules_end"<br /></pre><p>Instead of RUN here, ENV{SYSTEMD_WANTS} can be used to start a properly-handled service, but note that some hardware parameters are passed from udev properties and in general systemd unit can't reference these.</p><p>I.e. if just ENV{SYSTEMD_WANTS}="openct-handler.service" (or more generic smartcard.target) is started, it won't know which device to pass to "openct-control attach" command.</p><p>One way might be storing these parameters in some dir, where they'll be picked by some path unit, a bit more hacky way would be scanning usb bus in the handler, and yet another one (which I decided to go along with) is to use systemd unit-file templating to pass these parameters.</p><p>openct-handler@.service:</p><pre>[Unit]<br />Requires=openct.service<br /><br />[Service]<br />Type=forking<br />GuessMainPID=false<br />ExecStart=/bin/sh -c "exec openct-control attach %I"<br /></pre><p>Note that it requires openct.service, which is basically does "openct-control init" once per boot to setup paths and whatnot:</p><pre>[Service]<br />Type=oneshot<br />RemainAfterExit=yes<br />ExecStart=/usr/sbin/openct-control init<br />ExecStop=/usr/sbin/openct-control shutdown<br /><br />[Install]<br />WantedBy=multi-user.target<br /></pre><p>Another thing to note is that "sh" used in the handler.<br />It's intentional, because just %I will be passed by systemd as a single argument, while it should be three of them after "attach".</p><p>Finally, udev rules file for the device:</p><pre>SUBSYSTEM!="usb", GOTO="openct_rules_end"<br />ACTION!="add", GOTO="openct_rules_end"<br /><br />SUBSYSTEM=="usb", ENV{DEVTYPE}=="usb_device",\<br /> ENV{ID_VENDOR_ID}=="0529", ENV{ID_MODEL_ID}=="0600",\<br /> GROUP="usb", TAG+="systemd",\<br /> ENV{SYSTEMD_WANTS}="openct-handler@\<br /> usb:$env{ID_VENDOR_ID}-$env{ID_MODEL_ID}-$env{ID_REVISION}\<br /> \x20usb\x20-dev-bus-usb-$env{BUSNUM}-$env{DEVNUM}.service"<br /><br />LABEL="openct_rules_end"<br /><br /></pre><p>(I highly doubt newline escaping in ENV{SYSTEMD_WANTS} above will work - added them just for readability, so pls strip these in your mind to a single line without spaces)</p><p>Systemd escaping in the rule above is described in systemd.unit(5) and produces a name - and start a service - like this one:<br />  openct-handler@usb:0529-0600-0100\x20usb\x20-dev-bus-usb-002-003.service</p><p>Which then invokes:<br />  sh -c "exec openct-control attach usb:0529/0600/0100 usb /dev/bus/usb/002/003"</p><p>And it forks ifdhandler process, which works with smartcard from then on.</p><p>ifdhandler seem to be able to detect unplugging events and exits gracefully, but otherwise BindTo= unit directive can be used to stop the service when udev detects that device is unplugged.</p><p>Note that it might be more obvious to just do RUN+="systemctl start whatever.service", but it's a worse way to do it, because you don't bind that service to a device in any way, don't produce the "whatever.device" unit and there are lot of complications due to systemctl being a tool for the user, not the API proper.</p>-=|||=-<p>I use a smartcard token which requires long-running (while device is plugged) handler process to communicate with the chip.<br />Basically, udev has to start a daemon process when the device get plugged.</p><p>Until recently, udev didn't mind doing that via just RUN+="/path/to/binary ...", but in recent merged systemd-udevd versions this behavior was deprecated:</p><pre>RUN<br />...<br />Starting daemons or other long running processes is not appropriate for udev; the forked<br />processes, detached or not, will be unconditionally killed after the event handling has<br />finished.<br /></pre><p>I think it's for the best - less accumulating cruft and unmanageable pids forked from udevd, but unfortunately it also breaks existing udev rule-files, the ones which use RUN+="..." to do just that.</p><p>One of the most obvious breakage for me was the smartcard failing, so decided to fix that. Documentation on the whole migration process is somewhat lacking (hence this post), even though docs on all the individual pieces are there (which are actually awesome).</p><p>Main doc here is systemd.device(5) for the reference on the udev attributes which systemd recognizes, and of course udev(7) for a generic syntax reference.<br />Also, there's <a title="systemd for Developers II" href="http://0pointer.de/blog/projects/socket-activation2.html">this entry</a> on Lennart's blog.</p><p>In my case, when device (usb smartcard token) get plugged, ifdhandler process should be started via openct-control (<a title="OpenCT project" href="https://www.opensc-project.org/openct">OpenCT sc middleware</a>), which then creates unix socket through which openct libraries (used in turn by OpenSC PKCS#11 or PCSClite) can access the hardware.</p><p>So, basically I've had something like this (there are more rules for different hw, of course, but for the sake of clarity...):</p><pre>SUBSYSTEM!="usb", GOTO="openct_rules_end"<br />ACTION!="add", GOTO="openct_rules_end"<br /><br />PROGRAM="/bin/sleep 0.1"<br />SUBSYSTEM=="usb", ENV{DEVTYPE}=="usb_device",\<br /> ENV{ID_VENDOR_ID}=="0529", ENV{ID_MODEL_ID}=="0600",\<br /> GROUP="usb",\<br /> RUN+="/usr/sbin/openct-control attach usb:$env{PRODUCT} usb $env{DEVNAME}"<br /><br />LABEL="openct_rules_end"<br /></pre><p>Instead of RUN here, ENV{SYSTEMD_WANTS} can be used to start a properly-handled service, but note that some hardware parameters are passed from udev properties and in general systemd unit can't reference these.</p><p>I.e. if just ENV{SYSTEMD_WANTS}="openct-handler.service" (or more generic smartcard.target) is started, it won't know which device to pass to "openct-control attach" command.</p><p>One way might be storing these parameters in some dir, where they'll be picked by some path unit, a bit more hacky way would be scanning usb bus in the handler, and yet another one (which I decided to go along with) is to use systemd unit-file templating to pass these parameters.</p><p>openct-handler@.service:</p><pre>[Unit]<br />Requires=openct.service<br /><br />[Service]<br />Type=forking<br />GuessMainPID=false<br />ExecStart=/bin/sh -c "exec openct-control attach %I"<br /></pre><p>Note that it requires openct.service, which is basically does "openct-control init" once per boot to setup paths and whatnot:</p><pre>[Service]<br />Type=oneshot<br />RemainAfterExit=yes<br />ExecStart=/usr/sbin/openct-control init<br />ExecStop=/usr/sbin/openct-control shutdown<br /><br />[Install]<br />WantedBy=multi-user.target<br /></pre><p>Another thing to note is that "sh" used in the handler.<br />It's intentional, because just %I will be passed by systemd as a single argument, while it should be three of them after "attach".</p><p>Finally, udev rules file for the device:</p><pre>SUBSYSTEM!="usb", GOTO="openct_rules_end"<br />ACTION!="add", GOTO="openct_rules_end"<br /><br />SUBSYSTEM=="usb", ENV{DEVTYPE}=="usb_device",\<br /> ENV{ID_VENDOR_ID}=="0529", ENV{ID_MODEL_ID}=="0600",\<br /> GROUP="usb", TAG+="systemd",\<br /> ENV{SYSTEMD_WANTS}="openct-handler@\<br /> usb:$env{ID_VENDOR_ID}-$env{ID_MODEL_ID}-$env{ID_REVISION}\<br /> \x20usb\x20-dev-bus-usb-$env{BUSNUM}-$env{DEVNUM}.service"<br /><br />LABEL="openct_rules_end"<br /><br /></pre><p>(I highly doubt newline escaping in ENV{SYSTEMD_WANTS} above will work - added them just for readability, so pls strip these in your mind to a single line without spaces)</p><p>Systemd escaping in the rule above is described in systemd.unit(5) and produces a name - and start a service - like this one:<br />  openct-handler@usb:0529-0600-0100\x20usb\x20-dev-bus-usb-002-003.service</p><p>Which then invokes:<br />  sh -c "exec openct-control attach usb:0529/0600/0100 usb /dev/bus/usb/002/003"</p><p>And it forks ifdhandler process, which works with smartcard from then on.</p><p>ifdhandler seem to be able to detect unplugging events and exits gracefully, but otherwise BindTo= unit directive can be used to stop the service when udev detects that device is unplugged.</p><p>Note that it might be more obvious to just do RUN+="systemctl start whatever.service", but it's a worse way to do it, because you don't bind that service to a device in any way, don't produce the "whatever.device" unit and there are lot of complications due to systemctl being a tool for the user, not the API proper.</p>-=||||||||||=-2012/8/Unhosted-remoteStorage-idea-=|||=-2012-08-09T06:09:57-=|||=-Unhosted remoteStorage idea-=|||=-[u'Web', u'p2p', u'Social']-=|||=-<p>Having a bit of free time recently, worked a bit on <a title="My feedjack fork" href="https://github.com/mk-fg/feedjack/">feedjack</a> web rss reader / aggregator project.<br />To keep track of what's already read and what's not, historically I've used js + client-side localStorage approach, which has quite a few advantages:</p><ul>
<li>Works with multiple clients, i.e. everyone has it's own state.</li>
<li>Server doesn't have to store any data for possible-infinite number of clients, not even session or login data.</li>
<li>Same pages still can be served to all clients, some will just hide unwanted content.</li>
<li>Previous point leads to pages being very cache-friendly.</li>
<li>No need to "recognize" client in any way, which is commonly acheived with authentication.</li>
<li>No interation of "write" kind with the server means much less potential for abuse (DDoS, spam, other kinds of exploits).</li>
</ul>
<p>Flip side of that rosy picture is that localStorage only works in one browser (or possibly several synced instances), which is quite a drag, because one advantage of a web-based reader is that it can be accessed from anywhere, not just single platform, where you might as well install specialized app.</p><p>To fix that unfortunate limitation, about a year ago I've added ad-hoc storage mechanism to just dump localStorage contents as json to some persistent storage on server, authenticated by special "magic" header from a browser.<br />It was never a public feature, requiring some browser tweaking and being a server admin, basically.</p><p>Recently, however, <a title="remoteStorage.js client-side protocol implementation" href="http://remotestoragejs.com/">remoteStorage</a> project from <a title="Unhosted movement" href="http://unhosted.org/">unhosted group</a> has caught my attention.</p><p>Idea itself and the movement's goals are quite ambitious and otherwise awesome - to return to "decentralized web" idea, using simple already available mechanisms like webfinger for service discovery (reminds of <a title="Thimbl concept project" href="http://thimbl.net">Thimbl</a> concept by telekommunisten.net), WebDAV for storage and OAuth2 for authorization (meaning no special per-service passwords or similar crap).<br />But the most interesting thing I've found about it is that it should be actually easier to use than write ad-hoc client syncer and server storage implementation - just put off-the-shelf remoteStorage.js to the page (it even includes "syncer" part to sync localStorage to remote server) and depoy or find any remoteStorage provider and you're all set.</p><p>In practice, it works as advertised, but will have quite significant changes soon (with the release of 0.7.0 js version) and had only ad-hoc proof-of-concept server implementation in python (though there's also <a title="ownCloud project" href="http://owncloud.org/">ownCloud</a> in php and node.js/ruby versions), so I wrote <a title="my django-unhosted storage implementation" href="https://github.com/mk-fg/django-unhosted">django-unhosted</a> implementation, being basically a glue between simple WebDAV, <a title="OAuth 2.0 app for Django" href="https://github.com/hiidef/oauth2app/">oauth2app</a> and Django Storage API (which <a title="List of some Django Storage API backend implementations" href="https://github.com/mk-fg/django-unhosted#storage--webdav">has backends</a> for everything).<br />Using that thing in feedjack now (<a title="My instance of feedjack" href="http://fraggod.net/feeds/blogs_mesh">here</a>, for example) instead of that hacky json cache I've had with django-unhosted deployed on my server, allowing to also use it with all the <a title="List of apps (among other things) with remoteStorage support" href="https://github.com/unhosted/website/wiki/State-of-the-movement">apps with support</a> out there.</p><p>Looks like a really neat way to provide some persistent storage for any webapp out there, guess that's one problem solved for any future webapps I might deploy that will need one.<br />With JS being able to even load and use binary blobs (like images) that way now, it becomes possible to write even unhosted facebook, with only events like status updates still aggregated and broadcasted through some central point.</p><p>I bet there's gotta be something similar, but with facebook, twitter or maybe github backends, but as proven in many cases, it's not quite sane to rely on these centralized platforms for any kind of service, which is especially a pain if implementation there is one-platform-specific, unlike one remoteStorage protocol for any of them.<br />Would be really great if they'd support some protocol like that at some point though.</p><p>But aside for short-term "problem solved" thing, it's really nice to see such movements out there, even though whole stack of market incentives (which <a title="good summary of issues with privately-owned platforms" href="http://www.dmytri.info/commercialization-makes-your-online-rights-irrelevant-more-thoughts-from-my-talk-with-ioerror-at-rp12/">heavily favors</a> control over data, centralization and monopolies) is against them.</p>-=|||=-<p>Having a bit of free time recently, worked a bit on <a title="My feedjack fork" href="https://github.com/mk-fg/feedjack/">feedjack</a> web rss reader / aggregator project.<br />To keep track of what's already read and what's not, historically I've used js + client-side localStorage approach, which has quite a few advantages:</p><ul>
<li>Works with multiple clients, i.e. everyone has it's own state.</li>
<li>Server doesn't have to store any data for possible-infinite number of clients, not even session or login data.</li>
<li>Same pages still can be served to all clients, some will just hide unwanted content.</li>
<li>Previous point leads to pages being very cache-friendly.</li>
<li>No need to "recognize" client in any way, which is commonly acheived with authentication.</li>
<li>No interation of "write" kind with the server means much less potential for abuse (DDoS, spam, other kinds of exploits).</li>
</ul>
<p>Flip side of that rosy picture is that localStorage only works in one browser (or possibly several synced instances), which is quite a drag, because one advantage of a web-based reader is that it can be accessed from anywhere, not just single platform, where you might as well install specialized app.</p><p>To fix that unfortunate limitation, about a year ago I've added ad-hoc storage mechanism to just dump localStorage contents as json to some persistent storage on server, authenticated by special "magic" header from a browser.<br />It was never a public feature, requiring some browser tweaking and being a server admin, basically.</p><p>Recently, however, <a title="remoteStorage.js client-side protocol implementation" href="http://remotestoragejs.com/">remoteStorage</a> project from <a title="Unhosted movement" href="http://unhosted.org/">unhosted group</a> has caught my attention.</p><p>Idea itself and the movement's goals are quite ambitious and otherwise awesome - to return to "decentralized web" idea, using simple already available mechanisms like webfinger for service discovery (reminds of <a title="Thimbl concept project" href="http://thimbl.net">Thimbl</a> concept by telekommunisten.net), WebDAV for storage and OAuth2 for authorization (meaning no special per-service passwords or similar crap).<br />But the most interesting thing I've found about it is that it should be actually easier to use than write ad-hoc client syncer and server storage implementation - just put off-the-shelf remoteStorage.js to the page (it even includes "syncer" part to sync localStorage to remote server) and depoy or find any remoteStorage provider and you're all set.</p><p>In practice, it works as advertised, but will have quite significant changes soon (with the release of 0.7.0 js version) and had only ad-hoc proof-of-concept server implementation in python (though there's also <a title="ownCloud project" href="http://owncloud.org/">ownCloud</a> in php and node.js/ruby versions), so I wrote <a title="my django-unhosted storage implementation" href="https://github.com/mk-fg/django-unhosted">django-unhosted</a> implementation, being basically a glue between simple WebDAV, <a title="OAuth 2.0 app for Django" href="https://github.com/hiidef/oauth2app/">oauth2app</a> and Django Storage API (which <a title="List of some Django Storage API backend implementations" href="https://github.com/mk-fg/django-unhosted#storage--webdav">has backends</a> for everything).<br />Using that thing in feedjack now (<a title="My instance of feedjack" href="http://fraggod.net/feeds/blogs_mesh">here</a>, for example) instead of that hacky json cache I've had with django-unhosted deployed on my server, allowing to also use it with all the <a title="List of apps (among other things) with remoteStorage support" href="https://github.com/unhosted/website/wiki/State-of-the-movement">apps with support</a> out there.</p><p>Looks like a really neat way to provide some persistent storage for any webapp out there, guess that's one problem solved for any future webapps I might deploy that will need one.<br />With JS being able to even load and use binary blobs (like images) that way now, it becomes possible to write even unhosted facebook, with only events like status updates still aggregated and broadcasted through some central point.</p><p>I bet there's gotta be something similar, but with facebook, twitter or maybe github backends, but as proven in many cases, it's not quite sane to rely on these centralized platforms for any kind of service, which is especially a pain if implementation there is one-platform-specific, unlike one remoteStorage protocol for any of them.<br />Would be really great if they'd support some protocol like that at some point though.</p><p>But aside for short-term "problem solved" thing, it's really nice to see such movements out there, even though whole stack of market incentives (which <a title="good summary of issues with privately-owned platforms" href="http://www.dmytri.info/commercialization-makes-your-online-rights-irrelevant-more-thoughts-from-my-talk-with-ioerror-at-rp12/">heavily favors</a> control over data, centralization and monopolies) is against them.</p>-=||||||||||=-2012/8/A-new-toy-to-play-with-TI-Launchpad-with-MSP430-MCU-=|||=-2012-08-16T09:02:59-=|||=-A new toy to play with - TI Launchpad with MSP430 MCU-=|||=-[u'Hardware']-=|||=-<p>A friend gave me this thing to play with (and eventually adapt to his purposes).</p><p><img alt="Launchpad box and contents" title="Launchpad box and contents" style="width: 500px; height: 264px; margin: 2px; display: block;" src="http://blog.fraggod.net/static/embed/launchpad.jpg" align="middle" />What's interesting here is that TI seem to give these things out for free.<br />Seriously, a box with a debug/programmer board and two microcontroller chips (<a title="MSP430 specs" href="http://www.ti.com/product/msp430g2553">which are</a> basically your programmable computer with RAM, non-volatile flash memory, lots of interfaces, temp sensor, watchdog, etc that can be powered from 2 AA cells), to any part of the world with FedEx for a beer's worth - $5.</p><p>Guess it's time to put a computer into every doorknob indeed.</p>-=|||=-<p>A friend gave me this thing to play with (and eventually adapt to his purposes).</p><p><img alt="Launchpad box and contents" title="Launchpad box and contents" style="width: 500px; height: 264px; margin: 2px; display: block;" src="http://blog.fraggod.net/static/embed/launchpad.jpg" align="middle" />What's interesting here is that TI seem to give these things out for free.<br />Seriously, a box with a debug/programmer board and two microcontroller chips (<a title="MSP430 specs" href="http://www.ti.com/product/msp430g2553">which are</a> basically your programmable computer with RAM, non-volatile flash memory, lots of interfaces, temp sensor, watchdog, etc that can be powered from 2 AA cells), to any part of the world with FedEx for a beer's worth - $5.</p><p>Guess it's time to put a computer into every doorknob indeed.</p>-=||||||||||=-2012/9/Terms-of-Service-Didnt-Read-=|||=-2012-09-16T19:32:55-=|||=-Terms of Service - Didn't Read-=|||=-[u'Policy', u'Documentation', u'Social']-=|||=-<p>Right now I was working on python-skydrive module and further integration of <a title="Microsoft SkyDrive cloud service" href="http://skydrive.live.com/">MS SkyDrive</a> into <a title="Tahoe-LAFS least authority filesystem" href="http://tahoe-lafs.org/">tahoe-lafs</a> as a cloud backend, to keep the stuff you really care about safe.</p><p>And even if you don't trust SkyDrive to keep stuff safe, you still have to register your app with these guys, especially if it's an open module, because "You are solely and entirely responsible for all uses of Live Connect occurring under your Client ID." and it's unlikely that a generic python interface author will vouch for all it's uses like that.</p><p>What do "register app" mean? Agreeing to <a title="Microsoft LiveConnect Terms of Service" href="http://msdn.microsoft.com/en-US/library/live/ff765012">yet another "Terms of Service"</a>, of course!</p><p>Do anyone ever reads these?</p><p>What the hell "You may only use the Live SDK and Live Connect APIs to create software." sentence means there?<br />Did you know that "You are solely and entirely responsible for all uses of Live Connect occurring under your Client ID." (and that's an app-id, given out to the app developers, not users)?<br />How many more of such interesting stuff is there?</p><p>I hardly care enough to read, but <a title="tos;dr project" href="http://tos-dr.info/">there's an app</a> for exactly that, and it's relatively well-known by now.</p><p>What might be not as well-known, is that there's now a <a title="Click it!" href="http://www.indiegogo.com/terms-of-service-didnt-read">campaign on IndieGoGo</a> to keep the thing alive and make it better.<br />Please consider supporting the movement in any way, even just by spreading the word, right now, it's really one of the areas where filtering-out of all the legalese crap and noise is badly needed.</p><p><a title="tos;dr IndieGoGo Campaign" href="http://www.indiegogo.com/terms-of-service-didnt-read">http://www.indiegogo.com/terms-of-service-didnt-read</a></p>-=|||=-<p>Right now I was working on python-skydrive module and further integration of <a title="Microsoft SkyDrive cloud service" href="http://skydrive.live.com/">MS SkyDrive</a> into <a title="Tahoe-LAFS least authority filesystem" href="http://tahoe-lafs.org/">tahoe-lafs</a> as a cloud backend, to keep the stuff you really care about safe.</p><p>And even if you don't trust SkyDrive to keep stuff safe, you still have to register your app with these guys, especially if it's an open module, because "You are solely and entirely responsible for all uses of Live Connect occurring under your Client ID." and it's unlikely that a generic python interface author will vouch for all it's uses like that.</p><p>What do "register app" mean? Agreeing to <a title="Microsoft LiveConnect Terms of Service" href="http://msdn.microsoft.com/en-US/library/live/ff765012">yet another "Terms of Service"</a>, of course!</p><p>Do anyone ever reads these?</p><p>What the hell "You may only use the Live SDK and Live Connect APIs to create software." sentence means there?<br />Did you know that "You are solely and entirely responsible for all uses of Live Connect occurring under your Client ID." (and that's an app-id, given out to the app developers, not users)?<br />How many more of such interesting stuff is there?</p><p>I hardly care enough to read, but <a title="tos;dr project" href="http://tos-dr.info/">there's an app</a> for exactly that, and it's relatively well-known by now.</p><p>What might be not as well-known, is that there's now a <a title="Click it!" href="http://www.indiegogo.com/terms-of-service-didnt-read">campaign on IndieGoGo</a> to keep the thing alive and make it better.<br />Please consider supporting the movement in any way, even just by spreading the word, right now, it's really one of the areas where filtering-out of all the legalese crap and noise is badly needed.</p><p><a title="tos;dr IndieGoGo Campaign" href="http://www.indiegogo.com/terms-of-service-didnt-read">http://www.indiegogo.com/terms-of-service-didnt-read</a></p>-=||||||||||=-2013/1/rip-aaronsw-=|||=-2013-01-12T17:22:42-=|||=-rip aaronsw-=|||=--=|||=-<p>$DEITY knows, I'm as far from the best person to write such obituary as can be found, but it's just unbearable not to.</p><p>Aaron was a North Star to aspire to but never reach, before or after.</p><p>Despite what anyone might say about his personality, in my world, he was one of the founders of the internet as we know it.</p><p>From the initial p2p dream, Bram had the most success with the distributed part, Zooko - the secure one, Tav is working hard to put it all together and Aaron went after the human side.</p><p>I think it takes the most courage not to retreat into the safe digital world but fight for the future here, in the real one, with the messy people and politics, without screaming for violent revolution (which is bound to end in blood and tears), but working towards a gradual change for the best.</p><p>I never met Aaron in person, but when a few years back I sat before a bathtub with my own veins slashed, his blog and ideas were the very things that stopped me from just sitting there waiting till the bitter end.<br />So if anything, whether it's significant in any way or not, his brief spark of life still made the immense difference to my world.<br />There are surely no words in any language I know of to express my gratitude for that.</p><p>Thank you Aaron for all your choices and for everything you've done for this world.</p>-=|||=-<p>$DEITY knows, I'm as far from the best person to write such obituary as can be found, but it's just unbearable not to.</p><p>Aaron was a North Star to aspire to but never reach, before or after.</p><p>Despite what anyone might say about his personality, in my world, he was one of the founders of the internet as we know it.</p><p>From the initial p2p dream, Bram had the most success with the distributed part, Zooko - the secure one, Tav is working hard to put it all together and Aaron went after the human side.</p><p>I think it takes the most courage not to retreat into the safe digital world but fight for the future here, in the real one, with the messy people and politics, without screaming for violent revolution (which is bound to end in blood and tears), but working towards a gradual change for the best.</p><p>I never met Aaron in person, but when a few years back I sat before a bathtub with my own veins slashed, his blog and ideas were the very things that stopped me from just sitting there waiting till the bitter end.<br />So if anything, whether it's significant in any way or not, his brief spark of life still made the immense difference to my world.<br />There are surely no words in any language I know of to express my gratitude for that.</p><p>Thank you Aaron for all your choices and for everything you've done for this world.</p>-=||||||||||=-2013/1/Migrating-configuration-settings-to-E17-enlightenment-0170-from-older-E-versions-=|||=-2013-01-16T18:59:18-=|||=-Migrating configuration / settings to E17 (enlightenment) 0.17.0 from older E versions-=|||=-[u'Desktop']-=|||=-<p>It's a documented feature that <a title="E17 release announcement" href="http://www.enlightenment.org/p.php?p=news/show&amp;l=en&amp;news_id=77">0.17.0 release</a> (even if late pre-release version was used before) throws existing configuration out of the window.</p><p>I'm not sure what warranted such a drastic usability bomb, but it's not actually as bad as it seems - like 95% of configuration (and 100% of *important* parts of it) can be just re-used (even if you've already started new version!) with just a little bit of extra effort (thanks to ppurka in #e for pointing me in the right direction here).<br />So wasn't looking forward to restore all the keyboard bindings, for one thing (that's why I actually did the update just one week ago or so).</p><p>E is a bit special (at least among wm's, fairly sure some de's do similar things as well) in that it keeps it's settings on disk compiled and compressed (with <a title="eet efl component" href="http://docs.enlightenment.org/auto/eet/">eet</a>) - but it's much easier to work with than it might sound like.</p><p>So, to get the bits of config migrated, one just has to pull the old (pre-zero) config out, then start zero-release e to generate new config, decompile both of these, pull compatible bits from old into the new one, then compile it and put back into ~/.e/e/config</p><p>Before zero update, config can be found in ~/.e/e/config/standard/e.cfg</p><p>If release version was started already and dropped the config, then old one should be ~/.e/e/config/standard/e.1.cfg (or any different number instead of "1" there).</p><p>Note that "standard" there is a profile name, if it might be called differently, check ~/.e/e/config/profile.cfg (profile name should be readable there, or use "eet -x ~/.e/e/config/profile.cfg config").</p><p>"eet -d ~/.e/e/config/standard/e.cfg config" should produce perfectly readable version of the config to stdout.</p><p>Below is how I went about the whole process.</p><p>Make a repository to track changes (will help if workflow might have merge-test steps than one):</p><pre>% mkdir e_config_migration<br />% cd e_config_migration<br />% git init<br /></pre><p>Before zero update:</p><pre>% cp ~/.e/e/config/standard/e.cfg e_pre_zero<br />% eet -d e_pre_zero config > e_pre_zero.cfg<br /></pre><p>Start E-release (wipes the config, produces a "default" new one there).</p><pre>% cp ~/.e/e/config/standard/e.cfg e_zero<br />% eet -d e_zero config > e_zero.cfg<br />% git add e_*<br />% git commit -m "Initial pre/post configurations"<br /><br />% emacs e_pre_zero.cfg e_zero.cfg<br /></pre><p>Then copy all the settings that were used in any way to e_zero.cfg.</p><p>I copied pretty much all the sections with relevant stuff, checking that the keys in them are the same - and they were, but I've used 0.17.0alpha8 before going for release, so if not, I'd just try "filling the blanks", or, failing that, just using old settings as a "what has to be setup through settings-panel" reference.</p><p>To be more specific - "xkb" options/layouts (have 2 of them setup), shelves/gadgets (didn't have these, and was lazy to click-remove existing ones), "remembers" (huge section, copied all of it, worked!), all "bindings" (pain to setup these).</p><p>After all these sections, there's a flat list of "value" things, which turned out to contain quite a lot of hard-to-find-in-menus parameters, so here's what I did:</p><ul>
<li>copy that list (~200 lines) from old config to some file - say, "values.old", then from a new one to e.g. "values.new".</li>
<li>sort -u values.old > values.old.sorted<br />sort -u values.new > values.new.sorted</li>
<li>diff -uw values.{old,new}.sorted</li>
</ul>
<p>Should show everything that might need to be changed in the new config with descriptive names and reveal all the genuinely new parameters.<br />Just don't touch "config_version" value, so E won't drop the resulting config.</p><p>After all the changes:</p><pre>% eet -e e_zero config e_zero.cfg 1<br />% git commit -a -m Merged-1<br />% cp e_zero ~/.e/e/config/standard/e.cfg<br /><br />% startx<br /></pre><p>New config worked for me for all the changes I've made - wasn't sure if I can copy *that* much from the start, but it turned out that almost no reconfiguration was necessary.</p><p>Caveat is, of course, that you should know what you're doing here, and be ready to handle issues / rollback, if any, that's why putting all these changes in git might be quite helpful.</p>-=|||=-<p>It's a documented feature that <a title="E17 release announcement" href="http://www.enlightenment.org/p.php?p=news/show&amp;l=en&amp;news_id=77">0.17.0 release</a> (even if late pre-release version was used before) throws existing configuration out of the window.</p><p>I'm not sure what warranted such a drastic usability bomb, but it's not actually as bad as it seems - like 95% of configuration (and 100% of *important* parts of it) can be just re-used (even if you've already started new version!) with just a little bit of extra effort (thanks to ppurka in #e for pointing me in the right direction here).<br />So wasn't looking forward to restore all the keyboard bindings, for one thing (that's why I actually did the update just one week ago or so).</p><p>E is a bit special (at least among wm's, fairly sure some de's do similar things as well) in that it keeps it's settings on disk compiled and compressed (with <a title="eet efl component" href="http://docs.enlightenment.org/auto/eet/">eet</a>) - but it's much easier to work with than it might sound like.</p><p>So, to get the bits of config migrated, one just has to pull the old (pre-zero) config out, then start zero-release e to generate new config, decompile both of these, pull compatible bits from old into the new one, then compile it and put back into ~/.e/e/config</p><p>Before zero update, config can be found in ~/.e/e/config/standard/e.cfg</p><p>If release version was started already and dropped the config, then old one should be ~/.e/e/config/standard/e.1.cfg (or any different number instead of "1" there).</p><p>Note that "standard" there is a profile name, if it might be called differently, check ~/.e/e/config/profile.cfg (profile name should be readable there, or use "eet -x ~/.e/e/config/profile.cfg config").</p><p>"eet -d ~/.e/e/config/standard/e.cfg config" should produce perfectly readable version of the config to stdout.</p><p>Below is how I went about the whole process.</p><p>Make a repository to track changes (will help if workflow might have merge-test steps than one):</p><pre>% mkdir e_config_migration<br />% cd e_config_migration<br />% git init<br /></pre><p>Before zero update:</p><pre>% cp ~/.e/e/config/standard/e.cfg e_pre_zero<br />% eet -d e_pre_zero config > e_pre_zero.cfg<br /></pre><p>Start E-release (wipes the config, produces a "default" new one there).</p><pre>% cp ~/.e/e/config/standard/e.cfg e_zero<br />% eet -d e_zero config > e_zero.cfg<br />% git add e_*<br />% git commit -m "Initial pre/post configurations"<br /><br />% emacs e_pre_zero.cfg e_zero.cfg<br /></pre><p>Then copy all the settings that were used in any way to e_zero.cfg.</p><p>I copied pretty much all the sections with relevant stuff, checking that the keys in them are the same - and they were, but I've used 0.17.0alpha8 before going for release, so if not, I'd just try "filling the blanks", or, failing that, just using old settings as a "what has to be setup through settings-panel" reference.</p><p>To be more specific - "xkb" options/layouts (have 2 of them setup), shelves/gadgets (didn't have these, and was lazy to click-remove existing ones), "remembers" (huge section, copied all of it, worked!), all "bindings" (pain to setup these).</p><p>After all these sections, there's a flat list of "value" things, which turned out to contain quite a lot of hard-to-find-in-menus parameters, so here's what I did:</p><ul>
<li>copy that list (~200 lines) from old config to some file - say, "values.old", then from a new one to e.g. "values.new".</li>
<li>sort -u values.old > values.old.sorted<br />sort -u values.new > values.new.sorted</li>
<li>diff -uw values.{old,new}.sorted</li>
</ul>
<p>Should show everything that might need to be changed in the new config with descriptive names and reveal all the genuinely new parameters.<br />Just don't touch "config_version" value, so E won't drop the resulting config.</p><p>After all the changes:</p><pre>% eet -e e_zero config e_zero.cfg 1<br />% git commit -a -m Merged-1<br />% cp e_zero ~/.e/e/config/standard/e.cfg<br /><br />% startx<br /></pre><p>New config worked for me for all the changes I've made - wasn't sure if I can copy *that* much from the start, but it turned out that almost no reconfiguration was necessary.</p><p>Caveat is, of course, that you should know what you're doing here, and be ready to handle issues / rollback, if any, that's why putting all these changes in git might be quite helpful.</p>-=||||||||||=-2013/1/PyParsing-vs-Yapps-=|||=-2013-01-21T04:15:47-=|||=-PyParsing vs Yapps-=|||=-[u'Python']-=|||=-<p>As I've been decompiling dynamic E config <a title="fun with E zero update" href="http://blog.fraggod.net/2013/1/Migrating-configuration-settings-to-E17-enlightenment-0170-from-older-E-versions">in the past</a> anyway, wanted to back it up to git repo along with the rest of them.</p><p>Quickly stumbled upon a problem though - while E doesn't really modify it without me making some conscious changes, it reorders (or at least eet produces such) sections and values there, making straight dump to git a bit more difficult.<br />Plus, I have a <a title="aura project" href="http://desktop-aura.sourceforge.net/">pet project</a> to update background, and it also introduces transient changes, so some pre-git processing was in order.</p><p>e.cfg looks like this:</p><pre>group "E_Config" struct {<br />  group "xkb.used_options" list {<br />    group "E_Config_XKB_Option" struct {<br />      value "name" string: "grp:caps_toggle";<br />    }<br />  }<br />  group "xkb.used_layouts" list {<br />    group "E_Config_XKB_Layout" struct {<br />      value "name" string: "us";<br />...<br /><br /></pre><p>Simple way to make it "canonical" is just to order groups/values there alphabetically, blanking-out some transient ones.</p><p>That needs a parser, and while regexps aren't really suited to that kind of thing, pyparsing should work:</p><pre name="code" class="python">number = pp.Regex(r'[+-]?\d+(\.\d+)?')
string = pp.QuotedString('"') | pp.QuotedString("'")
value_type = pp.Regex(r'\w+:')
group_type = pp.Regex(r'struct|list')

value = number | string
block_value = pp.Keyword('value')\
 + string + value_type + value + pp.Literal(';')

block = pp.Forward()
block_group = pp.Keyword('group') + string\
 + group_type + pp.Literal('{') + pp.OneOrMore(block) + pp.Literal('}')
block << (block_group | block_value)

config = pp.StringStart() + block + pp.StringEnd()
</pre><p>Fun fact: this parser doesn't work.</p><p>Bails with some error in the middle of the large (~8k lines) real-world config, while working for all the smaller pet samples.</p><p>I guess some buffer size must be tweaked (kinda unusual for python module though), maybe I made a mistake there, or something like that.</p><p>So, yapps2-based parser:</p><pre>parser eet_cfg:<br /> ignore: r'[ \t\r\n]+'<br /> token END: r'$'<br /> token N: r'[+\-]?[\d.]+'<br /> token S: r'"([^"\\]*(\\.[^"\\]*)*)"'<br /> token VT: r'\w+:'<br /> token GT: r'struct|list'<br /><br /> rule config: block END {{ return block }}<br /><br /> rule block: block_group {{ return block_group }}<br />  | block_value {{ return block_value }}<br /><br /> rule block_group:<br />  'group' S GT r'\{' {{ contents = list() }}<br />  ( block {{ contents.append(block) }} )*<br />  r'\}' {{ return Group(S, GT, contents) }}<br /><br /> rule value: S {{ return S }} | N {{ return N }}<br /> rule block_value: 'value' S VT value ';' {{ return Value(S, VT, value) }}<br /><br /></pre><p>Less verbose (even with more processing logic here) and works.</p><p>Embedded in a python code (doing the actual sorting), it all looks like<a title="e_config_sorter.g" href="http://fraggod.net/static/code/e_config_sorter.g"> this</a> (might be useful to work with E configs, btw).</p><p>yapps2 actually generates <a title="e_config_sorter.py - compile from .g" href="http://fraggod.net/static/code/e_config_sorter.py">quite readable code</a> from it, and it was just simplier (and apparently more bugproof) to write grammar rules in it.</p><p>ymmv, but it's a bit of a shame that pyparsing seem to be the about the only developed parser-generator of such kind for python these days.</p><p>Had to package yapps2 runtime to install it properly, applying some community patches (from debian package) in process and replacing some scary cli code from 2003. <a title="my github fork of yapps" href="https://github.com/mk-fg/yapps">Here's a fork</a>.</p>-=|||=-<p>As I've been decompiling dynamic E config <a title="fun with E zero update" href="http://blog.fraggod.net/2013/1/Migrating-configuration-settings-to-E17-enlightenment-0170-from-older-E-versions">in the past</a> anyway, wanted to back it up to git repo along with the rest of them.</p><p>Quickly stumbled upon a problem though - while E doesn't really modify it without me making some conscious changes, it reorders (or at least eet produces such) sections and values there, making straight dump to git a bit more difficult.<br />Plus, I have a <a title="aura project" href="http://desktop-aura.sourceforge.net/">pet project</a> to update background, and it also introduces transient changes, so some pre-git processing was in order.</p><p>e.cfg looks like this:</p><pre>group "E_Config" struct {<br />  group "xkb.used_options" list {<br />    group "E_Config_XKB_Option" struct {<br />      value "name" string: "grp:caps_toggle";<br />    }<br />  }<br />  group "xkb.used_layouts" list {<br />    group "E_Config_XKB_Layout" struct {<br />      value "name" string: "us";<br />...<br /><br /></pre><p>Simple way to make it "canonical" is just to order groups/values there alphabetically, blanking-out some transient ones.</p><p>That needs a parser, and while regexps aren't really suited to that kind of thing, pyparsing should work:</p><pre name="code" class="python">number = pp.Regex(r'[+-]?\d+(\.\d+)?')<br />string = pp.QuotedString('"') | pp.QuotedString("'")<br />value_type = pp.Regex(r'\w+:')<br />group_type = pp.Regex(r'struct|list')<br /><br />value = number | string<br />block_value = pp.Keyword('value')\<br /> + string + value_type + value + pp.Literal(';')<br /><br />block = pp.Forward()<br />block_group = pp.Keyword('group') + string\<br /> + group_type + pp.Literal('{') + pp.OneOrMore(block) + pp.Literal('}')<br />block << (block_group | block_value)<br /><br />config = pp.StringStart() + block + pp.StringEnd()<br /></pre><p>Fun fact: this parser doesn't work.</p><p>Bails with some error in the middle of the large (~8k lines) real-world config, while working for all the smaller pet samples.</p><p>I guess some buffer size must be tweaked (kinda unusual for python module though), maybe I made a mistake there, or something like that.</p><p>So, yapps2-based parser:</p><pre>parser eet_cfg:<br /> ignore: r'[ \t\r\n]+'<br /> token END: r'$'<br /> token N: r'[+\-]?[\d.]+'<br /> token S: r'"([^"\\]*(\\.[^"\\]*)*)"'<br /> token VT: r'\w+:'<br /> token GT: r'struct|list'<br /><br /> rule config: block END {{ return block }}<br /><br /> rule block: block_group {{ return block_group }}<br />  | block_value {{ return block_value }}<br /><br /> rule block_group:<br />  'group' S GT r'\{' {{ contents = list() }}<br />  ( block {{ contents.append(block) }} )*<br />  r'\}' {{ return Group(S, GT, contents) }}<br /><br /> rule value: S {{ return S }} | N {{ return N }}<br /> rule block_value: 'value' S VT value ';' {{ return Value(S, VT, value) }}<br /><br /></pre><p>Less verbose (even with more processing logic here) and works.</p><p>Embedded in a python code (doing the actual sorting), it all looks like<a title="e_config_sorter.g" href="http://fraggod.net/static/code/e_config_sorter.g"> this</a> (might be useful to work with E configs, btw).</p><p>yapps2 actually generates <a title="e_config_sorter.py - compile from .g" href="http://fraggod.net/static/code/e_config_sorter.py">quite readable code</a> from it, and it was just simplier (and apparently more bugproof) to write grammar rules in it.</p><p>ymmv, but it's a bit of a shame that pyparsing seem to be the about the only developed parser-generator of such kind for python these days.</p><p>Had to package yapps2 runtime to install it properly, applying some community patches (from debian package) in process and replacing some scary cli code from 2003. <a title="my github fork of yapps" href="https://github.com/mk-fg/yapps">Here's a fork</a>.</p>-=||||||||||=-